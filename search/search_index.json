{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Before You Begin","text":"<p>This guide serves as both a solution accelerator and a learning tool for developers who want to build AI-powered solutions built on top of Azure Database for PostgreSQL and Azure AI Services. The Woodgrove Bank solution is an actively updated project that will reflect the latest features and best practices for development of AI-enabled applications and RAG-based copilots on the Azure AI platform.</p> <p>You can complete it as a self-guided workshop at home. Instructor-led workshop options will be provided in the near future.</p> <p>CHOOSE THE TAB FOR YOUR SESSION - This becomes the default context site-wide.</p> <p>To complete this guide, you will need the following:</p> <ul> <li> Requires using your own GitHub account - you can get one for free</li> <li> Requires using your own Azure subscription - you can get one for free</li> <li> Requires you to provision the infrastructure in your Azure subscription - we provide detailed instructions and Bicep deployment scripts</li> <li> Requires using your own computer</li> <li> Requires you to setup a development environment on your computer - we provide detaled instructions</li> </ul>"},{"location":"01-Introduction/","title":"Introduction","text":"<p>This solution accelarator is designed as an end-to-end example of a Financial Services Industry AI-enabled application. It demonstrates the implementation of generative AI capabilities to enhance an existing application with AI-driven data validation, vector search, semantic ranking, and GraphRAG on Azure Database for PostgreSQL, and illustrates how they can be combined to deliver high quality responses to financial questions via an intelligent copilot. The app uses a small sample dataset made up of statements of work (SOWs) and invoices. The source code for the accelerator is provided in the following repo: https://github.com/solliancenet/microsoft-postgresql-solution-accelerator-build-your-own-ai-copilot.</p> <p>The application has the following architecture:</p> <p></p>"},{"location":"01-Introduction/#bringing-your-own-data-to-the-solution","title":"Bringing your own data to the solution","text":"<p>This solution accelerator is structured to use sample vendor, SOW, and invoice data, which has been provided for demonstration purposes. However, if you want to use it with your own data or augment an existing solution, you will need to modify certain steps. Where applicable, notes are provided to indicate key areas where adjustments may be necessary to integrate custom datasets.</p>"},{"location":"01-Introduction/#learning-objectives","title":"Learning Objectives","text":"<p>The goal of the solution accelerator is to teach you to how to add rich AI capabilities using Azure Database for PostgreSQL and Azure AI Services to your existing applications. You will gain hands-on experience integrating advanced AI validation during data ingestion to ensure financial documents, like invoices, align with their associated statement of work. By leveraging Azure OpenAI for robust data validation and Azure Document Intelligence for comprehensive extraction and analysis, you will improve data quality. By adding a copilot chat feature, you will provide the ability for users to gain deep insights into vendors' invoicing accuracy, timeliness, and quality. This comprehensive approach equips you with the skills to seamlessly enrich your existing applications with AI-enhanced features, boosting their performance and reliability in the financial services industry.</p> <p>By the end of the workshop, you will learn to:</p> <ul> <li>Use Azure AI Services to automate data validation tasks during ingestion to streamline workflows.</li> <li>Integrate Generative AI capabilities into your Azure Database for PostgreSQL-based applications using the Azure AI extension.</li> <li>Use the Retrieval Augmented Generation (RAG) pattern in a copilot  (to ground responses in your own data).</li> <li>Use Azure Container Apps for deployment  (to get a hosted API endpoint for real-world use).</li> <li>Use Azure Developer CLI with AI Application Templates  (to provision &amp; deploy apps consistently across teams)</li> </ul>"},{"location":"01-Introduction/#learning-resources","title":"Learning Resources","text":"<ol> <li>Azure Database for PostgreSQL - Flexible Server | Overview</li> <li>Generative AI with Azure Database for PostgreSQL - Flexible Server | Overview</li> <li>Azure AI extension | How to integration Azure AI</li> <li>Azure AI Foundry  | Documentation \u00b7 Architecture \u00b7 SDKs \u00b7  Evaluation</li> <li>Azure Container Apps  | Azure Container Apps \u00b7 Deploy from code</li> <li>Responsible AI  | Overview \u00b7 With AI Services \u00b7 Azure AI Content Safety</li> </ol>"},{"location":"01-Introduction/01-App-Scenario/","title":"1.1 The App Scenario","text":""},{"location":"01-Introduction/01-App-Scenario/#streamlining-contract-validation-in-financial-services","title":"Streamlining Contract Validation in Financial Services","text":"<p>In the financial services industry, validating contract-related documents such as Statements of Work (SOWs) and invoices presents unique challenges. Ensuring that invoices align with SOWs, especially for milestone-based payments and specific deliverables, can be a meticulous and error-prone process. Traditionally, this validation involves manual comparison and cross-checking, often leading to delays, errors, and increased operational costs. This accelerator offers a solution that leverages Azure Database for PostgreSQL - Flexible and Azure's comprehensive suite of AI services to automate and streamline this process, resulting in faster, more accurate, and cost-effective invoice validation.</p> <p>The accelerator is designed to demonstrate how an existing financial services application can be enhanced by integrating advanced AI capabilities into Azure Database for PostgreSQL through the Azure AI extension and incorporating Azure OpenAI's GPT-4 model to validate and review contract-related documents.</p>"},{"location":"01-Introduction/01-App-Scenario/#getting-started-with-the-woodgrove-bank-application","title":"Getting Started with the Woodgrove Bank Application","text":"<p>You have been provided starter code and deployment scripts for the Woodgrove Bank web application. This application comprises an enterprise user portal integrated with a custom backend API. You will enhance this application by integrating Azure AI services throughout this accelerator. Key steps include:</p> <ol> <li>Integrating Generative AI (GenAI) Capabilities into Azure Database for PostgreSQL: Use the Azure AI <code>azure_ai</code> and pgvector (<code>vector</code>) extensions to extend your PostgreSQL database with advanced GenAI and vector search capabilities.</li> <li>Automating Data Validation with AI: Enhance the data ingestion process with automated, AI-driven validation using Azure Document Intelligence and Azure AI services.</li> <li>Building a Copilot: Create an intelligent assistant using Azure OpenAI and Azure Database for PostgreSQL - Flexible Server, incorporating the Retrieval Augmented Generation (RAG) design pattern to ensure its responses are based on the private data maintained by the enterprise.</li> <li>Adding GraphRAG functionality: Install the Apache AGE (<code>age</code>) extension to allow your PostgreSQL database to be used as a graph database, providing a comprehensive solution for analyzing interconnected data.</li> </ol> <p>This solution accelerator aims to teach you how to integrate AI capabilities into an existing application by leveraging Microsoft Azure's AI services to automate and streamline the validation of contract-related documents in the financial services industry. This integration results in faster, more accurate, and cost-effective processes. Additionally, the copilot will provide intelligent assistance, enabling users to gain actionable insights from data stored in the Azure Database for PostgreSQL, enhancing their overall experience.</p> <p>Using your own data?</p> <p>While the provided scenario utilizes pre-configured vendor, SOW, and invoice data, the framework is designed to be adaptable. You can replace these with your own datasets to better align with your specific business needs. Key steps where adjustments may be required have been highlighted throughout the guide.</p>"},{"location":"01-Introduction/02-App-Architecture/","title":"1.2 Application Architecture","text":"<p>The Woodgrove Bank Contract Management application automates extracting, validating, and storing data from invoices and SOWs to minimize manual effort and boost operational efficiency while allowing internal application users to gain actionable insights from the data. By focusing on this streamlined flow, the solution effectively automates tedious tasks, reduces errors, and provides valuable insights to internal users, enhancing overall operational efficiency and decision-making.</p> <p>Throughout this solution accelerator, you will enhance the application with AI capabilities. The application consists of a REACT single page application (SPA) providing the UX (user experience), a backend API written in Python using FastAPI, and various Azure services. The solution implements the following high-level architecture:</p> <p></p> <p>Decoupled application architecture</p> <p>Separating app functionality into a dedicated UI and backend API offers several benefits. Firstly, it enhances modularity and maintainability, allowing you to update the UI or backend independently without disrupting the other. REACT and Node.js provide an intuitive and interactive user interface that simplifies user interactions, while the Python API leveraging FastAPI ensures high-performance, asynchronous request handling and data processing. This separation also promotes scalability, as different components can be deployed across multiple servers, optimizing resource usage. Additionally, it enables better security practices, as the backend API can handle sensitive data and authentication separately, reducing the risk of exposing vulnerabilities in the UI layer. This approach leads to a more robust, efficient, and user-friendly application.</p>"},{"location":"01-Introduction/02-App-Architecture/#application-data-flow","title":"Application Data Flow","text":"<p>Select each tab below to learn more about how the movement of data in the context of the Woodgrove Bank Contract Management application!</p> Data Ingestion &amp; AI Processing <p>How The Automated Data Ingestion and AI Validation Process Works</p> <p>Internal users and external vendors can introduce new documents, SOWs, and invoices into the system via an intuitive browser-based user interface. This action kicks off automated processes to extract and validate the data within those documents. Extracted data and validation results are returned to the users in the UI, allowing them to review and make updates as necessary.</p> <p></p> <ol> <li> <p>SOWs, invoices, and related documents are ingested via the Woodgrove Bank Contract Management Portal, a REACT Single Page Application (SPA) accessed through a web browser. Internal users and external vendors can submit documents via the portal.</p> </li> <li> <p>The SPA web app sends uploaded documents directly with the backend API's <code>/documents</code> endpoint.</p> </li> <li> <p>The API, hosted as an Azure Container App (ACA), saves the uploaded documents into a container in Azure Blob storage.</p> <ol> <li> <p>Storing the original documents in blob storage allows raw data to be persisted.</p> </li> <li> <p>Should processing errors be detected or system requirements change, documents can be easily reprocessed.</p> </li> </ol> </li> <li> <p>When new documents are added into blob storage, the Data Ingestion Process on the API is invoked.</p> <ol> <li> <p>The data ingestion process handles data extraction and processing by sending uploaded documents to the Azure AI Document Intelligence service.</p> </li> <li> <p>AI models within the Document Intelligence service are tailored to extract specific data fields, such as payment milestones, due dates, billable amounts, and vendor details. These models are trained to recognize the structure of financial documents, improving data extraction accuracy.</p> </li> <li> <p>Document Intelligence's Semantic Chunking capability recognizes document structures, capturing headings and chunking the content body based on semantic coherence, such as paragraphs and sentences. This ensures that the chunks are of higher quality for use in RAG pattern queries.</p> </li> </ol> </li> <li> <p>The extracted document data securely written to Azure Database for PostgreSQL flexible server by the API.</p> </li> <li> <p>As part of the database insert statement, the GenAI capabilities of the <code>azure_ai</code> extension are used to:</p> <ol> <li> <p>Generate and save vector embeddings of document chunks using Azure OpenAI's <code>text-embedding-ada-002</code> model.</p> </li> <li> <p>Create abstractive summaries of SOWs using the Azure AI Language service.</p> </li> </ol> </li> <li> <p>Document data is then sent through an AI-driven data validation process on the API that uses Azure OpenAI to analyze the data extracted by Document Intelligence, ensuring it conforms to expected standards and is accurate based on related data already in the system.</p> <ol> <li> <p>Azure OpenAI's GPT-4o language model reviews all document data, employing natural language understanding to validate and cross-check information and ensure high data integrity.</p> </li> <li> <p>The RAG pattern allows the language model to cross-reference data between invoices and SOWs, evaluating payment milestone completion and billing and preventing issues like payment delays. It also validates that appropriate document sections and required compliance language exist in contracts and SOWs, helping to avoid incomplete contracts and compliance violations.</p> </li> </ol> </li> <li> <p>The data validation results are securely stored in Azure Database for PostgreSQL alongside the analyzed data. The validation result is also vectorized on insert using the <code>azure_ai</code> extension.</p> </li> </ol> <p>Using your own data?</p> <p>The architecture is designed for flexibility, allowing adaptation to custom datasets beyond the default sample data. If you plan to use your own data, it is recommended to review the database schema, data pipelines, and indexing strategies to ensure compatibility. Suggested changes and guidance for customization are provided in the documentation to help streamline this process.</p> AI Copilot with RAG <p>How The Custom Copilot Experience Works</p> <p>Internal users interact with data through an intelligent copilot that employs the Retrieval Augmented Generation (RAG) pattern. This copilot allows users to ask questions about contract data, offering valuable insights into vendor contract fulfillment and invoicing accuracy.</p> <p></p> <ol> <li> <p>Users interact with the Woodgrove Bank Contract Management Copilot through a browser interface to pose queries or seek information.</p> </li> <li> <p>The REACT SPA sends these chat messages to the <code>/chat</code> API endpoint hosted in ACA.</p> </li> <li> <p>The request query is embedded using the <code>text-embedding-ada-002</code> model in Azure OpenAI.</p> </li> <li> <p>A hybrid search is performed on the Azure Database for PostgreSQL flexible server, where the system searches for relevant data.</p> <ol> <li> <p>Hybrid search combines full-text search with vector-based search to provide more accurate and relevant results. It allows you to perform searches using both traditional keyword matching and semantic similarity, leveraging embeddings to understand the context and meaning behind the text.</p> </li> <li> <p>By integrating these two methods, hybrid search enhances the precision and comprehensiveness of search results, making it ideal for applications like semantic search, recommendation systems, and content discovery.</p> </li> </ol> </li> <li> <p>(Optional) Semantic Ranking via model inference from the <code>azure_ai</code> extension ranks search result relevance and is returned into the RAG context as part of the composite prompt.</p> </li> <li> <p>Azure OpenAI uses the composite prompt to formulates a response.</p> <ol> <li>The composite prompt contains the system prompt augmented with context provided by the results of the hybrid search against the PostgreSQL database.</li> </ol> </li> <li> <p>The AI-generated completion response is sent back to the user through the browser interface, providing them with actionable insights based on the data stored in the system. The efficient flow of information ensures users can quickly and accurately obtain the information they need.</p> </li> </ol>"},{"location":"01-Introduction/02-App-Architecture/#azure-cost","title":"Azure Cost","text":"<p>The Microsoft Azure resources you deploy will be provisioned within your own Azure Subscription, so you will be responsible for the cost of those services. The cost of the solution will vary depending on the Azure region chosen, as well as which deployment options you choose.</p> <p>Most notably, these are the deployment options that will affect the cost:</p> <ul> <li>Deploy without Azure Machine Learning model for Semantic Ranker: This deployment will cost approximately $50 per day.</li> <li>Deploy with Azure Machine Learning model for Semantic Ranker: This deployment will cost approximately $75 per day.</li> </ul> <p>The Setup section of this guide will tell you when / how to choose this deployment option.</p> <p>Here's a breakout of an estimated cost of Azure resources deployed for this solution:</p> <ul> <li>Azure ML VM (semantic ranking model deployment): ~$19.50/day</li> <li>Azure Database for PostgreSQL: ~$3.40/day</li> <li>Azure App Configuration: ~$1.20/day</li> <li>Azure Container Registry: ~$0.67/day</li> <li>Azure OpenAI Service: Dependent upon usage of Copilot, AI-validation, and number of documents processed in the solution.</li> <li>Other services are minimal cost.</li> </ul> <p></p>"},{"location":"02-Setup/","title":"Setup","text":"<p>To get started, you will provision the required resources in Azure and configure your development environment to run the provided starter solution.</p> <p>GitHub repo: PostgreSQL Solution Accelerator: Build your own AI Copilot</p> <p>Before starting, you should:</p> <ol> <li>Review the prerequisites for completing the lab.</li> <li>Select the appropriate provisioning and setup guide you will follow:<ul> <li>Self-Guided</li> <li>Instructor-Led</li> </ul> </li> </ol>"},{"location":"02-Setup/0-PreRequisites/","title":"Prerequisites","text":"<p>Select the tab of your chosen track for details about what you need to do before starting the workshop, what you are expected to know beforehand, and what you can expect to take away after completing it.</p> Self-GuidedInstructor-Led Workshop <p>Expand each block below and review the requirements within each.</p> 1. What You Need <ol> <li>Your own computer.<ul> <li>Any computer capable of running Visual Studio Code, Docker Desktop, and a modern web browser will do.</li> <li>You must have the ability to install software on the computer.</li> <li>We recommend installing a recent version of Edge, Chrome, or Safari.</li> </ul> </li> <li>A GitHub Account.<ul> <li>This is required to create a copy (known as a fork) of the sample repository.</li> <li>We recommend using a personal (vs. enterprise) GitHub account for convenience.</li> <li>If you don't have a GitHub account, sign up for a free one now. (It takes just a few minutes.)</li> </ul> </li> <li>An Azure Subscription.<ul> <li>This is needed to provision the Azure infrastructure for your AI project.</li> <li>If you don't have an Azure account, sign up for a free one now. (It takes just a few minutes.)</li> </ul> </li> <li>Sufficient Azure ML Online Endpoint CPU quota<ul> <li>To run the solution accelerator's Semantic Ranker element, you must have at least 32 Standard DASv4 Family Cluster Dedicated vCPUs cores available within your subscription. Detailed instructions are provided in the setup section to verify this in your subscription.</li> </ul> </li> </ol> 2. What You Should Know (expand to view) <p>Recommended knowledge and experience</p> <ol> <li>Familiarity with Visual Studio Code <ul> <li>The default editor used in this workshop is Visual Studio Code. You will configure your VS Code development environment with the required extensions and code libraries.</li> <li>The workshop requires Visual Studio Code and other tools to be installed on your computer. You will be running the solution code from your local computer.    </li> </ul> </li> <li>Familiarity with the Azure portal<ul> <li>The workshop assumes you are familiar with navigating to resources within the Azure portal.</li> <li>You will use the Azure portal to retrieve endpoints, keys, and other values associated with the resources you deploy for this workshop.</li> </ul> </li> <li>Familiarity with PostgreSQL<ul> <li>The workshop assumes you are familiar with basic SQL syntax.</li> <li>You will be executin SQL statements to alter tables, create extensions, and run queries against tables.</li> </ul> </li> </ol> <p>Preferred knowledge and experience</p> <ol> <li>Familiarity with <code>git</code> operations<ul> <li>You will be forking the sample repository into your GitHub account.</li> <li>You will be committing code changes to your forked repo.</li> </ul> </li> <li>Familiarity with the <code>bash</code> shell.<ul> <li>If needed, you will use <code>bash</code> in the VS Code terminal to run post-provisioning scripts.</li> <li>You will also use it to run Azure CLI and Azure Developer CLI commands during setup. </li> </ul> </li> <li>Familiarity with Python and JavaScript UI frameworks.<ul> <li>You will modify REACT JavaScript and Python code to implement changes to the starter solution.</li> <li>In some steps, you will create and run Python code from the command line and VS Code.</li> <li>You will select a Python kernel and run pre-existing scripts in some steps.</li> </ul> </li> </ol> 3. What You Will Take Away (expand to view) <p>After completing this workshop, you will have:</p> <ol> <li>A personal fork (copy) of the Build Your Own AI Copilot for FSI with PostgreSQL repository in your GitHub profile. This repo contains all the materials you need to reproduce the workshop later (e.g., as a Self-Guided session).</li> <li>Hands-on understanding of the Azure AI Foundry portal and relevant developer tools (e.g., Azure Developer CLI, Prompty, FastAPI) to streamline end-to-end development workflows for your own AI apps.</li> <li>An understanding of how Azure AI services can be integrated into applications to create powerful AI-enabled applications.</li> </ol> <p>Expand each block below and review the requirements within each.</p> <p>&lt;!-- ???+ info \"1. What You Need\"   </p> Text Only<pre><code>The instructor-guided labs are set up with everything you need to get started. To get the most from this session, please review the recommended and preferred knowledge and experience in the blocks below. _If you revisit the workshop later at home, use the [Self-Guided version](./../1-Provision-And-Setup/03-Self-Guided.md) instead_. --&gt;\n</code></pre> 1. What You Need <ol> <li>Your own computer.<ul> <li>Any computer capable of running Visual Studio Code, Docker Desktop, and a modern web browser will do.</li> <li>You must have the ability to install software on the computer.</li> <li>We recommend installing a recent version of Edge, Chrome, or Safari.</li> </ul> </li> <li>A GitHub Account.<ul> <li>This is required to create a copy (known as a fork) of the sample repository.</li> <li>We recommend using a personal (vs. enterprise) GitHub account for convenience.</li> <li>If you don't have a GitHub account, sign up for a free one now. (It takes just a few minutes.)</li> </ul> </li> <li>An Azure Subscription.<ul> <li>This is needed to provision the Azure infrastructure for your AI project.</li> <li>If you don't have an Azure account, sign up for a free one now. (It takes just a few minutes.)</li> </ul> </li> <li>Sufficient Azure ML Online Endpoint CPU quota<ul> <li>To run the solution accelerator's Semantic Ranker element, you must have at least 32 Standard DASv4 Family Cluster Dedicated vCPUs cores available within your subscription. Detailed instructions are provided in the setup section to verify this in your subscription.</li> </ul> </li> </ol> 2. What You Should Know (expand to view) <p>Recommended knowledge and experience</p> <ol> <li>Familiarity with Visual Studio Code <ul> <li>The default editor used in this workshop is Visual Studio Code. You will configure your VS Code development environment with the required extensions and code libraries.</li> <li>The workshop requires Visual Studio Code and other tools to be installed on your computer. You will be running the solution code from your local computer.    </li> </ul> </li> <li>Familiarity with the Azure portal<ul> <li>The workshop assumes you are familiar with navigating to resources within the Azure portal.</li> <li>You will use the Azure portal to retrieve endpoints, keys, and other values associated with the resources you deploy for this workshop.</li> </ul> </li> <li>Familiarity with PostgreSQL<ul> <li>The workshop assumes you are familiar with basic SQL syntax.</li> <li>You will be executin SQL statements to alter tables, create extensions, and run queries against tables.</li> </ul> </li> </ol> <p>Preferred knowledge and experience</p> <ol> <li>Familiarity with <code>git</code> operations<ul> <li>You will be forking the sample repository into your GitHub account.</li> <li>You will be committing code changes to your forked repo.</li> </ul> </li> <li>Familiarity with the <code>bash</code> shell.<ul> <li>If needed, you will use <code>bash</code> in the VS Code terminal to run post-provisioning scripts.</li> <li>You will also use it to run Azure CLI and Azure Developer CLI commands during setup. </li> </ul> </li> <li>Familiarity with Python and JavaScript UI frameworks.<ul> <li>You will modify REACT JavaScript and Python code to implement changes to the starter solution.</li> <li>In some steps, you will create and run Python code from the command line and VS Code.</li> <li>You will select a Python kernel and run pre-existing scripts in some steps.</li> </ul> </li> </ol> 3. What You Will Take Away (expand to view) <p>After completing this workshop, you will have:</p> <ol> <li>A personal fork (copy) of the Build Your Own AI Copilot for FSI with PostgreSQL repository in your GitHub profile. This repo contains all the materials you need to reproduce the workshop later (e.g., as a Self-Guided session).</li> <li>Hands-on understanding of the Azure AI Foundry portal and relevant developer tools (e.g., Azure Developer CLI, Prompty, FastAPI) to streamline end-to-end development workflows for your own AI apps.</li> <li>An understanding of how Azure AI services can be integrated into applications to create powerful AI-enabled applications.</li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/","title":"1. Provision &amp; Setup","text":"<p>A starter solution has been provided, which you will be modifying to add rich AI capabilities throughout this workshop. This initial application includes a user-friendly REACT UI, offering an intuitive frontend interface for users to interact with. Additionally, it features a Python-based backend API that handles the core business logic and data processing tasks. Throughout the workshop, you will enhance this existing solution by integrating advanced AI functionalities. This includes adding AI validation for data ingestion and leveraging AI-powered tools to analyze financial documents. You will also a the ability to ask questions over private data through an intelligent copilot. By the end of the workshop, you will have transformed the starter application into a sophisticated, AI-enhanced solution capable of providing deep insights into financial data, improving accuracy, efficiency, and overall performance in the financial services industry.</p> <p>To get started building the custom AI-enable Financial Services Industry (FSI) application, you need to:</p> <ul> <li>PROVISION the required Azure infrastructure for the resources needed for the application architecture</li> <li>SETUP your development environment and configure it to work with the infrastructure</li> <li>VALIDATE that the setup completed successfully, before diving into the ideation phase.</li> </ul>"},{"location":"02-Setup/1-Provision-And-Setup/00-Install-Software/","title":"Install Software","text":"<p>You will need to install the required software locally and provision the Azure infrastructure yourself, as described on the tabs below.</p>"},{"location":"02-Setup/1-Provision-And-Setup/00-Install-Software/#1-install-software","title":"1. Install Software","text":"<p>The required development environment uses a Visual Studio (VS) Code editor with a Python runtime. To complete this lab on your own computer, you must install the following required software. On completing this step, you should have installed:</p> <ul> <li> Azure command-line tools</li> <li> Git</li> <li> Python 3.11+</li> <li> Node.js</li> <li> Docker desktop</li> <li> Visual Studio Code and required extensions</li> <li> pgAdmin</li> </ul>"},{"location":"02-Setup/1-Provision-And-Setup/00-Install-Software/#11-install-azure-command-line-tools","title":"1.1 Install Azure command-line tools","text":"<p>In this task, you will install both the Azure CLI and the Azure Developer CLI (<code>azd</code>).</p> <ul> <li>The Azure CLI enables you to execute Azure CLI commands from a command prompt or VS Code terminal on your local machine.</li> <li>The Azure Developer CLI (<code>azd</code>) is an open-source tool that accelerates provisioning and deploying app resources on Azure.</li> </ul> <ol> <li> <p>Download and install the latest version of the Azure CLI.</p> </li> <li> <p>Once installed, open a command prompt on your machine and verify the installation by running the following:</p> <pre><code>az version\n</code></pre> </li> <li> <p>Next, install the <code>ml</code> extension to the Azure CLI.</p> <p>About the ml extension</p> <p>The <code>ml</code> extension to the Azure CLI is the enhanced interface for Azure Machine Learning. It enables you to train and deploy models from the command line, with features that accelerate scaling data science up and out while tracking the model lifecycle.</p> <p>To install the <code>ml</code> extensinon you should first remove any existing installation of the extension and also the CLI v1 <code>azure-cli-ml</code> extension:</p> <pre><code>az extension remove -n azure-cli-ml\naz extension remove -n ml\n</code></pre> <p>Then, run the following to install the latest version of the <code>ml</code> extension:</p> <pre><code>az extension add -n ml\n</code></pre> </li> <li> <p>Install Azure Developer CLI by following the instructions for your OS at https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd.</p> </li> <li> <p>Execute the following command from a terminal prompt to verify the tools were installed:</p> <pre><code>azd version\n</code></pre> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/00-Install-Software/#12-install-git","title":"1.2 Install Git","text":"<p>Git enables you to manage your code by tracking changes, maintaining a version history, and facilitating collaboration with others. This helps in organizing and maintaining the integrity of your project's development.</p> <ol> <li> <p>Download Git from https://git-scm.com/downloads.</p> </li> <li> <p>Run the installer using the default options.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/00-Install-Software/#13-install-python","title":"1.3 Install Python","text":"<p>Python is the programming used to build the backend API for the solution. By utilizing Python's versatile programming capabilities and Azure Database for PostgreSQL's generative AI and vector search capabilities, you can create powerful and efficient AI copilots and streamlining complex workflows.</p> <ol> <li> <p>Download Python 3.11+ from https://python.org/downloads.</p> </li> <li> <p>Run the installer using the default options.</p> </li> <li> <p>Use the following command from a terminal prompt to verify Python was installed:</p> <pre><code>python --version\n</code></pre> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/00-Install-Software/#14-install-nodejs","title":"1.4 Install Node.js","text":"<p>Node.js is an open-source runtime environment that lets you run JavaScript code outside of a browser. It's ideal for building scalable network applications and works seamlessly with REACT single-page applications by providing a backend environment to handle server-side logic and API requests. This allows for efficient development and smooth interactions between the frontend and backend.</p> <ol> <li> <p>Download Node.js from https://nodejs.org/en/download/, ensuring you select the most recent LTS version and your correct OS.</p> </li> <li> <p>Run the installer using the default options.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/00-Install-Software/#15-install-docker-desktop","title":"1.5 Install Docker Desktop","text":"<p>Docker Desktop is an application that allows you to build, share, and run containerized applications on your local machine. It provides a user-friendly interface to manage Docker containers, images, and networks. By streamlining the containerization process, Docker Desktop helps you develop, test, and deploy applications consistently across different environments.</p> <ol> <li>Download and install Docker Desktop for your OS using instructions provided on the https://docs.docker.com/desktop/:Text Only<pre><code>- [Linux](https://docs.docker.com/desktop/setup/install/linux/)\n- [Mac](https://docs.docker.com/desktop/setup/install/mac-install/)\n- [Windows](https://docs.docker.com/desktop/setup/install/windows-install/)\n</code></pre> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/00-Install-Software/#16-install-visual-studio-code-and-extensions","title":"1.6 Install Visual Studio Code (and extensions)","text":"<p>Visual Studio Code is a versatile, open-source code editor that combines powerful features with an intuitive interface to help you efficiently write, debug, and customize projects.</p> <ol> <li> <p>Download and install from https://code.visualstudio.com/download.</p> <ul> <li>Use the default options in the installer.</li> </ul> </li> <li> <p>After installation completed, launch Visual Studio Code.</p> </li> <li> <p>In the Extensions menu, search for and install the following extensions from Microsoft:</p> <ul> <li>Python</li> </ul> </li> <li> <p>Close VS Code.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/00-Install-Software/#17-install-pgadmin","title":"1.7 Install pgAdmin","text":"<p>Throughout this workshop, you will use pgAdmin to run queries against your PostgreSQL database. pgAdmin is the leading Open Source management tool for Postgres.</p> <ol> <li> <p>Download pgAdmin from https://www.pgadmin.org/download/.</p> </li> <li> <p>Run the installer using the default options.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/01-Fork-Repo/","title":"Fork Repo","text":"<p>You must create a copy (known as a fork) of the PostgreSQL Solution Accelerator: Build your own AI Copilot GitHub repo and then clone that onto your local computer so you can work with its contents. After completing this step, you should have:</p> <ul> <li> Forked the PostgreSQL Solution Accelerator: Build your own AI Copilot repo to your personal GitHub profile</li> <li> Created a local clone of the repo</li> <li> Opened the cloned repo in Visual Studio Code</li> </ul>"},{"location":"02-Setup/1-Provision-And-Setup/01-Fork-Repo/#fork-repo-to-your-profile","title":"Fork Repo To Your Profile","text":"<p>Forking in GitHub refers to creating a personal copy of a public repository, which allows you to freely experiment with changes without affecting the original project.</p> <ol> <li> <p>To fork the repo, open a new browser window or tab and navigate to https://github.com/solliancenet/microsoft-postgresql-solution-accelerator-build-your-own-ai-copilot.</p> </li> <li> <p>Select the Fork button to create a copy of the repo in your GitHub profile.</p> <p></p> </li> <li> <p>Login with your GitHub profile, if prompted.</p> </li> <li> <p>On the Create a new fork page, select Create fork to make a copy of the repo under your GitHub profile.</p> <p></p> </li> <li> <p>The forked repo will open within your GitHub profile.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/01-Fork-Repo/#clone-the-forked-repo","title":"Clone the Forked Repo","text":"<ol> <li> <p>On the GitHub page for your fork, select the Code button and then select the Copy URL to clipboard button next to the repo's HTTPS clone link:</p> <p></p> </li> <li> <p>Open a new command prompt and change directories to the folder in which you want to clone the repo (e.g., D:\\repos).</p> </li> <li> <p>Once in the desired directory, run the following <code>git clone</code> command to download a copy of your fork onto your local machine. Ensure you replace the <code>&lt;url_of_your_forked_repo&gt;</code> token with the clone link you copied in the previous step.</p> <pre><code>git clone &lt;url_of_your_forked_repo&gt;\n</code></pre> </li> <li> <p>Once the repository has been cloned, change directories at the command prompt to the folder of the cloned repo, then run the following command to open the project in Visual Studio Code:</p> <pre><code>code .\n</code></pre> </li> </ol> <p>Leave Visual Studio Code open as you will be using it throughout the remainder of the workshop.</p>"},{"location":"02-Setup/1-Provision-And-Setup/02-Verify-Azure-Quota/","title":"Verify Azure Quota","text":"<p>This solution contains an Azure Developer CLI <code>azd-template</code> that provisions the required resources in Azure and deploys the starter app to Azure Container Apps (ACA). The template allows for the infrastructure to be deployed with a single <code>azd up</code> command. On completing this step, you should have:</p> <ul> <li> Selected an Azure region for workshop resources</li> <li> Verified your Azure ML CPU quota</li> <li> Authenticated with Azure</li> <li> Provisioned Azure resources and deployed the starter solution</li> </ul>"},{"location":"02-Setup/1-Provision-And-Setup/02-Verify-Azure-Quota/#select-an-azure-region-for-your-workshop-resources","title":"Select an Azure region for your workshop resources","text":"<p>To ensure you can successfully deploy the Azure resources using the <code>azd up</code> command, you must choose a region that supports the required Azure OpenAI <code>gpt-4o</code> and <code>text-embedding-ada-002</code> models.</p> <ol> <li> <p>Before deciding on the Azure region you want to use for your workshop resources:</p> <ol> <li> <p>Review the regional availability guidance for the gpt-4o and text-embedding-ada-002 models in Azure OpenAI.</p> </li> <li> <p>Check the text abstractive summarization regional availability</p> </li> </ol> </li> <li> <p>Choose a region that supports both Azure OpenAI models and Text Abstractive Summarization.</p> </li> </ol> <p>Select a region that supports Azure OpenAI both models!</p> <p>Choosing a region that doesn't support both Azure OpenAI models will result in deployment failure when running <code>azd up</code>.</p> <p>Selecting a region that does not support abstractive summarization will not cause a deployment failure, but will need to make code changes later in the workshop to use extractive summarization in its place.</p>"},{"location":"02-Setup/1-Provision-And-Setup/02-Verify-Azure-Quota/#verify-azure-ml-cpu-quota","title":"Verify Azure ML CPU Quota","text":"<p>This solution accelerator contains a section dedicted to setting up and using a Semantic Ranking model directly from your PostgreSQL database. The deployment of this component of the architecture requires sufficient CPU quota (32 cores) in Azure Machine Learning to accomodate the Hugging Face BGE reranker model deployment. In this task, you must verify you have available quota for the target virtual machine (VM) instance type (<code>STANDARD_D16AS_V4</code>), and if not, request additional quota.</p> <ol> <li> <p>To view your available quota, you first need to retrieve your Microsoft Entra ID Tenant ID from the Azure portal.</p> </li> <li> <p>In the Azure portal, enter \"Microsoft Entra ID\" into the search bar, then select Microsoft Entra ID from the Services list in the results.</p> <p></p> </li> <li> <p>On the Overview page of your Microsoft Entra ID tenant, select the Copy to clipboard button for your Tenant ID.</p> <p></p> </li> <li> <p>Open a new browser window or tab and navigate to the following URL, replacing the <code>&lt;your-tenant-id&gt;</code> token with the Tenant ID you copied from the Entra ID overview page in the Azure portal.</p> Azure ML Quota page<pre><code>https://ml.azure.com/quota?tid=&lt;your-tenant-id&gt;\n</code></pre> </li> <li> <p>On the Azure ML Quota page, select the subscription you are using for this workshop.</p> <p></p> </li> <li> <p>On the quota page for your selected subscription, select the Azure region you plan to use for this workshop. This should be the region you chose in previous task that supports the required Azure OpenAI models.</p> </li> <li> <p>You should now see a list of CPUs and their quotas within your subscription. Locate Standard DASv4 Family Cluster Dedicated vCPUs in the list and inspect the Quota available.</p> <p></p> </li> <li> <p>If you have 32 cores or more available, you can skip to the Authenticate With Azure task. Otherwise, select the Standard DASv4 Family Cluster Dedicated vCPUs by checking the box to the left of the name, then scroll up to the top of the page and locate the Request quota button.</p> <p></p> </li> <li> <p>In the Request quota dialog, increase your New cores limit value by 32 and then select Submit.</p> <p></p> <p>Example</p> <p>Your new cores limit should be increased to ensure 32 cores are available for a new deployment. For example, if you have zero cores available, your new cores limit should be set to 32. If your core limit is 100 and you are currently using 90, your new cores limit should be set to 122.</p> </li> <li> <p>Quota increase requests typically take a few minutes to complete. You will receive notifications in the Azure portal as the request is processed and when it completes.</p> </li> <li> <p>If your request is denied, you don't have permissions to issue the request, or you prefer not to request additional quota, you have the option to exclude the Semantic Ranking model deployment when running the <code>azd up</code> command by setting the <code>deployAMLModel</code> flag to <code>false</code> when prompted.</p> </li> </ol> Self-GuidedInstructor-Led <p>You need to provision the infrastructure yourself! Jump to the Self-Guided section now!</p> <p>You will be provisioning the infrastructure during an Instructor-Led Workshop! Jump ahead to Instructor-Led section.</p>"},{"location":"02-Setup/1-Provision-And-Setup/03-Self-Guided/","title":"A. Self-Guided Setup","text":"<p>Welcome to the Self-Guided Lab Track! You will need a valid Azure subscription, a GitHub account, and access to relevant Azure OpenAI models to complete this lab. Review the prerequisites section if you need more details.</p> <p>WERE YOU LOOKING FOR THE INSTRUCTOR-LED OPTION INSTEAD? You can find that here.</p>"},{"location":"02-Setup/1-Provision-And-Setup/03-Self-Guided/#authenticate-with-azure","title":"Authenticate With Azure","text":"<p>Before running the <code>azd up</code> command, you must authenticate your VS Code environment to Azure.</p> <ol> <li>To create Azure resources, you need to be authenticated from VS Code. Open a new integrated terminal in VS Code. Then, complete the following steps:</li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/03-Self-Guided/#step-1-authenticate-with-az-for-post-provisioning-tasks","title":"Step 1: Authenticate with <code>az</code> for post-provisioning tasks","text":"<ol> <li> <p>Log into the Azure CLI <code>az</code> using the command below.</p> <pre><code>az login\n</code></pre> </li> <li> <p>Complete the login process in the browser window that opens.</p> <p>If you have more than one Azure subscription, you may need to run <code>az account set -s &lt;subscription-id&gt;</code> to specify the correct subscription to use.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/03-Self-Guided/#step-2-authenticate-with-azd-for-provisioning-managing-resources","title":"Step 2: Authenticate with <code>azd</code> for provisioning &amp; managing resources","text":"<ol> <li> <p>Log in to Azure Developer CLI. This is only required once per-install.</p> <pre><code>azd auth login\n</code></pre> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/03-Self-Guided/#provision-azure-resource-and-deploy-app-ui-and-api","title":"Provision Azure Resource and Deploy App (UI and API)","text":"<p>You are now ready to provision your Azure resources and deploy the Woodgrove back solution.</p> <ol> <li> <p>Use <code>azd up</code> to provision your Azure infrastructure and deploy the web application to Azure.</p> <pre><code>azd up\n</code></pre> <p>You will be prompted for several inputs for the <code>azd up</code> command:</p> <ul> <li>Enter a new environment name: Enter a value, such as <code>dev</code>.</li> <li>The environment for the <code>azd up</code> command ensures configuration files, environment variables, and resources are provisioned and deployed correctly.</li> <li>Should you need to delete the <code>azd</code> environment, locate and delete the <code>.azure</code> folder at the root of the project in the VS Code Explorer.</li> <li>Select an Azure Subscription to use: Select the Azure subscription you are using for this workshop using the up and down arrow keys.</li> <li>Select an Azure location to use: Select the Azure region into which resources should be deployed using the up and down arrow keys.</li> <li>Enter a value for the <code>deployAMLModel</code>: Select <code>True</code> if you were able to ensure you have sufficient Azure ML CPU quota available to deploy the model. Otherwise, choose <code>False</code>.</li> <li>If you select <code>False</code>, you will need to skip the optional Semantic Ranker section of this accelerator.</li> <li>Enter a value for the <code>resourceGroupName</code>: Enter <code>rg-postgresql-accelerator</code>, or a similar name.</li> </ul> </li> <li> <p>Wait for the process to complete. It may take 30-45 minutes or more.</p> <p>Not enough subscription CPU quota</p> <p>If you did not check your Azure ML CPU quota prior to starting running the <code>azd up</code> command, you may receive a CPU quota error message similar to the following:</p> <p>(OutOfQuota) Not enough subscription CPU quota. The amount of CPU quota requested is 32 and your maximum amount of quota is [N/A]. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota</p> <p>You can still continue with the workshop, but will need to skip the optional Semantic Ranking section, as you will not have the deployed model available.</p> <p>Deployment failed: Postgresql server is not in an accessible state</p> <p>It's possible a <code>server is not in an accessible state</code> error may occur when the Azure Bicep deployment attempts to add the PostgreSQL Admin User after the PostgreSQL Server has been provisioned. This can occur if the PostgreSQL server is still being provisioned in the Azure backend, but the Deployment returned that it's successful already. If you encounter this error, simply re-run the <code>azd up</code> command.</p> Text Only<pre><code>ERROR: error executing step command 'provision': deployment failed: error deploying infrastructure: deploying to subscription:\n\nDeployment Error Details:\nAadAuthOperationCannotBePerformedWhenServerIsNotAccessible: The server 'psql-datacvdjta5pfnc5e' is not in an accessible state to perform Azure AD Principal operation. Please make sure the server is accessible before executing Azure AD Principal operations.\n</code></pre> </li> <li> <p>On successful completion you will see a <code>SUCCESS: ...</code> message on the console.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/03-Self-Guided/#next-setup-dev-environment","title":"Next \u2192 Setup Dev Environment","text":""},{"location":"02-Setup/1-Provision-And-Setup/04-Instructor-Led/","title":"B. Instructor-Led Workshop Setup","text":"<p>Welcome to the Instructor-Guided Lab Track! You will need a valid Azure subscription, a GitHub account, and access to relevant Azure OpenAI models to complete this lab. Review the prerequisites section if you need more details.</p> <p>WERE YOU LOOKING FOR THE SELF-GUIDED OPTION INSTEAD? You can find that here.</p>"},{"location":"02-Setup/1-Provision-And-Setup/04-Instructor-Led/#authenticate-with-azure","title":"Authenticate With Azure","text":"<p>Before running the <code>azd up</code> command, you must authenticate your VS Code environment to Azure.</p> <ol> <li>To create Azure resources, you need to be authenticated from VS Code. Open a new integrated terminal in VS Code. Then, complete the following steps:</li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/04-Instructor-Led/#step-1-authenticate-with-az-for-post-provisioning-tasks","title":"Step 1: Authenticate with <code>az</code> for post-provisioning tasks","text":"<ol> <li> <p>Log into the Azure CLI <code>az</code> using the command below.</p> <pre><code>az login\n</code></pre> </li> <li> <p>Complete the login process in the browser window that opens.</p> <p>If you have more than one Azure subscription, you may need to run <code>az account set -s &lt;subscription-id&gt;</code> to specify the correct subscription to use.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/04-Instructor-Led/#step-2-authenticate-with-azd-for-provisioning-managing-resources","title":"Step 2: Authenticate with <code>azd</code> for provisioning &amp; managing resources","text":"<ol> <li> <p>Log in to Azure Developer CLI. This is only required once per-install.</p> <pre><code>azd auth login\n</code></pre> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/04-Instructor-Led/#provision-azure-resource-and-deploy-app-ui-and-api","title":"Provision Azure Resource and Deploy App (UI and API)","text":"<p>You are now ready to provision your Azure resources and deploy the Woodgrove back solution.</p> <ol> <li> <p>Navigate to the <code>./workshop</code> folder within the cloned repo.</p> <pre><code>cd workshop\n</code></pre> <p>The <code>./workshop</code> folder contains the Instructor-Led version of the infrastructure and Azure Developer CLI templates.</p> </li> <li> <p>Use <code>azd up</code> to provision your Azure infrastructure and deploy the web application to Azure.</p> <pre><code>azd up\n</code></pre> <p>You will be prompted for several inputs for the <code>azd up</code> command:</p> <ul> <li>Enter a new environment name: Enter a value, such as <code>dev</code>.</li> <li>The environment for the <code>azd up</code> command ensures configuration files, environment variables, and resources are provisioned and deployed correctly.</li> <li>Should you need to delete the <code>azd</code> environment, locate and delete the <code>.azure</code> folder at the root of the project in the VS Code Explorer.</li> <li>Select an Azure Subscription to use: Select the Azure subscription you are using for this workshop using the up and down arrow keys.</li> <li>Select an Azure location to use: Select the Azure region into which resources should be deployed using the up and down arrow keys.</li> <li>Enter a value for the <code>deployAMLModel</code>: Select <code>True</code> if you were able to ensure you have sufficient Azure ML CPU quota available to deploy the model. Otherwise, choose <code>False</code>.</li> <li>If you select <code>False</code>, you will need to skip the optional Semantic Ranker section of this accelerator.</li> <li>Enter a value for the <code>resourceGroupName</code>: Enter <code>rg-postgresql-accelerator</code>, or a similar name.</li> </ul> </li> <li> <p>Wait for the process to complete. It may take 30-45 minutes or more.</p> <p>Not enough subscription CPU quota</p> <p>If you did not check your Azure ML CPU quota prior to starting running the <code>azd up</code> command, you may receive a CPU quota error message similar to the following:</p> <p>(OutOfQuota) Not enough subscription CPU quota. The amount of CPU quota requested is 32 and your maximum amount of quota is [N/A]. Please see troubleshooting guide, available here: https://aka.ms/oe-tsg#error-outofquota</p> <p>You can still continue with the workshop, but will need to skip the optional Semantic Ranking section, as you will not have the deployed model available.</p> <p>Deployment failed: Postgresql server is not in an accessible state</p> <p>It's possible a <code>server is not in an accessible state</code> error may occur when the Azure Bicep deployment attempts to add the PostgreSQL Admin User after the PostgreSQL Server has been provisioned. This can occur if the PostgreSQL server is still being provisioned in the Azure backend, but the Deployment returned that it's successful already. If you encounter this error, simply re-run the <code>azd up</code> command.</p> Text Only<pre><code>ERROR: error executing step command 'provision': deployment failed: error deploying infrastructure: deploying to subscription:\n\nDeployment Error Details:\nAadAuthOperationCannotBePerformedWhenServerIsNotAccessible: The server 'psql-datacvdjta5pfnc5e' is not in an accessible state to perform Azure AD Principal operation. Please make sure the server is accessible before executing Azure AD Principal operations.\n</code></pre> </li> <li> <p>On successful completion you will see a <code>SUCCESS: ...</code> message on the console.</p> </li> </ol> <p>When following the Instructor-Led version of this guide, it is required to run all <code>azd up</code> and <code>azd deploy</code> commands from within the <code>./workshop</code> folder.</p>"},{"location":"02-Setup/1-Provision-And-Setup/04-Instructor-Led/#deploy-azure-openai-models","title":"Deploy Azure OpenAI Models","text":"<p>Now that the infrastructure has been provisioned, you are now ready to deploy the completions and embeddings models to Azure OpenAI Service. These models will be used when implementing the AI integration into the application. On completing this step, you will have these models deployed:</p> <ul> <li> Completions: The <code>gpt-40</code> model will be used for chat completions for the Copilot implementation.</li> <li> Embeddings: The <code>text-embedding-ada-002</code> model will be used to generate text embeddings that will be used to implement vector search for the Copilot implementation.</li> </ul> <p>Follow these steps to deploy the Azure OpenAI models:</p> <ol> <li> <p>Open a new browser tab to navigate to the link below. You may be prompted to login.</p> <pre><code>https://portal.azure.com/#browse/resourcegroups\n</code></pre> </li> <li> <p>You may be presented with a \"Welcome ot Microsoft Azure\" screen. Select Cancel (to dismiss it) or click Get Started (to take an introductory tour of the Azure Portal).</p> </li> <li> <p>You should be taken directly to the Resource Groups page for your subscription. In the list of resource groups, locate the one named <code>rg-postgresql-accelerator</code> (or, if you assigned a different name, find that one). This resource group was created for you as part of the <code>azd up</code> resource deployment. It contains all of the Azure resources required to build and deploy your AI-enabled solution.</p> <p>You can use the search filter to reduce the number resource groups displayed.</p> </li> <li> <p>Select your resource group.</p> </li> <li> <p>Within the list of resources within the resource group, locate and select the Azure OpenAI resource.</p> <p></p> </li> <li> <p>In the Get Started section of the Azure OpenAI resources Overview page, select the Explore Azure AI Foundry portal button to open up the Azure AI Foundry. You might be prompted to login the the AI Foundry.</p> <p></p> </li> <li> <p>In the Azure AI Foundry, select the Deployments option in the left-side navigation.</p> <p></p> </li> <li> <p>On the Model deployments page, select the Deploy model button, then select the Deploy base model option.</p> <p></p> </li> <li> <p>On the Select a model dialog, select the gpt-4o model, then select Confirm.</p> <p></p> </li> <li> <p>On the Deploy model gpt-4o dialog, ensure that <code>completions</code> is entered in the Deployment name field, then select Deploy. This will deploy the OpenAI GPT-4o completions model to the Azure OpenAI service. The name of the deployment must be <code>completions</code> for this workshop since the application code depends on this name.</p> <p></p> </li> <li> <p>Next, you'll deploy the Embeddings model. In the Azure AI Foundry, select the Deployments option in the left-side navigation.</p> <p></p> </li> <li> <p>On the Model deployments page, select the Deploy model button, then select the Deploy base model option.</p> </li> <li> <p>On the Select a model dialog, select the text-embedding-ada-002 model, then select Confirm.</p> <p></p> </li> <li> <p>On the Deploy model gpt-4o dialog, ensure that <code>embeddings</code> is entered in the Deployment name field, then select Deploy. This will deploy the OpenAI GPT-4o completions model to the Azure OpenAI service. The name of the deployment must be <code>embeddings</code> for this workshop since the application code depends on this name.</p> <p></p> </li> <li> <p>You have deployed the <code>gpt-4o</code> model that will be used for chat completions, and the <code>text-embedding-ada-002</code> model that will be used to generate text embeddings.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/05-Setup-Dev-Env/","title":"Setup Dev Environment","text":"<p>In this step, you will configure your Python development environment in Visual Studio Code. At the end of this step, you should have:</p> <ul> <li> Created a Python virtual environment</li> <li> Installed the required Python libraries from <code>requirements.txt</code></li> <li> Create and populated a <code>.env</code> file in the Woodgrove API project.</li> <li> Connected to your database using pgAdmin</li> </ul>"},{"location":"02-Setup/1-Provision-And-Setup/05-Setup-Dev-Env/#create-a-python-virtual-environment","title":"Create a Python virtual environment","text":"<p>Virtual environments in Python are essential for maintaining a clean and organized development space, allowing individual projects to have their own set of dependencies, isolated from others. This prevents conflicts between different projects and ensures consistency in your development workflow. By using virtual environments, you can manage package versions easily, avoid dependency clashes, and keep your projects running smoothly. It's a best practice that keeps your coding environment stable and dependable, making your development process more efficient and less prone to issues.</p> <ol> <li> <p>Return to Visual Studio Code, where you have the PostgreSQL Solution Accelerator: Build your own AI Copilot project open.</p> </li> <li> <p>In Visual Studio Code, open a new terminal window and change directories to the <code>src/api</code> folder of the repo.</p> </li> <li> <p>Create a virtual environment named <code>.venv</code> by running the following command at the terminal prompt:</p> <pre><code>python -m venv .venv \n</code></pre> <p>The above command will create a <code>.venv</code> folder under the <code>api</code> folder, which will provide a dedicated Python environment for the <code>api</code> project that can be used throughout this lab.</p> </li> <li> <p>Activate the virtual environment.</p> <p>Select the appropriate command for your OS and shell from the table.</p> Platform Shell Command to activate virtual environment POSIX bash/zsh <code>source .venv/bin/activate</code> fish <code>source .venv/bin/activate.fish</code> csh/tcsh <code>source .venv/bin/activate.csh</code> pwsh <code>.venv/bin/Activate.ps1</code> Windows cmd.exe <code>.venv\\Scripts\\activate.bat</code> PowerShell <code>.venv\\Scripts\\Activate.ps1</code> macOS bash/zsh <code>source .venv/bin/activate</code> </li> <li> <p>Execute the command at the terminal prompt to activate your virtual environment.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/05-Setup-Dev-Env/#install-required-python-libraries","title":"Install required Python libraries","text":"<p>The <code>requirements.txt</code> file in the <code>src\\api</code> folder contains the set of Python libraries needed to run the Python components of the solution accelerator.</p> <p>Review required libraries</p> <p>Open the <code>src\\api\\requirements.txt</code> file in the repo to review the required libraries and the versions that are being used.</p> <ol> <li> <p>From the integrated terminal window in VS Code, run the following command to install the required libraries in your virtual environment:</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/05-Setup-Dev-Env/#create-env-file","title":"Create <code>.env</code> file","text":"<p>Configuration values, such as connection string and endpoints, that allow your application to interact with Azure services are hosted in an Azure App Configuration service. To enable your application to retrieve these values, you must provide it with the endpoint of that service. You will use a <code>.env</code> file to host the endpoint as an environment variable, which will allow you to run the Woodgrove API locally. The <code>.env</code> file will be created within the <code>src\\api\\app</code> folder of the project.</p> <ol> <li> <p>In VS Code, navigate to the <code>src\\api\\app</code> folder in the Explorer panel.</p> </li> <li> <p>Right-click the <code>app</code> folder and select New file... from the context menu.</p> </li> <li> <p>Enter <code>.env</code> as the name of the new file within the VS Code Explorer panel.</p> </li> <li> <p>In the <code>.env</code> file, add the following as the first line, replacing the <code>{YOUR_APP_CONFIG_ENDPOINT}</code> with the endpoint for the App Configuration resource in your <code>rg-postgresql-accelerator</code> resource group.</p> <pre><code>AZURE_APP_CONFIG_ENDPOINT={YOUR_APP_CONFIG_ENDPOINT}\n</code></pre> <p>Retrieve the endpoint for your App Configuration resource</p> <p>To get the endpoint for your App Configuration resource:</p> <ol> <li> <p>Navigate to your App Configuration resource in the Azure portal.</p> </li> <li> <p>Select Access settings from the resource navigation menu, under Settings.</p> </li> <li> <p>Copy the Endpoint value and paste it into the <code>.env</code> file.</p> <p></p> </li> </ol> </li> <li> <p>Save the <code>.env</code> file.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/05-Setup-Dev-Env/#connect-to-your-database-from-pgadmin","title":"Connect to your database from pgAdmin","text":"<p>You will use pgAdmin from your machine to configure various features in the database and execute queries to test those features. The <code>azd up</code> deployment script added your Microsoft Entra ID user as the owner of the database, so you will authenticate with Entra ID to. Please follow the steps below to connect to your Azure Database for PostgreSQL - Flexible Server using pgAdmin:</p> <ol> <li> <p>Navigate to your Azure Database for PostgreSQL - Flexible Server resource in the Azure portal.</p> </li> <li> <p>On the Azure Database for PostgreSQL - Flexible Server page, copy the Server name value from the Essentials panel on the Overview page by selecting the Copy to clipboard button to the right of the value.</p> <p></p> </li> <li> <p>On your development computer, open pgAdmin.</p> </li> <li> <p>In the pgAdmin Object Explorer, right-click on Servers and in the context menu select Register &gt;, then Server....</p> <p></p> </li> <li> <p>In tab of Register - Server dialog, follow these steps:</p> <ol> <li> <p>On the General tab, enter \"PostgreSQLSolutionAccelerator\" into the Name field and clear the Connect now option.</p> <p></p> </li> <li> <p>Select the Connection tab and provide your Azure Database for PostgreSQL flexible server instance details for Hostname/address and Username.</p> <ol> <li> <p>Paste the Server name value of your Azure Database for PostgreSQL flexible server into the Host name/address field.</p> </li> <li> <p>The Username value is your Microsoft Entra ID or email.</p> </li> </ol> </li> <li> <p>Select Save.</p> </li> <li> <p>Right-click the newly added PostgreSQLSolutionAccelerator server in the pgAdmin Object Explorer, and select Connect Server in the context menu.</p> <p></p> </li> <li> <p>In the Connect to Server dialog, you will need to provide an access token.</p> <p>To Retrieve Your Microsoft Entra ID Access Token</p> <ol> <li> <p>In VS Code, open a new integrated terminal.</p> </li> <li> <p>At the integrated terminal prompt, execute the following command to generate and output an access token:</p> Bash<pre><code>az account get-access-token --resource-type oss-rdbms --output json | jq -r '.accessToken'\n</code></pre> </li> <li> <p>Copy the output value.</p> <p>The token is a Base64 string. It encodes all the information about the authenticated user and is targeted to the Azure Database for PostgreSQL service.</p> </li> </ol> </li> <li> <p>Return to pgAdmin and the Connect to Server dialog and paste the access token into the password field.</p> <p></p> <p>Do not save password!</p> <p>Ensure the Save Password box in the Connect to Server dialog is unchecked. Checking this box can cause your login to fail.</p> </li> <li> <p>Select OK.</p> <p>Access token expiration</p> <p>If your access token expires during the course of the workshop, you will need to come back and repeat the above steps to reauthenticate.</p> </li> </ol> </li> </ol> <p>Leave pgAdmin open as you will be using it throughout the remainder of the workshop.</p>"},{"location":"02-Setup/1-Provision-And-Setup/06-Validation/","title":"Validate Your Setup","text":"<p>SETUP IS COMPLETE!</p> <p>You just completed the PROVISION and SETUP steps of workshop. </p> <ul> <li> You installed the required tools and software</li> <li> You forked the sample repo and created a local clone</li> <li> You provisioned infrastructure resources on Azure</li> <li> You deployed the REACT UI and Python API to Azure Container Apps</li> <li> You configured your local development environment</li> </ul> <p>Here's a reminder of the Azure Application Architecture you can reference as you check your provisioned Resource Group to enure these resources were created.</p> <p></p> <p>In this section, you will validate your setup before moving on to the next phase of solution development.</p>"},{"location":"02-Setup/1-Provision-And-Setup/06-Validation/#1-inspect-deployed-azure-resources","title":"1. Inspect deployed Azure resources","text":"<p>The Azure Portal allows you to view the resources provisioned on Azure and verify that they are setup correctly</p> <ol> <li> <p>Open a new browser tab and navigate to the link below. You may be prompted to login.</p> <pre><code>https://portal.azure.com/#browse/resourcegroups\n</code></pre> </li> <li> <p>You may be presented with a \"Welcome to Microsoft Azure\" screen. Select Cancel (to dismiss it) or click Get Started (to take an introductory tour of the Azure Portal).</p> </li> <li> <p>You should be taken directly to the Resource Groups page for your subscription. In the list of resource groups, locate the one named <code>rg-postgresql-accelerator</code> (or, if you assigned a different name, find that one). This resource group was created for you as part of the <code>azd up</code> resource deployment. It contains all of the Azure resources required to build and deploy your AI-enable solution.</p> <p>You can use the search filter to reduce the number resource groups displayed.</p> </li> <li> <p>Select your resource group.</p> <p>Review the list of deployed resources.</p> <p>In addition to creating a resource group, the <code>azd up</code> command deployed multiple resources into that resource group, as shown in the table below.</p> Resource type Name Container Registry <code>cr&lt;unique_string&gt;</code> Log Analytics workspace <code>log-&lt;unique_string&gt;</code> Key Vault <code>kv-&lt;unique_string&gt;</code> Document Intelligence <code>di-&lt;unique_string&gt;</code> Language <code>lang-&lt;unique_string&gt;</code> Application Insights <code>appi-&lt;unique_string&gt;</code> Container Apps Environment <code>cae-&lt;unique_string&gt;</code> Storage account <code>st&lt;unique_string&gt;</code> Azure Database for PostgreSQL - Flexible Server <code>psql-data&lt;unique_string&gt;</code> Azure OpenAI <code>openai-&lt;unique_string&gt;</code> Machine Learning Workspace <code>mlw-&lt;unique_string&gt;</code> Machine Learning Endpoint <code>mle-&lt;unique_string&gt;</code> Container App <code>ca-api-&lt;unique_string&gt;</code> Container App <code>ca-portal-&lt;unique_string&gt;</code> <p>The <code>&lt;unique_string&gt;</code> token in the above resource names represents the unique string that is generated by the Bicep scripts when naming your resources. This ensures resources are uniquely named and avoid resource naming collisions.</p> <p>In addition to the above resources, you will also see several other resources, like Managed Identities, that are supporting resources for those in the table.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/06-Validation/#2-ensure-the-deployed-apps-are-running","title":"2. Ensure the deployed apps are running","text":"<p>The <code>azd up</code> command included steps to deploy the Woodgrove Bank application into Azure Container Apps (ACA). Two containers were created. One for the Woodgrove Bank portal UI and a second for the backend API that supports it.</p> <p>Azure Container Apps (ACA) deployment</p> <p>ACA is a fully managed serverless platform that allows you to deploy and manage containerized applications effortlessly. They simplify deployment, offer scalability and cost-effectiveness, and make it easier to focus on building applications without worrying about infrastructure management.</p>"},{"location":"02-Setup/1-Provision-And-Setup/06-Validation/#21-confirm-the-woodgrove-api-is-running","title":"2.1 Confirm the Woodgrove API Is Running","text":"<ol> <li> <p>In the browser window opened to your Azure resource group, select the Container app resource whose name starts with ca-api.</p> <p></p> </li> <li> <p>In the Essentials section of the API Container App's Overview page, select the Application Url to open the deployed Woodgrove Bank API in a new browser tab.</p> <p></p> </li> <li> <p>You should see a <code>Welcome to the Woodgrove Bank API!</code> message on the screen, which serves as confirmation the API app was deployed successfully.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/06-Validation/#22-open-the-woodgrove-portal-ui","title":"2.2 Open the Woodgrove Portal UI","text":"<ol> <li> <p>In the Azure portal, return to the resource group containing your resources and select the Container app resource whose name begins with ca-portal.</p> <p></p> </li> <li> <p>In the Essentials section of the Portal Container App's Overview page, select the Application Url to open the deployed Woodgrove Bank Portal in a new browser tab.</p> <p></p> </li> <li> <p>In the Woodgrove Bank Contract Management Portal, select the Vendors page and verify the list of vendors loads correctly.</p> <p></p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/06-Validation/#3-view-azure-openai-model-deployments-in-azure-ai-foundry","title":"3. View Azure OpenAI model deployments in Azure AI Foundry","text":"<p>The Azure AI Foundry portal lets you view and manage the Azure AI resources for your app.</p> <p>You will use the Azure AI Foundry portal to verify the <code>gpt-4o</code> and <code>text-embedding-ada-002</code> models were deployed into your Azure OpenAI service.</p> <ol> <li> <p>In the Azure portal, return to the resource group containing your resources and select the Azure OpenAI resource.</p> <p></p> </li> <li> <p>On the Azure OpenAI resource's Overview page, select Explore Azure AI Foundry portal.</p> <p></p> </li> <li> <p>In Azure AI Foundry, select the Deployments menu item under Shared resources in the left-hand navigation menu.</p> <p></p> </li> <li> <p>Verify you see a <code>completions</code> deployment for the <code>gpt-4o</code> model and an <code>embeddings</code> deployment for the <code>text-embedding-ada-002</code> model.</p> </li> </ol>"},{"location":"02-Setup/1-Provision-And-Setup/06-Validation/#4-verify-semantic-ranker-model-deployment-optional","title":"4. Verify Semantic Ranker Model Deployment (optional)","text":"<p>If you chose to deploy the Azure ML semantic ranker model during setup, you will use the Azure Machine Learning Studio to ensure the semantic ranker model was successfully deployed to an online endpoint.</p> <ol> <li> <p>In the Azure portal, return to the resource group containing your resources and select the Azure Machine Learning Workspace resoure.</p> <p></p> </li> <li> <p>From the Azure ML workspace page, select the Launch studio button to open Azure Machine Learning Studio in a new browser window.</p> <p></p> </li> <li> <p>Sign into Machine Learning Studio if prompted.</p> </li> <li> <p>In Machine Learning Studio, select Endpoints under Assets in the left-hand resource menu, then select the endpoint for your <code>bge-v2-m3-reranker model</code>:</p> <p></p> </li> <li> <p>On your endpoint page, ensure the Provisioning state for the bgev2m3-v1 deployment is Succeeded.</p> <p></p> </li> </ol> <p>Leave the Azure Portal open. You will revisit it later.</p>"},{"location":"02-Setup/2-Review-Manual-Process/","title":"2. Review Manual Process","text":"<p>Before AI is integrated into the application, let's review the fields and data that would need to be manually entered into the old manual workflow. Keep in mind that a user would need to first read the SOW and Invoice document, before copying and pasting the correct information into the application fields. This manual process can be very time consuming to perform.</p>"},{"location":"02-Setup/2-Review-Manual-Process/#21-review-sows","title":"2.1 Review SOWs","text":"<p>Let's review the application UI for SOWs.</p> <ol> <li> <p>In the Woodgrove Bank Contract Management Portal, select the SOWs page and to view the list of SOWs.</p> <p></p> </li> <li> <p>Locate the SOW-2024-073 SOW, then select the Edit button under the Actions column. This will open the edit page for the SOW.</p> <p></p> </li> <li> <p>On the SOW Edit page, notice the primary fields that need to be entered for the Statement of Work (Vendor, SOW Name, Budget, State Date, and End Date). Also, notice the list of Milestones that must be manually entered for the SOW based on the SOW document that was uploaded.</p> <p></p> </li> <li> <p>Select the Edit button for the Monitoring milestone. This will open the Edit Milestone page, where you can see the required fields for the Milestone as well as the Deliverables that must be manually entered based on the SOW document that was uploaded. After reviewing the fields and deliverables, select the Back to SOW button.</p> <p></p> </li> <li> <p>On the Edit SOW page, notice the Validations history list. These validations have each been manually entered over time as the SOW has bee updated.</p> <p></p> </li> <li> <p>Below the Validations you will see the list of SOW Chunks. These are the titles and content from the various different sections of the SOW document. In the manual workflow, these were manually copy and pasted into the application to extract text from the SOW document. This is entered by a user as they read through and review the SOW document.</p> <p></p> </li> </ol>"},{"location":"02-Setup/2-Review-Manual-Process/#22-review-invoices","title":"2.2 Review Invoices","text":"<p>Let's review the application UI for Invoices.</p> <ol> <li> <p>In the Woodgrove Bank Contract Management Portal, select the Invoices page and to view the list of Invoices. Then select the Edit button for the first Invoice in the list. This will bring up the Invoice Edit page.</p> <p></p> </li> <li> <p>On the Edit Invoice page, review all the different sections of data that must be manually entered for the uploaded invoice document:</p> <ul> <li>Invoice fields (Vendor, SOW Name, Invoice Number, Amount, Invoice Date, Payment Status)</li> <li>Line Items must be extracted from the Invoice document and manually entered.</li> <li>Validations are manually entered by the user as they manually review the uploaded invoice document.</li> </ul> <p></p> </li> <li> <p>Within the Validations, notice the Result entered manually is generally very short and does not contain very much information.</p> <p></p> <p>Something you will notice as you integrate AI into the application, in later sections, is that Validation summaries generated by AI will be much more informative.</p> </li> </ol> <p>Congratulations! You have reviews the SOW and Invoice data fields that are entered manually for the old workflow. In the next steps, you will work to integrate an AI-powered workflow!</p>"},{"location":"03-Integrate-AI-Into-PostgreSQL/","title":"Integrate Generative AI into Azure Database for PostgreSQL - Flexible Server","text":"<p>Generative AI (GenAI) represents a cutting-edge class of AI algorithms designed to create new, original content based on patterns and data learned from existing information. Natural language processing (NLP) is a key part of this. NLP allows generative AI to understand and produce human language, making it capable of tasks like summarizing large blocks of text, translating languages, or conversing with people. Using NLP, generative AI can create content that sounds natural and makes sense in context. By employing techniques like prompting and retrieval augmented generation (RAG), GenAI can produce innovative outputs, such as text, images, and more.</p> <p>Incorporating Generative AI (GenAI) within Azure Database for PostgreSQL - Flexible Server is accomplished through the Azure AI (<code>azure_ai</code>) and pgvector (<code>vector</code>) extensions. The <code>azure_ai</code> extension enables the integration of large language models (LLMs) directly within your database, providing seamless interaction with Azure's advanced AI services, such as Azure OpenAI and Azure Cognitive Services. With these integrations, you can elevate your applications by embedding robust AI functionalities directly into your database infrastructure. The <code>vector</code> extension works with the <code>azure_ai</code> extension, allowing vector embeddings to be generated in database queries, then stored and queried in the database. It also enables powerful vector similarity search capabilities.</p> <p>In addition, the <code>pg_diskann</code> extension enables DiskANN support for efficient vector indexing and searching. DiskANN is a scalable approximate nearest neighbor search algorithm for efficient vector search at any scale. It offers high recall, high queries per second, and low query latency, even for billion-point datasets.</p> <p>In this section, you will use extensions to enhance your PostgreSQL database with Generative AI and Vector Search capabilities. Here's what you will accomplish:</p> <ul> <li> Install the <code>azure_ai</code>, <code>pg_diskann</code>, and <code>vector</code> extensions on your PostgreSQL database</li> <li> Configure the <code>azure_ai</code> extension with the connection details for your Azure AI services</li> <li> Add vector columns to database tables to allow embeddings to be stored alongside text data</li> <li> Improve vector query performance with DiskANN</li> <li> Generate and store embeddings for existing data</li> </ul> <p>Following these steps will transform your PostgreSQL database into a powerful AI-enhanced platform capable of executing advanced generative AI tasks and providing deeper insights from your data.</p>"},{"location":"03-Integrate-AI-Into-PostgreSQL/01/","title":"3.1 Install extensions","text":"<p>Azure Database for PostgreSQL flexible server allows you to extend the functionality of your database using extensions. Extensions bundle multiple related SQL objects into a single package that can be loaded or removed from your database with a single command. After being loaded into the database, extensions function like built-in features.</p>"},{"location":"03-Integrate-AI-Into-PostgreSQL/01/#allowlist-the-extensions","title":"Allowlist the extensions","text":"<p>Before installing and using extensions in Azure Database for PostgreSQL flexible server, you must add them to the server's allowlist, as described in how to use PostgreSQL extensions.</p> <p>Select the tab of the method you want to use for allowlisting the extensions and follow the instructions provided.</p> Azure CLIAzure portal <ol> <li>Open a new integrated terminal window in VS Code and execute the following Azure CLI command at the prompt.</li> </ol> <p>Ensure you replace the tokens in the command below with the appropriate values from your Azure environment.</p> <ul> <li>[YOUR_RESOURCE_GROUP]: The name of the resource group hosting your Azure Database for PostgreSQL flexible server.</li> <li>[YOUR_POSTGRESQL_SERVER]: The name of your Azure Database for PostgreSQL server.</li> <li>[YOUR_SUBSCRIPTION_ID]: Your Azure subscription ID.</li> </ul> Bash<pre><code>az postgres flexible-server parameter set --resource-group [YOUR_RESOURCE_GROUP] \u00a0--server-name [YOUR_POSTGRESQL_SERVER] --subscription [YOUR_SUBSCRIPTION_ID] --name azure.extensions --value azure_ai,pg_diskann,vector\n</code></pre> <ol> <li> <p>Navigate to your Azure Database for PostgreSQL flexible server instance in the Azure portal.</p> </li> <li> <p>From the left-hand resource menu:</p> <ol> <li>Expand the Settings section and select Server parameters.</li> <li>Enter \"azure.extensions\" into the search filter.</li> <li>Select the AZURE_AI, PG_DISKANN, and VECTOR extensions by checking the box for each in the VALUE dropdown list.</li> <li>Select Save on the toolbar.</li> </ol> <p></p> </li> </ol>"},{"location":"03-Integrate-AI-Into-PostgreSQL/01/#install-extensions","title":"Install extensions","text":"<p>With the required extensions added to the allowlist, you can now install them in your database. To enable them, you will run a CREATE EXTENSION command for each in PostgreSQL.</p> <p><code>CREATE EXTENSION</code> loads a new extension into the database by running its script file. This script typically creates new SQL objects such as functions, data types, and schemas. An error is thrown if an extension of the same name already exists, so adding <code>IF NOT EXISTS</code> allows the command to execute without throwing an error if it is already installed.</p> <p>You will use pgAdmin to install the extensions by executing SQL commands against your database.</p> <ol> <li> <p>On your local machine, return to the open instance of pgAdmin (or open it if you closed it after the setup tasks) and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> </ol> <p>Select each of the tabs below and execute the <code>CREATE EXTENSION</code> command in the pgAdmin query window to install the extensions.</p> Azure AI extensionpgvector extensionDiskANN extension <p>The Azure AI (<code>azure_ai</code>) extension transforms your database into an AI-powered platform. It lets you connect directly with Azure's AI services, such as Azure OpenAI and Azure Cognitive Services, from your PostgreSQL database and incorporate advanced functionalities like natural language processing, text analysis, and embedding generation into your database operations. This integration simplifies the development process, enabling seamless interaction with Azure's AI tools and enhancing your database with cutting-edge AI features.</p> <ol> <li> <p>Enable the <code>azure_ai</code> extension:</p> SQL<pre><code>CREATE EXTENSION IF NOT EXISTS azure_ai;\n</code></pre> </li> </ol> <p>The pgvector (<code>vector</code>) extension adds advanced vector operations to your PostgreSQL database. It is designed to facilitate vector similarity searches by enabling the storage, indexing, and querying of vector data directly within PostgreSQL. This extension provides more complex and meaningful data retrieval based on vector similarity.</p> <ol> <li> <p>Create the <code>vector</code> extension:</p> SQL<pre><code>CREATE EXTENSION IF NOT EXISTS vector;\n</code></pre> </li> </ol> <p>The DiskANN (<code>pg_diskann</code>) extension adds support for using DiskANN for efficient vector indexing and searching. Once the extension is installed, you can create <code>diskann</code> indexes on table columns that contain vector data.</p> <ol> <li> <p>Create the <code>pg_diskann</code> extension:</p> SQL<pre><code>CREATE EXTENSION IF NOT EXISTS pg_diskann;\n</code></pre> </li> </ol> <p>The DiskANN extension is dependent on the pgvector extension.</p> <p>Because <code>pg_diskann</code> has a dependency on the <code>vector</code> extension, you must first allow and create the <code>vector</code> extension in the same database. Alternatively, you can use the <code>CASCADE</code> clause in the above query, which will cause PostgreSQL to implicitely run <code>CREATE EXTENSION</code> on the extension's dependencies.</p>"},{"location":"03-Integrate-AI-Into-PostgreSQL/02/","title":"3.2 Configure the Azure AI extension","text":"<p>The <code>azure_ai</code> extension lets you directly integrate the Azure OpenAI, Azure AI Language, and Azure ML services into your database. To start using the extension's capabilities, you must first configure its connection to your Azure AI and ML services, providing each service's endpoint and subscription key.</p>"},{"location":"03-Integrate-AI-Into-PostgreSQL/02/#execute-sql-in-pgadmin-to-configure-the-extension","title":"Execute SQL in pgAdmin to configure the extension","text":"<p>You will use pgAdmin to configure the <code>azure_ai</code> extension by executing SQL commands against your database.</p> <ol> <li> <p>On your local machine, return to the open instance of pgAdmin (or open it if you closed it after the setup tasks) and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> </ol> <p>Select each tab below and execute SQL statements provided to connect to each Azure AI service.</p> Azure OpenAILanguage serviceAzure ML <p>The Azure AI extension includes the <code>azure_openai</code> schema, which allows you to integrate the creation of vector representations of text values directly into your database by invoking Azure OpenAI embeddings. The vector embeddings can then be used in vector similarity searches.</p> <ol> <li> <p>In the new pgAdmin query window, paste the following SQL commands to configure the extension's connection to Azure OpenAI using the <code>set_setting()</code> function. Do not run the commands yet, as you first need to retrieve the endpoint and API key for your Azure OpenAI resource.</p> SQL<pre><code>SELECT azure_ai.set_setting('azure_openai.endpoint', '&lt;endpoint&gt;');\nSELECT azure_ai.set_setting('azure_openai.subscription_key', '&lt;api-key&gt;');\n</code></pre> </li> <li> <p>In a browser window, navigate to your Azure OpenAI service in the Azure portal.</p> </li> <li> <p>On the Azure OpenAI service page:</p> <ol> <li> <p>Select the Keys and Endpoint menu item under Resource Management.</p> </li> <li> <p>Copy the Endpoint value, paste it as the <code>&lt;endpoint&gt;</code> value in the query to set the <code>azure_openai.endpoint</code> value in your pgAdmin query window.</p> </li> <li> <p>Copy the KEY 1 value, paste it as the <code>&lt;api-key&gt;</code> value in the query to set the <code>azure_openai.subscription_key</code> value in your pgAdmin query window.</p> </li> </ol> <p></p> </li> <li> <p>In pgAdmin, execute the updated SQL commands by selecting the Execute script button.</p> <p></p> </li> <li> <p>The <code>azure_ai</code> extension also provides the <code>get_setting()</code> function, allowing users with appropriate permissions to view the values stored in each schema's <code>endpoint</code> and <code>key</code> settings. Run the following queries to view the Azure OpenAI endpoint and key values stored in the database.</p> <pre><code>select azure_ai.get_setting('azure_openai.endpoint');\n</code></pre> <pre><code>select azure_ai.get_setting('azure_openai.subscription_key');\n</code></pre> </li> </ol> <p>The Azure AI services integrations included in the <code>azure_cognitive</code> schema of the <code>azure_ai</code> extension provide a rich set of AI Language features accessible directly from the database.</p> <ol> <li> <p>In the pgAdmin query window, overwrite the previous commands by pasting the following SQL commands to configure the extension's connection to your Language service. Do not run the commands yet, as you first need to retrieve your service's endpoint and API key.</p> SQL<pre><code>SELECT azure_ai.set_setting('azure_cognitive.endpoint', '&lt;endpoint&gt;');\nSELECT azure_ai.set_setting('azure_cognitive.subscription_key', '&lt;api-key&gt;');\n</code></pre> </li> <li> <p>In a browser window, navigate to your Language service in the Azure portal.</p> </li> <li> <p>On the Language service page:</p> <ol> <li> <p>Select the Keys and Endpoint menu item under Resource Management.</p> </li> <li> <p>Copy the Endpoint value, paste it as the <code>&lt;endpoint&gt;</code> value in the query to set the <code>azure_cognitive.endpoint</code> value in your pgAdmin query window.</p> </li> <li> <p>Copy the KEY 1 value, paste it as the <code>&lt;api-key&gt;</code> value in the query to set the <code>azure_cognitive.subscription_key</code> value in your pgAdmin query window.</p> </li> </ol> <p></p> </li> <li> <p>In pgAdmin, execute the updated SQL commands by selecting the Execute script button.        </p> </li> </ol> <p>The Azure AI extension allows you to invoke any machine learning models deployed on Azure Machine Learning (ML) online endpoints from within SQL. These models can be from the Azure ML catalog or custom models that have been trained and deployed.</p> <ol> <li> <p>In the pgAdmin query window, overwrite the previous commands by pasting the following SQL commands to configure the extension's connection to Azure ML. Do not run the commands yet, as you first need to retrieve the endpoint and key for the model deployed on Azure ML.</p> SQL<pre><code>SELECT azure_ai.set_setting('azure_ml.scoring_endpoint','&lt;endpoint&gt;');\nSELECT azure_ai.set_setting('azure_ml.endpoint_key', '&lt;api-key&gt;');\n</code></pre> </li> <li> <p>In a browser window, navigate to your Azure ML workspace in the Azure portal.</p> </li> <li> <p>From the Azure ML workspace page, select the Launch studio button to open Azure Machine Learning Studio in a new browser window.</p> <p></p> </li> <li> <p>Sign into Machine Learning Studio if prompted.</p> </li> <li> <p>In Machine Learning Studio, select Endpoints under Assets in the left-hand resource menu, then select the endpoint for your <code>bge-v2-m3-reranker model</code>:</p> <p></p> </li> <li> <p>On your endpoint page:</p> </li> <li> <p>Select the Consume tab.</p> </li> <li>Copy the REST endpoint value, paste it as the <code>&lt;endpoint&gt;</code> value in the query to set the <code>azure_ml.scoring_endpoint</code> value in your pgAdmin query window.</li> <li> <p>Copy the Primary key value, paste it as the <code>&lt;api-key&gt;</code> value in the query to set the <code>azure_ml.endpoint_key</code> value in your pgAdmin query window.</p> <p></p> </li> <li> <p>In pgAdmin, execute the updated SQL commands by selecting the Execute script button.</p> </li> </ol> <p>Be sure to run the scripts to add the endpoints and keys for all three services (Azure OpenAI, Language Service, and Azure Machine Learning) in the database.</p>"},{"location":"03-Integrate-AI-Into-PostgreSQL/03/","title":"3.3 Enable vector storage","text":"<p>The pgvector extension is a cutting-edge addition to PostgreSQL that empowers the database with the ability to handle vector data natively. This feature allows you to store and query vector information, making it ideal for applications involving copilots, recommendation systems, and similarity searches. To enable vector embeddings to be stored alongside the rest of your data in a PostgreSQL database, you must alter the tables in which you want to store embeddings to add columns with the <code>vector</code> data type provided by the extension.</p>"},{"location":"03-Integrate-AI-Into-PostgreSQL/03/#what-are-vectors","title":"What are vectors?","text":"<p>Vectors, also referred to as embeddings or vector embeddings, are mathematical structures that represent data in a high-dimensional space. Each dimension in this space corresponds to a particular feature of the data, and complex data might be represented using tens of thousands of these dimensions. The location of a vector in this space captures its unique attributes. Various types of data, including words, phrases, entire documents, images, audio, and more, can be transformed into vectors. By using vector search, it's possible to find similar data across different types thanks to this uniform representation.</p> <p>An embedding is a dense and informative data format that machine learning models and algorithms can effectively leverage. Embeddings encapsulate the semantic meaning of a piece of text in the form of a vector of floating-point numbers. Consequently, the proximity of two embeddings in the vector space reflects the semantic similarity between the original inputs.</p>"},{"location":"03-Integrate-AI-Into-PostgreSQL/03/#add-vector-columns-to-tables","title":"Add vector columns to tables","text":"<p>The <code>vector</code> data type installed by the pgvector extension allows for the efficient storage and manipulation of high-dimensional numerical vectors in PostgreSQL databases. These vectors are arrays of numbers representing a wide range of data, such as text embeddings, image features, or user preferences.</p> <p>Below, you will <code>vector</code> columns to the <code>deliverables</code>, <code>invoice_line_items</code>, <code>invoice_validation_results</code>,  <code>sow_chunks</code>, and <code>sow_validation_results</code> tables. Each of these tables contains a column storing a block of text. The vector embeddings will be stored alongside the text that they represent.</p> <p>The size of the vector column should correspond to the number of dimensions generated by the embedding model being used. For this solution, you use OpenAI's <code>text-embedding-ada-002</code> model, configured to return 1,536 dimensions. You will see this number represented in the size of the vector columns added to the tables below.</p> <p>Using pgAdmin, execute the SQL statement for each table.</p> <p>Select the tab for each table below and execute the <code>ALTER TABLE</code> statement to create an <code>embedding</code> column for storing vector data.</p> deliverablesinvoice_line_itemsinvoice_validation_resultssow_chunkssow_validation_results <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query to add the embedding vector column.</p> SQL<pre><code>ALTER TABLE deliverables ADD COLUMN IF NOT EXISTS embedding vector(1536);\n</code></pre> <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query to add the embedding vector column.</p> SQL<pre><code>ALTER TABLE invoice_line_items ADD COLUMN IF NOT EXISTS embedding vector(1536);\n</code></pre> <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query to add the embedding vector column.</p> SQL<pre><code>ALTER TABLE invoice_validation_results ADD COLUMN IF NOT EXISTS embedding vector(1536);\n</code></pre> <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query to add the embedding vector column.</p> SQL<pre><code>ALTER TABLE sow_chunks ADD COLUMN IF NOT EXISTS embedding vector(1536);\n</code></pre> <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query to add the embedding vector column.</p> SQL<pre><code>ALTER TABLE sow_validation_results ADD COLUMN IF NOT EXISTS embedding vector(1536);\n</code></pre> <p>Be sure to run the <code>ALTER TABLE</code> statements for each of the tables to add all the embeddings columns before moving on to the next step.</p>"},{"location":"03-Integrate-AI-Into-PostgreSQL/04/","title":"3.4 Optimize Vector Query Performance","text":"<p>The <code>vector</code> extension adds vector storage and similarity search capabilities to Azure Database for PostgreSQL flexible server. To improve efficiency when executing vector searches, you can use the <code>pg_diskann</code> extension, which allows you to leverage DiskANN to create indexes on tables containing a vector column. Indexing can help improve the performance of vector queries against the database. Without indexes, the <code>vector</code> extension performs an exact search, which provides perfect recall at the expense of performance. To perform approximate nearest neighbor searches, you can create DiskANN indexes on your data, which offers high recall, high queries per second, and low query latency, even for billion-point datasets.</p> <p>Always load your data before indexing it.</p> <p>Loading data before indexing creates the index faster and results in a more optimal layout.</p>"},{"location":"03-Integrate-AI-Into-PostgreSQL/04/#index-types","title":"Index Types","text":"<p>There are three supported vector index types, each with its own pros and cons. You can learn more about the index types using these links:</p> <ul> <li>Disk Approximate Nearest Neighbor (DiskANN)</li> <li>Inverted File with Flat Compression (IVVFlat)</li> <li>Hierarchical Navigable Small Worlds (HNSW)</li> </ul> <p>The <code>IVFFlat</code> index has faster build times and uses less memory than <code>HNSW</code>, but it has lower query performance (in terms of the speed-recall tradeoff). <code>DiskANN</code> offers a great balance between highly accurate query performance and fast build times.</p> Index Type Limitations <ul> <li>To index a column, it must have dimensions defined. Attempting to index a column defined as <code>col vector</code> will result in the error: <code>ERROR: column does not have dimensions</code>.</li> <li>When using <code>ivfflat</code> and <code>hnsw</code>, you are limited to indexing columns with 2000 dimensions or less. Attempting to index a column with more dimensions results in an error: <code>ERROR: column cannot have more than 2000 dimensions for INDEX_TYPE index where INDEX_TYPE is either ivfflat or hnsw</code>.</li> </ul>"},{"location":"03-Integrate-AI-Into-PostgreSQL/04/#index-access-and-vector-distance-functions","title":"Index Access and Vector Distance Functions","text":"<p>The <code>vector</code> type added to your database by the <code>vector</code> extension allows you to perform three types of searches on the stored vectors. You need to select the correct access function for your index to have the database consider your index when executing your queries.</p> Distance Function Index Access Function Vector Operator Description Cosine distance <code>vector_cosine_ops</code> <code>&lt;=&gt;</code> Measures similarity between vectors by calculating the cosine of the angle between them. L2 (or Euclidean) distance <code>vector_l2_ops</code> <code>&lt;-&gt;</code> Measures the straight-line distance between two vectors in multi-dimensional space. Inner (dot) product <code>vector_ip_ops</code> <code>&lt;#&gt;</code> Measures the similarity between two vectors by summing the products of their corresponding components."},{"location":"03-Integrate-AI-Into-PostgreSQL/04/#vector-operators","title":"Vector Operators","text":"<p>The vector operators in the table above indicate the \"distance operator\" used to calculate the distances between two vectors in a multi-dimensional space. They are used when writing queries to calculate similarity. For example, in the following query, records are selected based on the similarity to the provided vector, where the cosine distance similarity is less than 0.5:</p> <pre><code>SELECT * FROM sow_chunks\nWHERE embedding &lt;=&gt; '[-0.031766646,-0.033289704,...,0.016508864,0.031440277]' &lt; 0.5\n</code></pre>"},{"location":"03-Integrate-AI-Into-PostgreSQL/04/#create-vector-indexes","title":"Create Vector Indexes","text":"<p>In the previous tasks, you added <code>vector</code> columns to several tables in your database and populated them with embeddings using the <code>azure_ai</code> extension and Azure OpenAI. To ensure queries over those embeddings are efficient, you will add <code>diskann</code> indexes to each table to which you added a <code>vector</code> column.</p> <p>Select the tab for each table below and execute the <code>CREATE INDEX</code> statement to create an <code>diskann</code> index.</p> deliverablesinvoice_line_itemsinvoice_validation_resultssow_chunkssow_validation_results <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query to add the index.</p> SQL<pre><code>-- Create a diskann index by using Cosine distance operator\nCREATE INDEX deliverables_diskann_idx ON deliverables USING diskann (embedding vector_cosine_ops);\n</code></pre> <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query to add the index.</p> SQL<pre><code>-- Create a diskann index by using Cosine distance operator\nCREATE INDEX line_items_diskann_idx ON invoice_line_items USING diskann (embedding vector_cosine_ops);\n</code></pre> <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query to add the index.</p> SQL<pre><code>-- Create a diskann index by using Cosine distance operator\nCREATE INDEX invoice_validation_results_diskann_idx ON invoice_validation_results USING diskann (embedding vector_cosine_ops);\n</code></pre> <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query to add the index.</p> SQL<pre><code>-- Create a diskann index by using Cosine distance operator\nCREATE INDEX sow_chunks_diskann_idx ON sow_chunks USING diskann (embedding vector_cosine_ops);\n</code></pre> <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query to add the index.</p> SQL<pre><code>-- Create a diskann index by using Cosine distance operator\nCREATE INDEX sow_validation_results_diskann_idx ON sow_validation_results USING diskann (embedding vector_cosine_ops);\n</code></pre> <p>Be sure to run the <code>CREATE INDEX</code> statements for each of the tables before moving on to the next step.</p>"},{"location":"03-Integrate-AI-Into-PostgreSQL/05/","title":"3.5 Vectorize data","text":"<p>By leveraging an embedding model, such as Azure OpenAI's <code>text-embedding-ada-002</code>, you can generate vector representations of textual data and store them in a vector store like Azure Database for PostgreSQL - Flexible Server. This approach facilitates efficient and accurate similarity searches, significantly enhancing the copilot's ability to retrieve relevant information and provide contextually rich interactions.</p> <p>The <code>azure_openai</code> schema installed by the <code>azure_ai</code> extension contains the <code>create_embeddings()</code> function. This function enables you to generate embeddings for text input by invoking an embedding model deployed in Azure OpenAI directly from a query.</p> Function signatures for the create_embeddings() function<pre><code>-- Single text input\nazure_openai.create_embeddings(deployment_name text, input text, timeout_ms integer DEFAULT 3600000, throw_on_error boolean DEFAULT true, max_attempts integer DEFAULT 1, retry_delay_ms integer DEFAULT 1000)\n\n-- Array of input text\nazure_openai.create_embeddings(deployment_name text, input text[], batch_size integer DEFAULT 100, timeout_ms integer DEFAULT 3600000, throw_on_error boolean DEFAULT true, max_attempts integer DEFAULT 1, retry_delay_ms integer DEFAULT 1000)\n</code></pre> <p>Learn more about the <code>create_embeddings()</code> function, its overloads, and expected arguments in the function documentation.</p>"},{"location":"03-Integrate-AI-Into-PostgreSQL/05/#generate-embeddings","title":"Generate embeddings","text":"<p>The <code>azure_ai</code> extension makes calling the Azure OpenAI embedding API trivial. In its simplest form, the <code>create_embeddings()</code> function can be called with two arguments, <code>deployment_name</code> and <code>input</code>, as shown below:</p> SQL<pre><code>SELECT azure_openai.create_embeddings(deployment_name, input)\n</code></pre> <p>To demonstrate how to generate vector embeddings through a SQL query, execute the following query in pgAdmin.</p> <ol> <li> <p>Return to the open instance of pgAdmin on your local machine and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Run the following query, which creates a vector embedding for the <code>result</code> field in the <code>invoice_validation_results</code> table. The <code>deployment_name</code> parameter in the function is set to <code>embeddings</code>, which is the name of the deployment of the <code>text-embedding-ada-002</code> model in your Azure OpenAI service (it was created with that name by the Bicep deployment script):</p> SQL<pre><code>SELECT \n    invoice_id,\n    azure_openai.create_embeddings('embeddings', result) AS embedding\nFROM invoice_validation_results\nLIMIT 1;\n</code></pre> <p>You can view the deployment name of your embedding model in Azure AI Foundry.</p> <ol> <li>Open Azure AI Foundry from the landing page of your Azure OpenAI service.</li> <li>In Azure AI Foundry, select the Deployments option from the resource navigation menu.</li> <li> <p>Observe the Name associated with the <code>text-embedding-ada-002</code> model.</p> <p></p> </li> </ol> </li> <li> <p>The results of the query will look similar to this:</p> SQL<pre><code>   id |\u00a0          \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0     embedding\n------+-------------------------------------------------------------------------\n    1\u00a0| {-0.031766646,-0.033289704,0.0009468119,...,0.016508864,0.031440277}\n</code></pre> <p>The 1,536 dimensions in the vector are abbreviated in the above output for brevity.</p> </li> </ol>"},{"location":"03-Integrate-AI-Into-PostgreSQL/05/#vectorize-existing-data","title":"Vectorize existing data","text":"<p>You added <code>vector</code> columns to the <code>deliverables</code>, <code>invoice_line_items</code>, <code>invoice_validation_results</code>, \u00a0<code>sow_chunks</code>, and <code>sow_validation_results</code> tables. You will now use the <code>azure_openai.create_embeddings()</code> function in SQL <code>UPDATE</code> statements to generate embeddings for the text data already in each table.</p> <p>Each table query may take several minutes, depending on the configured TPM limits.</p> <p>Using pgAdmin, execute the SQL statement for each table.</p> <p>Select the tab for each table below and execute the <code>UPDATE</code> statement to create embeddings for the specified column.</p> deliverablesinvoice_line_itemsinvoice_validation_resultssow_chunkssow_validation_results <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query.</p> SQL<pre><code>UPDATE deliverables\nSET embedding = azure_openai.create_embeddings('embeddings', description, max_attempts =&gt; 5, retry_delay_ms =&gt; 500)\nWHERE embedding IS NULL;\n</code></pre> <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query.</p> SQL<pre><code>UPDATE invoice_line_items\nSET embedding = azure_openai.create_embeddings('embeddings', description, max_attempts =&gt; 5, retry_delay_ms =&gt; 500)\nWHERE embedding IS NULL;\n</code></pre> <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query.</p> SQL<pre><code>UPDATE invoice_validation_results\nSET embedding = azure_openai.create_embeddings('embeddings', result, max_attempts =&gt; 5, retry_delay_ms =&gt; 500)\nWHERE embedding IS NULL;\n</code></pre> <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query.</p> SQL<pre><code>UPDATE sow_chunks\nSET embedding = azure_openai.create_embeddings('embeddings', content, max_attempts =&gt; 5, retry_delay_ms =&gt; 500)\nWHERE embedding IS NULL;\n</code></pre> <p>Copy and paste the following SQL statement into a new query window in pgAdmin, then execute the query.</p> SQL<pre><code>UPDATE sow_validation_results\nSET embedding = azure_openai.create_embeddings('embeddings', result, max_attempts =&gt; 5, retry_delay_ms =&gt; 500)\nWHERE embedding IS NULL;\n</code></pre> <p>Be sure to run the <code>UPDATE</code> statements for each table to create all the embeddings before moving on to the next step.</p> <p>Generate Embeddings on Database <code>INSERT</code> using a Trigger</p> <p>The code for this application performs the calls to the <code>azure_openai</code> extension to generate the vector embedding directly within the database <code>INSERT</code> command. There are times where it's more useful to use an <code>INSERT</code> trigger on the table. The trigger will automatically generate the embeddings anytime a row is inserted into table.</p> <p>The following is an example of what an <code>INSERT</code> trigger for generating vector embedding on the <code>sow_chunks</code> table might look:</p> Example INSERT Trigger to Generate Embeddings<pre><code>-- create function that generates embeddings\nCREATE OR REPLACE FUNCTION sow_chunks_insert_trigger_fn()\nRETURNS trigger AS $$\nBEGIN\n  IF NEW.content IS NOT NULL THEN\n    NEW.embeddings := azure_openai.create_embeddings('embeddings', NEW.content, throw_on_error =&gt; FALSE, max_attempts =&gt; 1000, retry_delay_ms =&gt; 2000);\n  END IF;\n  RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n-- setup INSERT TRIGGER to call function\nCREATE TRIGGER sow_chunks_insert_trigger\nBEFORE INSERT ON sow_chunks\nFOR EACH ROW\nEXECUTE FUNCTION sow_chunks_insert_trigger_fn();\n</code></pre>"},{"location":"04-Create-AI-Pipeline/","title":"AI-driven Data Validation","text":""},{"location":"04-Create-AI-Pipeline/#building-an-ai-enhanced-data-ingestion-and-processing-pipeline","title":"Building an AI-Enhanced Data Ingestion and Processing Pipeline","text":"<p>An end-to-end data pipeline has been created to create an AI-powered solution for advanced data analysis. This pipeline starts with documents uploaded to Azure Blob Storage and employs Azure services for intelligent ingestion, automated validation, semantic analysis, and optimized storage.</p> <p>AI-driven Data Validation offers a transformative solution by automating document processing, improving accuracy, and reducing the burden on human reviewers. Through a combination of intelligent document ingestion, machine learning, and natural language processing, AI will revolutionize how contract validation is performed.</p> <p>Prior to the integration of AI into the data ingestion workflow, the application would require a lot of manual parsing and input of Invoice and SOW data into the application. This can be a time consuming and labor intensive process.</p> <p>In this section, you will review the code that integrates AI components into the solution.</p>"},{"location":"04-Create-AI-Pipeline/#validate-documents","title":"Validate documents","text":"<p>The document ingestion workflow process performs document text extraction, validation on document parts, comparing milestone pricing with invoiced amounts and work performed, looking for key SOW components such as compliance sections and wording, etc.</p> <ol> <li> <p>Use Azure AI Document Intelligence to perform document text extraction, and Azure OpenAI to generate text embeddings of document chunks/sections that are inserted into the database.</p> </li> <li> <p>Use Azure OpenAI and GPT-4o model along with PostgreSQL vector extensions to do semantic similarity comparisons between key sections and expected language. Set a threshold similarity score to validate documents contain appropriate language.</p> <ul> <li>Provide example document(s) with missing sections or incorrect and missing wording to show how these can be identified and flagged.</li> <li>Provide good documents.</li> </ul> </li> <li> <p>Insert text and associated embeddings into PostgreSQL.</p> </li> <li> <p>Perhaps use a custom ML model to do numerical comparisons/analysis of project milestones + assigned dollar amount against invoices for the project?</p> <ul> <li>Try with Azure OpenAI first, but it's not so good with numbers...</li> </ul> </li> </ol> Document validation worker process architecture <p>The best practice for enterprise systems is to build a background worker process that performs the document validation workflow. Then when a document is uploaded a message is sent using a queue, like Event Grid, that is then handled by the background worker process to perform the document ingestion and validation workflow.</p> <p>To make this guide easier to follow, the document validation workflow has been implemented as REST API methods that are called directly by the front-end application. The following sections will guide you through the review of the AI integration code into the application.</p>"},{"location":"04-Create-AI-Pipeline/01/","title":"4.1 Extract Data with Document Intelligence","text":"<p>Azure Document Intelligence enables automated data extraction and processing from documents using AI. This section provides details on the AI-enhanced data ingestion and processing pipeline.</p> <p>Using your own data?</p> <p>If you are using your own data, you must copy the structure and format of '../scripts/sql/deploy-database-tables.sql' and incorporate INSERT statements for vendors, sows, sow_chunks, milestones, deliverables, sow_validation_results, invoices, invoice_line_items and invoice_validation_results.</p>"},{"location":"04-Create-AI-Pipeline/01/#overview-of-document-upload-and-ingestion-workflow","title":"Overview of Document Upload and Ingestion Workflow","text":"<ol> <li> <p>The process begins with financial documents, such as invoices and statements of work (SOWs), uploaded through the application UI.</p> </li> <li> <p>The setup of infrastructure seeds the database with sample data for vendors, statement of works and invoices.</p> </li> <li> <p>Sample documents to load into Document Intelligence have been provided within the repository and are located at '../data/sample_docs'.</p> <ol> <li>You can load invoice documents from the 'Exercise_1_Load_Invoices' folder to complete the invoices for the Trey Research vendor.</li> <li>You can then load a Statement of Work and 3 invoices from the 'Exercise_2_Load_SOW_Invoices' folder for the Lucerne Publishing vendor.</li> <li>To load in bad data - there are documents loacted in 'Exercise_3_Load_Bad_SOWandInv' folder.</li> </ol> </li> </ol> <p>!!! info If you want to generate your own documents there is a document generator located in '../data/data_generator/src'. Follow the instructions in the README document. You need to have installed python and installed the necessary dependencies listed in requirements.txt to run the scripts.</p> <ol> <li> <p>When new documents are uploaded they are saved to Azure Blob Storage at the start of the data ingestion workflow.</p> </li> <li> <p>The data ingestion workflow handles data extraction and processing by sending uploaded documents to Azure AI Document Intelligence service.</p> </li> <li> <p>The solution uses pre-built AI models within Document Intelligence which analyze data fields, such as project deliverables, due dates, billable amounts, and vendor details. These models are designed for specific document types, such as invoices, SOWs, contracts, receipts, and identity documents, and leverage AI-powered optical character recognition (OCR) and natural language processing (NLP) to extract relevant data.</p> </li> <li> <p>Once the document is analyzed, the model:</p> <ol> <li>Extracts key fields and assigns confidence scores (e.g., \"Invoice Total: $1,250.00 (Confidence: 0.98)\").</li> <li>Processes tables and line items using structured JSON or key-value pairs.</li> <li>Recognizes relationships between extracted entities (e.g., linking an invoice number to its corresponding due date).</li> </ol> </li> <li> <p>Document Intelligence's Semantic Chunking capability recognizes document structures, capturing headings and chunking the content body based on semantic coherence, such as paragraphs and sentences. This ensures that the chunks are of higher quality for use in Retrieval-Augmented Generation (RAG) pattern queries.</p> </li> <li> <p>Once extracted, the document data is securely stored in Azure Database for PostgreSQL flexible server.</p> </li> <li> <p>As part of the database insert statement, the GenAI capabilities of the <code>azure_ai</code> extension are leveraged to:</p> <ul> <li>Generate and save vector embeddings of document text using Azure OpenAI, enabling efficient semantic search and retrieval.</li> <li>Create abstractive summaries of Statements of Work (SOWs) using the Azure AI Language service, distilling key insights from lengthy documents into concise, actionable summaries.</li> </ul> </li> <li> <p>If you have your own documents then you can use the application UI to load them into blob storage which will trigger the ingestion pipeline into AI services.</p> </li> </ol>"},{"location":"04-Create-AI-Pipeline/01/#ai-enhanced-data-ingestion-and-validation","title":"AI-Enhanced Data Ingestion and Validation","text":"<p>Azure Document Intelligence extracts structured and unstructured data while applying AI-driven validation to ensure accuracy.</p> <p>Automated Text Extraction: Identifies key fields such as transaction amounts and customer details. AI-Driven Data Validation: Cross-references extracted data with predefined rules to ensure compliance. Semantic Chunking: Segments large documents into meaningful parts for efficient processing.</p>"},{"location":"04-Create-AI-Pipeline/01/#benefits-of-the-enhanced-pipeline","title":"Benefits of the Enhanced Pipeline","text":"<ul> <li>High-Quality Data: AI-driven validation ensures accuracy.</li> <li>Scalability: Handles large document volumes efficiently.</li> <li>Semantic Understanding: Enables AI-powered search and analytics.</li> <li>End-to-End Automation: Streamlines document processing workflows.</li> </ul> <p>By leveraging Azure Document Intelligence, unstructured documents can be transformed into actionable insights efficiently.</p>"},{"location":"04-Create-AI-Pipeline/01/#references","title":"References","text":"<p>Azure AI Document Intelligence Documentation</p> <p>Azure Document Intelligence code samples</p>"},{"location":"04-Create-AI-Pipeline/02/","title":"4.2 Configure Semantic Chunking","text":"<p>Semantic chunking is a critical step in preparing documents for retrieval-augmented generation (RAG) and other AI-driven workflows. It involves breaking down large documents into smaller, semantically meaningful chunks that enable better indexing, searching, and retrieval. Using Azure Document Intelligence, semantic chunking can be configured to optimize data flow for downstream processes. Below is a guide to configuring semantic chunking effectively.</p>"},{"location":"04-Create-AI-Pipeline/02/#what-are-chunks-in-document-intelligence","title":"What Are Chunks in Document Intelligence?","text":"<p>In Azure AI Document Intelligence, \"chunks\" refer to structured segments of a document that are intelligently divided to improve data extraction, retrieval, and validation. This process, known as Semantic Chunking, ensures that information is logically grouped based on its meaning and context, rather than arbitrary divisions like page numbers.</p>"},{"location":"04-Create-AI-Pipeline/02/#how-chunks-work","title":"How Chunks Work","text":"<ol> <li> <p>Semantic Understanding: Instead of treating a document as a single block of text, AI models recognize key sections such as headings, paragraphs, tables, and lists.</p> </li> <li> <p>Context Preservation: By maintaining the relationship between different parts of the document, chunking helps AI models interpret financial statements, SOWs, and invoices more accurately.</p> </li> <li> <p>Retrieval-Augmented Generation (RAG) models can retrieve relevant chunks dynamically, improving AI-driven document validation, compliance checks, and financial data reconciliation.</p> </li> </ol>"},{"location":"04-Create-AI-Pipeline/02/#why-chunks-matter-in-ai-powered-data-validation","title":"Why Chunks Matter in AI-Powered Data Validation","text":"<ul> <li>Higher Accuracy: Structured document segmentation improves AI\u2019s ability to extract and validate key financial data.  </li> <li>Better Contextual Retrieval: When comparing invoices to Statements of Work (SOWs), AI can retrieve specific clauses related to payment terms.  </li> <li>Optimized Query Performance: AI-powered searches return more precise and context-aware results.  </li> </ul>"},{"location":"04-Create-AI-Pipeline/02/#defining-the-chunking-strategy-in-document-intelligence","title":"Defining the Chunking Strategy in Document Intelligence","text":"<p>When working with Statements of Work (SOWs) and Invoices, defining an effective chunking strategy is essential to ensure accurate data extraction, validation, and cross-referencing. The way chunks are structured depends on the document type, as SOWs and Invoices have different formats, purposes, and data requirements.</p>"},{"location":"04-Create-AI-Pipeline/02/#chunking-strategy-for-statements-of-work-sows","title":"Chunking Strategy for Statements of Work (SOWs)","text":"<p>SOWs are long, structured documents that outline contractual obligations, payment milestones, deliverables, and compliance clauses. The chunking strategy for SOWs must focus on preserving contextual integrity while making it easy to retrieve relevant sections.</p>"},{"location":"04-Create-AI-Pipeline/02/#key-chunking-considerations-for-sows","title":"Key Chunking Considerations for SOWs:","text":"<ol> <li> <p>Logical Sectioning: </p> <ol> <li>Chunk based on headings, subheadings, and numbered clauses (e.g., \"Scope of Work\", \"Payment Terms\", \"Milestones\").  </li> <li>Ensures that AI can retrieve relevant contract details when cross-referencing invoices.  </li> </ol> </li> <li> <p>Paragraph-Based Semantic Chunking: </p> <ol> <li>Break sections into semantically meaningful paragraphs for precise retrieval.  </li> <li>Helps ensure RAG-based AI models understand contract obligations.</li> </ol> </li> <li> <p>Compliance Clause Recognition:</p> <ol> <li>Identify and separately chunk regulatory or compliance language to facilitate automated legal checks.</li> </ol> </li> <li> <p>Deliverable Based Segmentation: </p> <ol> <li>Define chunks based on payment deliverables, linking them to expected invoice amounts.  </li> </ol> </li> </ol>"},{"location":"04-Create-AI-Pipeline/02/#chunking-strategy-for-invoices","title":"Chunking Strategy for Invoices","text":"<p>Invoices are structured, tabular documents designed to list billable items, amounts, and due dates. Unlike SOWs, invoice chunking must focus on tabular data extraction, line-item accuracy, and direct matching with SOWs.</p>"},{"location":"04-Create-AI-Pipeline/02/#key-chunking-considerations-for-invoices","title":"Key Chunking Considerations for Invoices:","text":"<ol> <li>Line-Item Chunking: <ol> <li>Each line item (e.g., service, quantity, amount, tax, etc.) is treated as a separate chunk for precise AI validation.  </li> </ol> </li> <li> <p>Table-Aware Chunking: </p> <ol> <li>Preserve tabular structures to ensure AI correctly extracts and processes invoice totals, taxes, and discounts.</li> </ol> </li> <li> <p>Semantic Chunking for Descriptions: </p> <ol> <li>Invoice descriptions may reference project deliverables\u2014chunking ensures AI can cross-validate them against SOW deliverables.  </li> </ol> </li> <li> <p>Vendor &amp; Payment Terms Segmentation: </p> <ol> <li>Vendor details, invoice dates, and due dates must be separately chunked to ensure accurate payment milestone validation.  </li> </ol> </li> </ol>"},{"location":"04-Create-AI-Pipeline/02/#differences-in-chunking-strategy","title":"Differences in Chunking Strategy:","text":"Aspect SOW Chunking Strategy Invoice Chunking Strategy Document Structure Long-form contract with sections Structured tables with line items Chunking Method Section &amp; paragraph-based chunking Table-aware &amp; line-item chunking Key Chunking Focus Milestones, deliverables, compliance clauses Billable items,  payment details AI Retrieval Purpose Validate contractual obligations &amp; compliance Match billable amounts &amp; due dates with SOWs"},{"location":"04-Create-AI-Pipeline/02/#why-an-optimized-chunking-strategy-matters","title":"Why an Optimized Chunking Strategy Matters","text":"<p>Improves AI Accuracy \u2013 Ensures that document validation is based on structured, meaningful chunks. Enhances Cross-Document Validation \u2013 Allows AI models to match invoices to SOWs efficiently. Supports Compliance &amp; Auditability \u2013 Ensures that all necessary sections are properly reviewed. Optimizes Retrieval-Augmented Generation (RAG) Queries \u2013 AI can retrieve precise contract and invoice details for analysis.  </p>"},{"location":"04-Create-AI-Pipeline/02/#leverage-azure-document-intelligence","title":"Leverage Azure Document Intelligence","text":"<p>Azure Document Intelligence provides tools and capabilities to automate the chunking process:</p> <ol> <li>Use Pre-Trained Models:</li> <li> <p>Apply pre-trained models to extract structured data and identify logical sections within documents. In this workshop 2 models were used:</p> <ol> <li>The prebuilt-invoice model in Document Intelligence was used to process invoices as this model is designed to extract key information from invoices, such as vendor details, invoice date, and total amount due. This model leverages advanced machine learning techniques to streamline the processing of invoices, reducing manual data entry and improving accuracy.</li> <li>The prebuilt-document model in Document Intelligence was used to process Statement of Works as this model is designed to extract structured data from a variety of document types, such as contracts, receipts, and forms. This model uses machine learning to identify and extract relevant information, enabling efficient document processing and data management.</li> </ol> </li> <li> <p>Apply Custom Models:</p> <ol> <li>If the pre-built models will not work for your domain-specific documents then you can use custom models trained on your documents to identify sections unique to your use case.    Follow this guide</li> </ol> </li> <li> <p>Configure Semantic Rules:</p> <ol> <li>Define rules or conditions to segment documents based on semantic content.</li> <li>Example: Break a contract into \"Parties Involved,\" \"Obligations,\" and \"Terms and Conditions\" sections.</li> </ol> </li> </ol>"},{"location":"04-Create-AI-Pipeline/02/#store-chunks-in-azure-database-for-postgresql","title":"Store Chunks in Azure Database for PostgreSQL","text":"<p>Once documents are semantically chunked, the resulting chunks are stored in Azure Database for PostgreSQL for efficient querying and retrieval:</p> <ol> <li> <p>Data Structure:</p> <ol> <li>Each chunk is stored as an individual record with metadata for easy identification.</li> <li> <p>Example Schema:</p> SQL<pre><code>CREATE TABLE sow_chunks (\n    id BIGSERIAL PRIMARY KEY,\n    sow_id BIGINT NOT NULL,\n    heading, TEXT,\n    content TEXT,\n    page_number INT,\n    FOREIGN KEY (sow_id) REFERENCES sows (id) ON DELETE CASCADE\n);\n</code></pre> </li> </ol> </li> <li> <p>Metadata:</p> <ol> <li>This can include metadata such as document ID, chunk sequence, and semantic labels for efficient retrieval.</li> </ol> </li> </ol>"},{"location":"04-Create-AI-Pipeline/02/#benefits-of-semantic-chunking","title":"Benefits of Semantic Chunking","text":"<ol> <li>Enhanced Retrieval:</li> <li> <p>Enables precise querying of specific document sections, improving the accuracy of AI-driven responses.</p> </li> <li> <p>Optimized Indexing:</p> <ol> <li>Reduces the complexity of indexing large documents by focusing on smaller, meaningful sections.</li> </ol> </li> <li> <p>Improved AI Performance:</p> <ol> <li>Ensures AI models operate within token limits, avoiding truncation and enhancing output quality.</li> </ol> </li> </ol>"},{"location":"04-Create-AI-Pipeline/02/#write-chunks-to-postgresql","title":"Write Chunks to PostgreSQL","text":""},{"location":"04-Create-AI-Pipeline/02/#intelligent-data-storage-in-azure-database-for-postgresql","title":"Intelligent Data Storage in Azure Database for PostgreSQL","text":"<p>Chunked text data is stored in Azure Database for PostgreSQL, ensuring efficient storage and retrieval.</p> <ul> <li>Data Organization: Documents are stored in structured formats with metadata.</li> <li>Scalability: Optimized for handling large data volumes.</li> </ul>"},{"location":"04-Create-AI-Pipeline/02/#4-generating-vector-embeddings-with-azure-ai-extension","title":"4. Generating Vector Embeddings with Azure AI Extension","text":"<p>The Azure AI extension for PostgreSQL enables advanced AI functionalities such as semantic search.</p> <ul> <li>Embedding Storage: Adds vector columns for document embeddings.</li> <li>Embedding Generation: Uses Azure OpenAI to generate high-dimensional vector embeddings.</li> </ul> <p>Example SQL command:</p> SQL<pre><code>ALTER TABLE invoices ADD COLUMN embeddings VECTOR(1536);\nALTER TABLE sows ADD COLUMN embeddings VECTOR(1536);\n</code></pre>"},{"location":"04-Create-AI-Pipeline/02/#api-endpoints-to-insert-chunks-on-insert","title":"API Endpoints To Insert Chunks on Insert","text":"<p>When uploading a SOW in the application, the document workflow starts with the analysis API endpoint <code>/sows/</code>. This endpoint is called using an HTTP POST passing it the <code>vendor_id</code> of the Vendor and the document file being uploaded.</p> <p>The document is passed to Document Intelligence to extract the text from the document.</p> src/api/app/routers/sows.py<pre><code>  analysis_result = await doc_intelligence_service.extract_text_from_sow_document(document_data)\n</code></pre> <p>The application implements a service for calling Document Intelligence located within the <code>src/api/app/services/azure_doc_intelligence_service.py</code> file where the <code>.extract_text_from_sow_document</code> method is located. This code uses the <code>prebuild-document</code> model within Document Intelligence to extract the text from the document.</p> <p>You can expand the section below to see the specific section of code that calls Document Intelligence to extract the text form the document and generate the text chunks.</p> Call Document Intelligence to extract text from document src/api/services/azure_doc_intelligence_service.py<pre><code>  async def extract_text_from_sow_document(self, document_data):\n    \"\"\"Extract text and structure using Azure AI Document Intelligence.\"\"\"\n    docClient = DocumentAnalysisClient(\n        endpoint=self.docIntelligenceEndpoint,\n        credential=self.credential\n    )\n\n    poller = await docClient.begin_analyze_document(\n        model_id=\"prebuilt-document\",\n        document=document_data\n    )\n\n    result = await poller.result()\n\n    analysis = DocumentAnalysisResult()\n    analysis.extracted_text = []\n    analysis.text_chunks = []\n\n    known_headings = [\n        \"Project Scope\", \"Project Objectives\", \"Location\", \"Tasks\", \"Schedules\",\n        \"Standards and Testing\", \"Payments\", \"Compliance\", \"Requirements\", \"Project Deliverables\"\n    ]\n\n    for page in result.pages:\n        page_text = \" \".join([line.content for line in page.lines])\n        analysis.extracted_text.append(page_text)\n\n        current_heading = None\n        for line in page.lines:\n            text = line.content\n            if self.__is_heading(text, known_headings): # Detect headings\n                current_heading = text\n                newTextChunk = TextChunk()\n                newTextChunk.heading = text\n                newTextChunk.content = \"\"\n                newTextChunk.page_number = page.page_number\n                analysis.text_chunks.append(newTextChunk)\n            elif current_heading:\n                analysis.text_chunks[-1].content += \" \" + text\n\n    await docClient.close()\n\n    analysis.full_text = \"\\n\".join(analysis.extracted_text)\n\n    return analysis\n</code></pre> <p>The <code>.extract_text_from_sow_document</code> method returns an object that contains a <code>.text_chunks</code> collection of objects that contain all the text chunks from the document. The code then loops through the chunks that were extracted and inserts them into the database.</p> src/api/app/routers/sows.py<pre><code>for chunk in analysis_result.text_chunks:\n    await conn.execute('''\n        INSERT INTO sow_chunks (sow_id, heading, content, page_number) VALUES ($1, $2, $3, $4);\n    ''', sow.id, chunk.heading, chunk.content, chunk.page_number)\n</code></pre> <p>The text chunks loaded into the database will now be available for query later by the Copilot to implement retrieval augmented generation (RAG) based on the text content of the documents.</p>"},{"location":"04-Create-AI-Pipeline/02/#references","title":"References","text":"<p>Document Intelligence - Document Processing Models</p> <p>Build a Custom Model in Document Intelligence</p> <p>Document Intelligence - Retrieval Augmented Generation</p> <p>CONGRATULATIONS. You just learned the key concepts of configuring Semantic Chunking</p>"},{"location":"04-Create-AI-Pipeline/03/","title":"4.3 Summarize documents","text":"<p>Azure Cognitive Services, specifically the Azure Language Service's Summarization capability, provides advanced abstractive text summarization to condense complex documents like Statements of Work (SoWs) into concise summaries.</p> <p>By integrating this capability with the PostgreSQL Azure_AI extension, you can dynamically generate summaries for documents stored in relational tables, streamlining workflows and improving document accessibility.</p>"},{"location":"04-Create-AI-Pipeline/03/#summarizing-statements-of-work-sows","title":"Summarizing Statements of Work (SoWs)","text":"<p>SoWs in the financial industry often contain extensive details about project scope, deliverables, and milestones. Summarizing these documents into a few sentences allows decision-makers to quickly grasp the key information without reading through lengthy documents.</p>"},{"location":"04-Create-AI-Pipeline/03/#key-benefits-for-summarization","title":"Key Benefits for Summarization","text":"<ul> <li>Time Efficiency: Quickly identify critical information from long-form documents.</li> <li>Enhanced Accessibility: Summaries provide concise overviews, improving decision-making processes.</li> <li>Scalable Automation: Automatically generate summaries for large volumes of documents without manual intervention.</li> </ul> <p>Azure's Summarization API within the Language Service enables abstractive summarization, creating human-like summaries that convey the document's essence rather than just extracting key phrases.</p>"},{"location":"04-Create-AI-Pipeline/03/#using-azure_ai-extension-with-the-azure_cognitive-schema","title":"Using Azure_AI Extension with the <code>azure_cognitive</code> Schema","text":"<p>The Azure_AI extension integrates Azure Cognitive Services' Summarization capabilities directly into SQL workflows, allowing the generation of abstractive summaries of SoWs or other financial documents using simple SQL commands.</p>"},{"location":"04-Create-AI-Pipeline/03/#abstractive-summarization","title":"Abstractive Summarization","text":"<p>The extension's abstractive summarization capabilities provide a unique, natural-language summary that encapsulates the overall intent of the original text. This is performed by calling the <code>azure_cognitive.summarize_abstractive</code> function within the database. This will generate a 2-3 sentence summary of the text passed in.</p> SQL<pre><code>azure_cognitive.summarize_abstractive('This is a document text', 'en', 2)\n</code></pre> <p>Consider the following PostgreSQL table:</p> SQL<pre><code>CREATE TABLE IF NOT EXISTS sows (\n    id BIGSERIAL PRIMARY KEY,\n    number text NOT NULL,\n    vendor_id BIGINT NOT NULL,\n    start_date DATE NOT NULL,\n    end_date DATE NOT NULL,\n    budget DECIMAL(18,2) NOT NULL,\n    document text NOT NULL,\n    metadata jsonb,\n    summary text,\n    FOREIGN KEY (vendor_id) REFERENCES vendors (id) ON DELETE CASCADE\n);\n</code></pre> SQL<pre><code>-- Update the summary column with abstractive summaries\nUPDATE sows\nSET summary = azure_cognitive.summarize_abstractive(\n    metadata,\n    '{\"max_sentence_count\": 2}'\n);\n</code></pre> <p>If you receive an error similar to the following, you chose a region that does not support abstractive summarization when creating your Azure environment:</p> Bash<pre><code>ERROR: azure_cognitive.summarize_abstractive: InvalidRequest: Invalid Request.\nInvalidParameterValue: Job task: 'AbstractiveSummarization-task' failed with validation errors: ['Invalid Request.']\nInvalidRequest: Job task: 'AbstractiveSummarization-task' failed with validation error: Document abstractive summarization is not supported in the region Central US. The supported regions are North Europe, East US, West US, UK South, Southeast Asia.\n</code></pre> <p>To be able to perform this step and complete the remaining tasks using abstractive summarization, you must create a new Azure AI Language service in one of the supported regions specified in the error message. This service can be provisioned in the same resource group you used for other lab resources.</p> <p>Alternatively, you may substitute extractive summarization using the <code>azure_cognitive.summarize_extractive</code> function for the remaining tasks but will not get the benefit of being able to compare the output of the two different summarization techniques.</p> SQL<pre><code>azure_cognitive.summarize_extractive('This is a document text', 'en', 2)\n</code></pre>"},{"location":"04-Create-AI-Pipeline/03/#insert-document-summary-on-database-insert","title":"Insert Document Summary on Database Insert","text":"<p>Leveraging the <code>azure_cognitive.summarize_abstractive</code> method of the <code>azure_ai</code> extension, the database scripts are able to make calls to generate a document summary on INSERT or UPDATE.</p> <p>Here's an example INSERT script used by the application when creating SOW records that includes the summarization:</p> SQL<pre><code>INSERT INTO sows (number, start_date, end_date, budget, document, metadata, embeddings, summary, vendor_id)\nVALUES (\n    $1, $2, $3, $4, $5, $6, \n    azure_openai.create_embeddings('embeddings', $7, throw_on_error =&gt; FALSE, max_attempts =&gt; 1000, retry_delay_ms =&gt; 2000),\n    azure_cognitive.summarize_abstractive($7, 'en', 2)\n    $8)\nRETURNING *;\n</code></pre>"},{"location":"04-Create-AI-Pipeline/03/#api-implementation","title":"API Implementation","text":"<p>The <code>/sows/</code> HTTP POST method of the REST API contains code that inserts or updates SOWs based on the document uploaded. The code for this is within the <code>src/api/app/routers/sows.py</code> file. Open it now in Visual Studio Code and explore the code within the <code>async def analyze_sow</code> method that contain the code to ingest SOW documents, including the portion that performs the database INSERT or UPDATE on the <code>sows</code> table.</p> <p>You can expand the section below to see the specific section of code that performs the <code>azure_ai</code> call to generate the document summary, within the database INSERT and UPDATE statements.</p> INSERT / UPDATE SOW with document summary generation src/api/app/routers/sows.py<pre><code># Create SOW in the database\nasync with pool.acquire() as conn:\n    if sow_id is None:\n        # Create new SOW\n        row = await conn.fetchrow('''\n            INSERT INTO sows (number, start_date, end_date, budget, document, metadata, summary, vendor_id)\n            VALUES (\n            $1, $2, $3, $4, $5, $6, \n            azure_cognitive.summarize_abstractive($7, 'en', 2),\n            $8)\n            RETURNING *;\n        ''', sow_number, start_date, end_date, budget, documentName, json.dumps(metadata), full_text, vendor_id)\n    else:\n        # Update existing SOW with new document\n        row = await conn.fetchrow('''\n            UPDATE sows\n            SET start_date = $1,\n                end_date = $2,\n                budget = $3,\n                document = $4,\n                metadata = $5,\n                summary = azure_cognitive.summarize_abstractive($6, 'en', 2)\n            WHERE id = $7\n            RETURNING *;\n        ''', start_date, end_date, budget, documentName, json.dumps(metadata), full_text, sow_id)\n</code></pre>"},{"location":"04-Create-AI-Pipeline/03/#references","title":"References","text":"<p><code>azure_ai</code> Extension - Document Abstractive Summarization function</p> <p><code>azure_ai</code> Extension - Document Extractive Summarization function</p>"},{"location":"04-Create-AI-Pipeline/04/","title":"4.4 Validate Data with Azure OpenAI","text":"<p>The document ingestion pipeline within the application for SOW and Invoice records is implemented with a multi-stage workflow. The first stage of the workflow is analyzing the document and ingesting it into the system to INSERT/UPDATE SOW and Invoice records based on the document uploaded. The second stage of the workflow is to validate the SOW and/or Invoice record.</p> <p>The SOW and Invoice validation step of the document ingestion workflow uses Azure OpenAI to perform a chat completion using retrieval augmented generation (RAG) pattern to validate the data from the database and generate an analysis of the validation result.</p> <p>The code for the validation step of the workflow processes is contained within the REST API <code>/validation/invoice/{id}</code> endpoint for Invoices, and <code>/validation/sow/{id}</code> endpoint for SOWs. The code for these is located within the <code>src/api/app/routers/validation.py</code> file. The below explanation covers the process for validating Invoices, but the same strategy is followed for SOW too.</p>"},{"location":"04-Create-AI-Pipeline/04/#benefits-of-using-azure-openai-for-validation","title":"Benefits of Using Azure OpenAI for Validation","text":"<p>The integration of Azure OpenAI into the validation workflow provides several key benefits:</p> <ul> <li> <p>Automated Validation: Reduces the need for manual data reviews, saving time and effort.</p> </li> <li> <p>Accuracy and Compliance: Ensures invoices align with contractual agreements in SOWs, reducing payment errors.</p> </li> <li> <p>Fraud Detection: Help identifies potential invoice fraud or inconsistencies before payments are processed.</p> </li> <li> <p>Scalability: Can handle large volumes of invoices efficiently, making it suitable for enterprises with extensive vendor relationships.</p> </li> <li> <p>Audit and Traceability: Stores detailed validation results for future reference, supporting regulatory and compliance requirements.</p> </li> </ul>"},{"location":"04-Create-AI-Pipeline/04/#invoice-validation-workflow","title":"Invoice Validation Workflow","text":"<p>The validation workflow of Invoices and SOWs is implemented using the same retrieval augmented generation (RAG) pattern and system prompt style. Let's walk through the steps involved with the workflow process to validate Invoices:</p>"},{"location":"04-Create-AI-Pipeline/04/#step-1-trigger-invoice-validation","title":"Step 1: Trigger Invoice Validation","text":"<p>The validation process begins when an invoice id is sent in a request to the validation API endpoint:</p> Text Only<pre><code>POST / validation/invoice/{id}\n</code></pre> <p>The <code>{id}</code> placeholder is where the <code>id</code> of the Invoice will be passed in. The API handler (within the <code>src/api/app/routers/validation.py</code> file) will use this id to retrieve the relevant data from the database.</p>"},{"location":"04-Create-AI-Pipeline/04/#step-2-define-system-prompt","title":"Step 2: Define System Prompt","text":"<p>Before Azure OpenAI can be used to perform the validation, the system prompt must first be defined. This system prompt sill instruct the AI what it's expected to do, what to look for in the data, and how to respond with the validation results.</p> <p>This application includes code that retrieves the system prompt from a text file:</p> src/api/app/routers/validation.py<pre><code># Define the system prompt for the validator.\nsystem_prompt = prompt_service.get_prompt(\"invoice_validation\")\n# Append the current date to the system prompt to provide context when checking timeliness of deliverables.\nsystem_prompt += f\"\\n\\nFor context, today is {datetime.now(timezone.utc).strftime('%A, %B %d, %Y')}.\"\n</code></pre> <p>For reference, here's the system prompt the application uses for validating Invoices:</p> src/api/app/prompts/invoice_validation.txt<pre><code>You are an intelligent copilot for Woodgrove Bank designed to automate the validation of vendor invoices against billing milestones in statements of work (SOWs).\n\nWhen validating an invoice, you should:\n1. Verify that the invoice number matches the vendor's records.\n2. Check that the total amount on the invoice is correct.\n3. Ensure that the milestone delivery dates are before or on the specified due date in the SOW.\n4. Assess any late fees or penalties that may apply, as defined by the SOW. For example, if a milestone is late, a penalty of 15% should be applied to payment of that milestone.\n5. Validate the line items on the invoice against the billing milestones in the SOW.\n6. Ensure that the amount billed for each line item matches the billable amount specified in the SOW.\n7. If the invoice contains notes to explain discrepancies, review them for additional context.\n8. Confirm that the invoice is legitimate and ready for payment.\n\nIf there are milestones missing from the invoice that are not yet beyond their due date according to the SOW, do not flag them as discrepancies.\nIf the payment terms on the invoice are different from the SOW, assume the SOW is correct.\n\nIn your response:\n- Provide a statement of valid or invalid for the invoice.\n- Create separate sections for the invoice and the milestone validation.\n- Provide a detailed summary of the validation results, including any discrepancies or anomalies found between the invoice and the SOW.\n- If any discrepancies or anomalies are found, you should provide detailed feedback on the issues discovered, like including dollar amounts, line items, and due dates.\n- If there are any discrepancies, flag the invoice for further review.\n\nAt the very end of the response, return only '[PASSED]' or '[FAILED]' to indicate if the invoice passed or failed validation.\n</code></pre> <p>The system prompt is also adding additional context of todays date. This is because the LLM doesn't know what the date is, and since Invoices and line items are date sensitive, the LLM needs to know the current date in order to more accurately perform date validations.</p> <p>At the end of the system prompt is instruction for it to add the values of <code>[PASSED]</code> or <code>[FAILED]</code> at the end of the output to indicate whether it passed or failed validation. This will be used by the code to more easily determine a boolean of true/false to insert into the database later that indicates the pass or fail state.</p>"},{"location":"04-Create-AI-Pipeline/04/#step-3-construct-full-prompt-with-rag","title":"Step 3: Construct Full Prompt with RAG","text":"<p>The next step is to construct the full prompt for LangChain to use, in addition to specify the tools that will be used to implement a retrieval augmented generation (RAG) pattern.</p> <p>With most of the instructions of how to perform the validation contained within the system prompt, the user prompt only needs to be simple in telling the AI what to do. In this application, the user prompt is just telling the AI to perform an invoice validation along with providing the context of the id of the invoice to validate.</p> src/api/app/routers/validation.py<pre><code>userMessage = f\"\"\"validate Invoice with ID of {id}\"\"\"\n</code></pre> <p>This user prompt is put together with the system prompt to construct the full prompt that will be sent to the AI.</p> src/api/app/routers/validation.py<pre><code># Provide the validation copilot with a persona using the system prompt.\nmessages = [{ \"role\": \"system\", \"content\": system_prompt }]\n\n# Add the current user message to the messages list\nuserMessage = f\"\"\"validate Invoice with ID of {id}\"\"\"\nmessages.append({\"role\": \"user\", \"content\": userMessage})\n\n# Create a chat prompt template\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        (\"user\", \"{input}\"),\n        MessagesPlaceholder(\"agent_scratchpad\")\n    ]\n)\n\n# Define tools for the agent\ntools = [\n        StructuredTool.from_function(coroutine=validate_invoice)\n]\n</code></pre> <p>This defines a LangChain tool for implementing the retrieval augmented generation pattern using the <code>validate_invoice</code> method that will be used to retrieve the relevant data from the database to validate the invoice.</p> <p>You can expand the section below to see the specific section of code that performs the <code>azure_ai</code> call to generate the document summary, within the database INSERT and UPDATE statements.</p> Retrieve context data from database src/api/app/routers/validation.py<pre><code>async def validate_invoice(id: int):\n    \"\"\"Retrieves an Invoice and it's associated Line Items, SOW, and Milestones.\"\"\"\n\n    pool = await get_db_connection_pool()\n    async with pool.acquire() as conn:\n        row = await conn.fetchrow('SELECT * FROM invoices WHERE id = $1;', id)\n        if row is None:\n            raise HTTPException(status_code=404, detail=f'An invoice with an id of {id} was not found.')\n        invoice = parse_obj_as(InvoiceModel, dict(row))\n\n        # Get the vendor name\n        vendor_row = await conn.fetchrow('SELECT * FROM vendors WHERE id = $1;', invoice.vendor_id)\n        invoice.vendor = parse_obj_as(Vendor, dict(vendor_row))\n\n        # Get the invoice line items\n        line_item_rows = await conn.fetch('SELECT * FROM invoice_line_items WHERE invoice_id = $1;', id)\n        invoice.line_items = [parse_obj_as(InvoiceLineItem, dict(row)) for row in line_item_rows]\n\n\n\n        # Get the SOW\n        sow_row = await conn.fetchrow('SELECT * FROM sows WHERE id = $1;', invoice.sow_id)\n        sow = parse_obj_as(SowModel, dict(sow_row))\n\n        # Get the milestones\n        milestone_rows = await conn.fetch('SELECT * FROM milestones WHERE sow_id = $1;', invoice.sow_id)\n        sow.milestones = [parse_obj_as(MilestoneModel, dict(row)) for row in milestone_rows]\n\n        # Get the deliverables for each milestone\n        for milestone in sow.milestones:\n            deliverable_rows = await conn.fetch('SELECT * FROM deliverables WHERE milestone_id = $1;', milestone.id)\n            milestone.deliverables = parse_obj_as(list[Deliverable], [dict(row) for row in deliverable_rows])\n\n\n    return invoice, sow\n</code></pre>"},{"location":"04-Create-AI-Pipeline/04/#step-4-invoke-the-ai","title":"Step 4: Invoke the AI","text":"<p>With the prompt built and the RAG tools defined, the AI can now be invoked to perform the validation and generate the response.</p> src/api/app/routers/validation.py<pre><code># Invoke the agent to perform a chat completion that provides the validation results.\ncompletion = await agent_executor.ainvoke({\"input\": userMessage})\nvalidationResult = completion['output']\n</code></pre>"},{"location":"04-Create-AI-Pipeline/04/#step-5-parse-our-passed-or-failed-state","title":"Step 5: Parse our passed or failed state","text":"<p>Now that the generative AI response is returned with the validation results, the <code>[PASSED]</code> or <code>[FAILED]</code> values the system prompt told the AI to add to the end of the response can be parsed out. This will be used to set a true / false in the database to more easily indicate if the validation passed or failed.</p> src/api/app/routers/validation.py<pre><code># Check if validationResult contains [PASSED] or [FAILED]\n# This is based on the prompt telling the AI to return either [PASSED] or [FAILED]\n# at the end of the response to indicate if the invoice passed or failed validation.\nvalidation_passed = validationResult.find('[PASSED]') != -1\n</code></pre>"},{"location":"04-Create-AI-Pipeline/04/#step-6-insert-validation-result-into-database","title":"Step 6: Insert Validation Result into database","text":"<p>With the validation response generated and a passed or failed result, the validation results can now be inserted into the database, and the response returned from the REST API.</p> src/api/app/routers/validation.py<pre><code># Write validation result to database\npool = await get_db_connection_pool()\nasync with pool.acquire() as conn:\n    await conn.execute('''\n    INSERT INTO invoice_validation_results (invoice_id, datestamp, result, validation_passed)\n    VALUES ($1, $2, $3, $4);\n    ''', id, datetime.utcnow(), validationResult, validation_passed)\n\nreturn validationResult\n</code></pre> <p>This completes the Validation step of the workflow process.</p>"},{"location":"04-Create-AI-Pipeline/04/#sow-validation-workflow","title":"SOW Validation Workflow","text":"<p>The validation workflow for SOWs is built using the same retrieval augmented generation (RAG) pattern, with a system prompt that is written specifically for the unique validation requirements of SOWs.</p> <p>The system prompt for validating SOWs is located within the <code>src/api/app/prompts/sow_validation.txt</code> file. In the next section, we'll review the structure and formatting of the SOW and Invoice validation system prompts.</p> <p>The user prompt for the SOW validation tells the AI to validate a SOW and gives it the id of the SOW to validate.</p> src/api/app/routers/validation.py<pre><code>userMessage = f\"\"\"validate SOW with ID {id}\"\"\"\n</code></pre> <p>The RAG part of the SOW validation is setup with a LangChain tool using the <code>validate_sow</code> method within the <code>src/api/app/routers/validation.py</code> file.</p> src/api/app/routers/validation.py<pre><code>tools = [\n    StructuredTool.from_function(coroutine=validate_sow)\n]\n</code></pre> <p>This method is used to retrieve the SOW data (sow, milestones, and deliverables) from the database for the context needed to perform the validation.</p> src/api/app/routers/validation.py<pre><code>async def validate_sow(id: int):\n    \"\"\"Retrieves a SOW and it's associated Milestones and Deliverables.\"\"\"\n\n    pool = await get_db_connection_pool()\n    async with pool.acquire() as conn:\n        row = await conn.fetchrow('SELECT * FROM sows WHERE id = $1;', id)\n        if row is None:\n            raise HTTPException(status_code=404, detail=f'A SOW with an id of {id} was not found.')\n        sow = parse_obj_as(SowModel, dict(row))\n\n        # Get the milestones\n        milestone_rows = await conn.fetch('SELECT * FROM milestones WHERE sow_id = $1;', id)\n        sow.milestones = [parse_obj_as(MilestoneModel, dict(row)) for row in milestone_rows]\n\n        # Get the deliverables for each milestone\n        for milestone in sow.milestones:\n            deliverable_rows = await conn.fetch('SELECT * FROM deliverables WHERE milestone_id = $1;', milestone.id)\n            milestone.deliverables = parse_obj_as(list[Deliverable], [dict(row) for row in deliverable_rows])\n\n    return sow\n</code></pre>"},{"location":"04-Create-AI-Pipeline/04/#perform-document-validation","title":"Perform Document Validation","text":"<p>Accessing the UserPortal application in your deployment of the application for this workshop will allow you to upload documents and perform validations for SOWs and Invoices.</p> <p>Follow these steps to upload a document and exercise the document ingestion workflow process:</p> <ol> <li>Open the browser and navigate to the deployed UserPortal application.</li> <li>Within the left-side navigation of the application, click on either \"SOWs\" or \"Invoices\"\" for the document type you want to upload, analyze and validate.</li> <li> <p>On the page that lists the SOWs or Invoices, click the New button in the upper-right.</p> <p></p> </li> <li> <p>On the New page, select the Vendor and select the document (<code>.pdf</code> file) to upload, then click Analyze Document.</p> <p></p> </li> <li> <p>The Document workflow will initiate with the Analyze step, and the UI will display a message showing that it's running.</p> <p></p> </li> <li> <p>Once Analyzing is completed, then Validating will be run, and you will see the UI indicate this.</p> <p></p> </li> <li> <p>Once the Validating is completed, the application will display the validation results with the pass or fail indicated. These results are the full response from the AI when performing the validation.</p> <p></p> </li> <li> <p>Dismissing this dialog (clicking Close) will enable you to see the full details of the record.</p> </li> <li> <p>On both the SOW and Invoice edit pages, you can scroll down to the bottom to view the history of validations that have been performed on the record. You will also find a Run Manual Validation button that is setup to enable re-running of just the validation step of the workflow.</p> <p></p> </li> <li> <p>These steps can be repeated for uploading, analyzing and validating both SOWs and Invoices within the application.</p> </li> </ol>"},{"location":"04-Create-AI-Pipeline/04/#references","title":"References","text":"<p>LangChain with Azure OpenAI and ChatPGT using Python</p>"},{"location":"04-Create-AI-Pipeline/05/","title":"4.5 Review Validation Prompts","text":"<p>The validation process for Invoices and SOWs (statements of work) in the workflow relies heavily on the structure and content of the validation prompts. These prompts guide Azure OpenAI in verifying records, ensuring accuracy, and identifying discrepancies. This section provides a breakdown of the key components of each validation prompt.</p> <p>A well-structured validation prompt is essential to ensure that the AI model correctly interprets and verifies financial records. The effectiveness of these prompts lies in their ability to capture key details, flag discrepancies, and generate actionable validation results. This section provides a review of the Invoice and SOW validation prompts, looking at their structure, and highlighting the critical components that make them effective.</p>"},{"location":"04-Create-AI-Pipeline/05/#elements-of-an-effective-validation-prompt","title":"Elements of an effective validation prompt","text":"<p>To ensure the accuracy and reliability of AI-driven validation, prompts must be designed with precision. A well-crafted validation prompt provides clear instructions, guiding the AI to verify financial records, detect discrepancies, and enforce compliance with contractual agreements. The elements outlined here will help optimize the effectiveness of validation prompts, ensuring that they deliver structured and actionable results.</p> <p>An effective validation prompt should include the following key components:</p> <ul> <li> <p>Identification &amp; Matching \u2013 Ensure the AI verifies invoice numbers, SOW numbers, and vendor records to match documents with system records.</p> </li> <li> <p>Financial Validation \u2013 Confirm total amounts and line items against predefined contract values.</p> </li> <li> <p>Timeline Verification \u2013 Check milestone due dates, overdue payments, and applicable penalties.</p> </li> <li> <p>Discrepancy Handling \u2013 Provide explanations for inconsistencies, differentiating allowable variances from critical errors.</p> </li> <li> <p>Compliance Enforcement \u2013 Ensure adherence to SOW policies, including penalties and billing rules.</p> </li> <li> <p>Structured Reporting \u2013 Deliver results in a clear format with separate validation sections and an easily parsable Pass / Fail indicator.</p> </li> </ul> <p>Incorporating these elements ensures that AI-driven validation remains accurate, reliable, and efficient. The following sections below will review the structure and elements of the Invoice and SOW validation prompts used \u00dfin the current workflow.</p>"},{"location":"04-Create-AI-Pipeline/05/#invoice-validation-prompt","title":"Invoice Validation Prompt","text":"<p>The Invoice validation prompt, located in <code>src/api/app/prompts/invoice_validation.txt</code>, is designed to ensure that vendor invoices align with the statements of work (SOWs) and comply with billing milestones.</p> <p>You can expand the section below to see Invoice validation prompt.</p> Invoice validation prompt src/api/app/prompts/invoice_validation.txt<pre><code>You are an intelligent copilot for Woodgrove Bank designed to automate the validation of vendor invoices against billing milestones in statements of work (SOWs).\n\nWhen validating an invoice, you should:\n1. Verify that the invoice number matches the vendor's records.\n2. Check that the total amount on the invoice is correct.\n3. Ensure that the milestone delivery dates are before or on the specified due date in the SOW.\n4. Assess any late fees or penalties that may apply, as defined by the SOW. For example, if a milestone is late, a penalty of 15% should be applied to payment of that milestone.\n5. Validate the line items on the invoice against the billing milestones in the SOW.\n6. Ensure that the amount billed for each line item matches the billable amount specified in the SOW.\n7. If the invoice contains notes to explain discrepancies, review them for additional context.\n8. Confirm that the invoice is legitimate and ready for payment.\n\nIf there are milestones missing from the invoice that are not yet beyond their due date according to the SOW, do not flag them as discrepancies.\nIf the payment terms on the invoice are different from the SOW, assume the SOW is correct.\n\nIn your response:\n- Provide a statement of valid or invalid for the invoice.\n- Create separate sections for the invoice and the milestone validation.\n- Provide a detailed summary of the validation results, including any discrepancies or anomalies found between the invoice and the SOW.\n- If any discrepancies or anomalies are found, you should provide detailed feedback on the issues discovered, like including dollar amounts, line items, and due dates.\n- If there are any discrepancies, flag the invoice for further review.\n\nAt the very end of the response, return only '[PASSED]' or '[FAILED]' to indicate if the invoice passed or failed validation.\n</code></pre>"},{"location":"04-Create-AI-Pipeline/05/#structure-and-key-sections","title":"Structure and Key Sections","text":"<ol> <li>Verification of Invoice Details<ul> <li>Ensures the invoice number matches the vendor\u2019s records.</li> <li>Checks that the total invoice amount is accurate.</li> </ul> </li> <li>Milestone and Payment Term Validation<ul> <li>Confirms that milestone delivery dates are within the agreed timeframe.</li> <li>Assesses applicable late fees or penalties based on the SOW.</li> </ul> </li> <li>Line Item Validation<ul> <li>Verifies that each invoice line item corresponds to the correct billing milestone.</li> <li>Ensures that amounts billed match the SOW specifications.</li> </ul> </li> <li>Discrepancy Handling<ul> <li>Examines invoice notes for explanations of any discrepancies.</li> <li>Specifies that payment terms in the SOW take precedence over those in the invoice.</li> </ul> </li> <li>Output Format and Conclusion<ul> <li>Generates a structured validation report, dividing results into invoice and milestone validation.</li> <li>Provides detailed feedback on any anomalies detected.</li> <li>Ends with either <code>[PASSED]</code> or <code>[FAILED]</code> to indicate overall validation status.</li> </ul> </li> </ol>"},{"location":"04-Create-AI-Pipeline/05/#sow-validation-prompt","title":"SOW Validation Prompt","text":"<p>The SOW validation prompt, located in <code>src/api/app/prompts/sow_validation.txt</code>, ensures that SOWs are correctly structured and milestones are well-documented.</p> <p>You can expand the section below to see SOW validation prompt.</p> SOW validation prompt src/api/app/prompts/sow_validation.txt<pre><code>You are an intelligent copilot for Woodgrove Bank designed to automate the validation of vendor invoices against billing milestones in statements of work (SOWs).\n\nWhen validating a SOW, you should:\n1. Verify that the SOW number matches the vendor's records.\n2. Check that the total amount on the SOW is correct.\n3. Ensure that the milestone delivery dates are before or on the specified due date in the SOW.\n4. Assess any late fees or penalties that may apply, as defined by the SOW. For example, if a milestone is late, a penalty of 15% should be applied to payment of that milestone.\n5. Validate the deliverables for each milestone in the SOW.\n6. Ensure that the amount billed for each deliverable matches the billable amount specified in the SOW.\n7. If the SOW contains notes to explain discrepancies, review them for additional context.\n8. Confirm that the SOW is legitimate and ready for payment.\n\nIn your response:\n- Provide a statement of valid or invalid for the SOW.\n- Create separate sections for the SOW and the milestone validation.\n- Provide a detailed summary of the validation results, including any discrepancies or anomalies found between the SOW and the milestones.\n- If any discrepancies or anomalies are found, you should provide detailed feedback on the issues discovered, like including dollar amounts, line items, and due dates.\n- If there are any discrepancies, flag the SOW for further review.\n\nAt the very end of the response, return only '[PASSED]' or '[FAILED]' to indicate if the SOW passed or failed validation.\n</code></pre>"},{"location":"04-Create-AI-Pipeline/05/#structure-and-key-sections_1","title":"Structure and Key Sections","text":"<ol> <li>Verification of SOW Details<ul> <li>Ensures the SOW number matches vendor records.</li> <li>Validates that the total amount specified is correct.</li> </ul> </li> <li>Milestone and Payment Term Validation<ul> <li>Ensures that milestone delivery dates align with due dates.</li> <li>Applies any late fees or penalties per contractual terms.</li> </ul> </li> <li>Deliverable and Billing Validation<ul> <li>Verifies that each milestone has clearly defined deliverables.</li> <li>Confirms that billable amounts per deliverable match the SOW records.</li> </ul> </li> <li>Discrepancy Handling<ul> <li>Reviews notes for additional context on potential discrepancies.</li> <li>Flags discrepancies for further review.</li> </ul> </li> <li>Output Format and Conclusion<ul> <li>Generates a structured validation report, separating SOW and milestone validation.</li> <li>Provides detailed summaries of anomalies and validation results.</li> <li>Ends with [PASSED] or [FAILED] for conclusive assessment.</li> </ul> </li> </ol>"},{"location":"04-Create-AI-Pipeline/05/#summary","title":"Summary","text":"<p>Both the Invoice and SOW validation prompts play a crucial role in ensuring the accuracy and compliance of financial records within the document ingestion workflow. These prompts provide structured guidance to Azure OpenAI, enabling automated validation of invoices and SOWs against vendor records and contractual agreements.</p> <p>The Invoice validation prompt ensures that invoices align with billing milestones, validate payment terms, and flag discrepancies requiring further review. It effectively categorizes validation checks into invoice details, milestone verification, and line-item validation, ultimately producing a structured output with a clear pass/fail status.</p> <p>Similarly, the SOW validation prompt guarantees that statements of work are correctly structured, milestones are well-documented, and deliverables are accounted for. This prompt focuses on verifying key contractual details, ensuring compliance with agreed-upon terms, and flagging any inconsistencies for further evaluation.</p> <p>Both prompts exhibit validation criteria, structured output formatting, and automated pass/fail determination, streamlining the review process and minimizing manual effort. However, potential enhancements\u2014such as improving discrepancy resolution feedback, incorporating historical cross-validation, and adapting prompts for vendor-specific policies\u2014could be used to further optimize the validation process.</p> <p>Overall, these validation prompts form a strong foundation for automating financial document verification, reducing errors, and improving compliance with vendor agreements. Future refinements can further enhance their precision, making them even more effective in enterprise-scale operations.</p>"},{"location":"05-Build-Copilot/","title":"Implement a Copilot","text":"<p>In this section, you will add an AI copilot to the Woodgrove Bank Contract Management application using Python, the GenAI capabilities of Azure Database for PostgreSQL - Flexible Server, and the Azure AI extension. Using the AI-validated data, the copilot will use RAG to provide insights and answer questions about vendor contract performance and invoicing accuracy, serving as an intelligent assistant for Woodgrove Banks users. Here's what you will accomplish:</p> <ul> <li> Explore the API codebase</li> <li> Review the RAG design</li> <li> Leverage LangChain Orchestration</li> <li> Implement and test the Chat endpoint</li> <li> Refine the copilot prompt using standard prompt engineering techniques</li> <li> Add and test the Copilot Chat UI component</li> </ul> <p>Following these steps will transform your application into a powerful AI-enhanced platform capable of executing advanced generative AI tasks and providing deeper insights from your data.</p>"},{"location":"05-Build-Copilot/#what-are-copilots","title":"What are copilots?","text":"<p>Copilots are advanced AI assistants designed to augment human capabilities and improve productivity by providing intelligent, context-aware support, automating repetitive tasks, and enhancing decision-making processes. For instance, the Woodgrove Bank copilot will assist in data analysis, helping users identify patterns and trends in financial datasets.</p>"},{"location":"05-Build-Copilot/#why-use-python","title":"Why use Python?","text":"<p>Python's simplicity and readability make it a popular programming language for AI and machine learning projects. Its extensive libraries and frameworks, such as LangChain, FastAPI, and many others, provide robust tools for developing sophisticated copilots. Python's versatility allows developers to iterate and experiment quickly, making it a top choice for building AI applications.</p>"},{"location":"05-Build-Copilot/01/","title":"5.1 Explore the API Codebase","text":"<p>In this step, you will review the backend API code and structure. You can create powerful and efficient AI copilots that streamline complex workflows by utilizing Python's versatile programming capabilities and Azure Database for PostgreSQL's vector search functionality. The copilot's backend API enriches its abilities to handle intricate data, provide real-time insights, and connect seamlessly with diverse services, making interactions more dynamic and informative.</p>"},{"location":"05-Build-Copilot/01/#api-implementation","title":"API Implementation","text":"<p>The Woodgrove Bank API is built using the FastAPI Python library. FastAPI is a modern, high-performance web framework designed to enable you to build APIs with Python based on standard Python type hints. By decoupling the copilot UI from the backend using this approach, you ensure greater flexibility, maintainability, and scalability, allowing the copilot's capabilities to evolve independently from the UI.</p> <p>The entry point of the FastAPI application is implemented in the <code>src/api/app/main.py</code> file. Open it now in Visual Studio Code and explore the code in sections. You can also expand the section below to see the code inline and review explanations for each line of code.</p> FASTAPI application code src/api/app/main.py<pre><code>from dotenv import load_dotenv\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom app.lifespan_manager import lifespan\nfrom app.routers import (\n    completions,\n    deliverables,\n    documents,\n    embeddings,\n    invoices,\n    invoice_line_items,\n    milestones,\n    sows,\n    status,\n    statuses,\n    validation,\n    validation_results,\n    vendors,\n    webhooks\n)\n\n# Load environment variables from the .env file\nload_dotenv()\n\n# Instantiate the FastAPI app\napp = FastAPI(\n    lifespan=lifespan,\n    title=\"Woodgrove Bank API\",\n    summary=\"Woodgrove Bank API for the Build Your Own Copilot with Azure Database for PostgreSQL Solution Accelerator\",\n    version=\"1.0.0\",\n    docs_url=\"/swagger\",\n    openapi_url=\"/swagger/v1/swagger.json\"\n)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Add routers to API endpoints\napp.include_router(deliverables.router)\napp.include_router(documents.router)\napp.include_router(embeddings.router)\napp.include_router(invoices.router)\napp.include_router(invoice_line_items.router)\napp.include_router(milestones.router)\napp.include_router(sows.router)\napp.include_router(status.router)\napp.include_router(statuses.router)\napp.include_router(validation.router)\napp.include_router(validation_results.router)\napp.include_router(vendors.router)\napp.include_router(webhooks.router)\n\n@app.get(\"/\")\nasync def get():\n    \"\"\"API welcome message.\"\"\"\n    return {\"message\": \"Welcome to the Woodgrove Bank API!\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000, log_level=\"info\")\n</code></pre> <ol> <li> <p>Import libraries (lines 1-20): Required classes and functions are imported from various libraries.</p> </li> <li> <p>Load environment variables (line 23). The load_dotenv() function imports environment variable values stored in the <code>.env</code> file you created in the <code>src\\api\\app</code> folder.</p> </li> <li> <p>Instantiate the FastAPI app server (lines 26-33). The FastAPI application server is created, and some basic configuration settings are applied.</p> </li> <li>Assign lifespan manager (line 27): The lifespan manager, defined in <code>src\\api\\app\\lifespan_manager.py</code> manages the lifespan of objects used throughout the life of the app, such as database connection pools and the Azure OpenAI chat client. This approach ensures objects can be shared across sessions, creating them when the app starts and destroying them gracefully when it shuts down. Dependency injection is used to access lifespan-managed objects from the API endpoint code on routers.</li> <li> <p>Assign Swagger metadata (lines 28-32): Various metadata fields add context to the API's exposed Swagger UI. The docs_url value changes the API's default documentation page from <code>/docs</code> to the more commonly used <code>/swagger</code>.</p> </li> <li> <p>Add endpoint routers (lines 44-56): In FastAPI, routers are components that help organize API code, grouping related endpoints and enabling modular route definitions for better maintainability and scalability of your application.</p> </li> <li> <p>Define a default route (lines 58-61): The \"/\" route maps to the application server's base URL.</p> <ul> <li>It accepts GET requests without parameters (equivalent to a browser site visit).</li> <li>It returns a JSON response with a \"Welcome to the Woodgrove Bank API\" message.</li> <li>This serves as a \"health check\" for the app server, verifying it is alive (e.g., during setup).</li> </ul> </li> </ol>"},{"location":"05-Build-Copilot/01/#organize-code-using-routers","title":"Organize Code using Routers","text":"<p>When building more extensive applications, routers allow API endpoint code to be split across multiple files, providing a convenient tool for structuring your application. FastAPI's <code>APIRouter</code> class allows path operations to be maintained in a dedicated code file, isolated from other paths and logic.</p> <p>For example, the Woodgrove Bank API contains a file dedicated to handling just vendors. This file, the submodule at <code>api/app/routers/vendors.py</code>, contains all the path operations related to vendors. The router separates vendor-specific logic from the rest of the application code. The router is connected to the FastAPI application using the <code>APIRouter</code> class.</p> Expand this block to view the Vendors router code src/api/app/routers/vendors.py<pre><code>from fastapi import APIRouter, Depends, HTTPException\nfrom pydantic import parse_obj_as\n\n# Initialize the router\nrouter = APIRouter(\n    prefix = \"/vendors\",\n    tags = [\"Vendors\"],\n    dependencies = [Depends(get_db_connection_pool)],\n    responses = {404: {\"description\": \"Not found\"}}\n)\n\n@router.get('/', response_model = ListResponse[Vendor])\nasync def list_vendors(skip: int = 0, limit: int = 10, sortby: str = None, pool = Depends(get_db_connection_pool)):\n    \"\"\"Retrieves a list of vendors from the database.\"\"\"\n    async with pool.acquire() as conn:\n        orderby = 'id'\n        if (sortby):\n            orderby = sortby\n\n        if limit == -1:\n            rows = await conn.fetch('SELECT * FROM vendors ORDER BY $1;', orderby)\n        else:\n            rows = await conn.fetch('SELECT * FROM vendors ORDER BY $1 LIMIT $2 OFFSET $3;', orderby, limit, skip)\n\n        vendors = parse_obj_as(list[Vendor], [dict(row) for row in rows])\n\n        total = await conn.fetchval('SELECT COUNT(*) FROM vendors;')\n\n    if (limit == -1):\n        limit = total\n\n    return ListResponse[Vendor](data = vendors, total = len(vendors), skip = 0, limit = len(vendors))\n\n@router.get('/{id:int}', response_model = Vendor)\nasync def get_by_id(id: int, pool = Depends(get_db_connection_pool)):\n    \"\"\"Retrieves a vendor by ID from the database.\"\"\"\n    async with pool.acquire() as conn:\n        row = await conn.fetchrow('SELECT * FROM vendors WHERE id = $1;', id)\n        if row is None:\n            raise HTTPException(status_code=404, detail=f'A vendor with an id of {id} was not found.')\n        vendor = parse_obj_as(Vendor, dict(row))\n    return vendor\n\n@router.get('/type/{type}', response_model = list[Vendor])\nasync def get_by_type(type: str, pool = Depends(get_db_connection_pool)):\n    \"\"\"Retrieves vendors of the specified type from the database.\"\"\"\n    async with pool.acquire() as conn:\n        rows = await conn.fetch('SELECT * FROM vendors WHERE LOWER(type) = $1;', type.lower())\n        if not rows or len(rows) == 0:\n            raise HTTPException(status_code=404, detail=f'No vendors with a type of \"{type}\" were found.')\n        vendors = parse_obj_as(list[Vendor], [dict(row) for row in rows])\n    return vendors\n</code></pre> <ol> <li> <p>Define the router (lines 5-10): The <code>APIRouter</code> initialization.</p> <ul> <li>The prefix allows you to specify the path prefix of all endpoints within the router. In this case, it is <code>/vendors</code>.</li> <li>Setting tags allows the endpoints within the router to be grouped by a friendly name in the Swagger UI.</li> <li>The dependencies array defines any dependencies injected into every endpoint request.</li> <li>The responses object allows you to dictate the types of responses from the API endpoints defined within the router.</li> </ul> </li> <li> <p>Define the get vendors route (lines 12-32). The API's <code>/vendors/</code> route maps to an endpoint for retrieving the list of vendors from the database.</p> <ul> <li>It accepts GET requests from clients and extracts optional parameters.</li> <li>It uses the injected database connection pool and invokes a SELECT query against the database to retrieve vendor records.</li> <li>It returns a list of vendors.</li> </ul> </li> <li> <p>Define additional get routes (lines 34-52). The <code>/vendors/{id:int}</code> and <code>vendors/type/{type}</code> routes provide GET request endpoints for getting individual vendors by ID or a list of vendors by type.</p> </li> </ol> <p>Congratulations! You just reviewed the FastAPI application structure!</p>"},{"location":"05-Build-Copilot/02/","title":"5.2 Review RAG Design","text":"<p>The solution leverages the Retrieval Augmented Generation (RAG) design pattern to ensure the copilot's responses are grounded in the (private) data maintained by Woodgrove Bank. RAG works by retrieving relevant information from your data stores and augmenting the model prompt to create a composite prompt. This enhanced prompt is used by the LLM to generate the final response or completion.</p> <p></p> <p>To understand how the RAG design pattern works in the context of the Woodgrove Bank Contract Management application, select each tab in order and review the sequence of events shown in the figure above.</p> 1. User Query2. Vectorize Query3. Retrieve Data4. Augment Query5. Generate Response <p>The user query arrives at our copilot implementation via the chat API endpoint.</p> <p>User queries are submitted via the copilot interface of the Woodgrove Bank Contract Management portal. These queries are sent to the backend API's <code>/chat</code> endpoint. The incoming user query has three components:</p> <ol> <li>The user's question (text input)</li> <li>An optional chat history (object array)</li> <li>And, a max history setting, which indicates the number of chat history messages to use in the prompt</li> </ol> <p>The API extracts these parameters from the incoming request and invokes the <code>/chat</code> endpoint, starting the workflow that reflects this RAG design pattern.</p> <p>Embeddings representing the user query are generated.</p> <p>When performing similarity searches to find data, the <code>/chat</code> endpoint sends the text of the user's request to Azure OpenAI, where it is vectorized using a large language \"Embedding\" model (e.g., Azure Open AI <code>text-embedding-ada-002</code>). This vector is then used in the query to retrieve similar records in the next step.</p> Generate Embeddings <p>The below <code>__create_query_embeddings</code> function in the Woodgrove Bank API uses the LangChain AzureOpenAIEmbeddings class to generate embeddings for the provided user query.</p> src/api/app/functions/chat_functions.py<pre><code>async def __create_query_embeddings(self, user_query: str):\n    \"\"\"\n    Generates vector embeddings for the user query.\n    \"\"\"\n    # Create embeddings using the LangChain Azure OpenAI Embeddings client\n    # This makes an API call to the Azure OpenAI service to generate embeddings,\n    # which can be used to compare the user query with vectorized data in the database.\n    query_embeddings = await self.embedding_client.aembed_query(user_query)\n    return query_embeddings\n</code></pre> <p>This step may be skipped in cases where raw data is requested, such as a list of vendors.</p> <p>Queries are executed against the database to retrieve (private) data.</p> <p>This step retrieves data from the PostgreSQL database to \"augment\" the prompt. Depending on the user query, hybrid search and direct data retrieval techniques may be used to obtain relevant records.</p> Improve RAG accuracy <p>The accuracy of the RAG pattern can be improved by using techniques like semantic ranking to order the returned results and GraphRAG to identify relationships between data. You will learn about these techniques in the next task.</p> <p>Select each tab below to learn more about the implementation of Hybrid Search and Direct Data Retrieval in the context of the Woodgrove Bank Contract Management application!</p> Hybrid SearchDirect Data Retrieval <p>Hybrid search in Azure Database for PostgreSQL combines traditional full-text search functionality with the vector similarity search capabilities enabled by the <code>azure_ai</code> and <code>vector</code> extensions to deliver highly relevant results. This dual approach leverages the precision of keyword matching with full-text search and the contextual understanding of vector search, ensuring that users obtain exact matches and semantically related content. This synergy enhances search efficiency, provides a richer user experience, and supports diverse use cases\u2014from technical document retrieval to broad content discovery\u2014making it an invaluable tool for modern copilots</p> Hybrid Search Example Code <p>The <code>find_invoice_validation_results</code> function below provides an example of the hybrid search technique used in the Woodgrove Bank API.</p> src/api/app/functions/chat_functions.py<pre><code>async def find_invoice_validation_results(self, user_query: str, invoice_id: int = None, vendor_id: int = None, sow_id: int = None):\n    \"\"\"\n    Retrieves invoice accuracy and performance validation results similar to the user query for specified invoice, vendor, or SOW.\n    If no invoice_id, vendor_id, or sow_id is provided, return all similar validation results.\n    \"\"\"\n    # Define the columns to retrieve from the table\n    # Exclude the embedding column in results\n    columns = [\"invoice_id\", \"datestamp\", \"result\", \"validation_passed\"]\n\n    # Get the embeddings for the user query\n    query_embeddings = await self.__create_query_embeddings(user_query)\n\n    # Use hybrid search to rank records, with exact matches ranked highest\n    columns.append(f\"\"\"CASE\n                        WHEN result ILIKE '%{user_query}%' THEN 0\n                        ELSE (embedding &lt;=&gt; '{query_embeddings}')::real\n                    END as rank\"\"\")\n\n    query = f'SELECT {\", \".join(columns)} FROM invoice_validation_results'\n\n    # Filter the validation results by invoice_id, vendor_id, or sow_id, if provided\n    if invoice_id is not None:\n        query += f' WHERE invoice_id = {invoice_id}'\n    else:\n        if vendor_id is not None:\n            query += f' WHERE vendor_id = {vendor_id}'\n            if sow_id is not None:\n                query += f' AND sow_id = {sow_id}'\n        elif sow_id is not None:\n            query += f' WHERE sow_id = {sow_id}'\n\n    query += f' ORDER BY rank ASC'\n\n    rows = await self.__execute_query(f'{query};')\n    return [dict(row) for row in rows]\n</code></pre> <p>In the code above: </p> <ol> <li> <p>The <code>CASE</code> statement on lines 14-17 handles the Hybrid Search.</p> <ol> <li> <p><code>WHEN result ILIKE '%{user_query}%'</code> performs a case-insensitive search for the exact text of the user query. If found, a <code>rank</code> of <code>0</code> is assigned to the record, which ranks these records as the highest or most similar matches.</p> </li> <li> <p>When an exact match is not found, the <code>ELSE</code> statement executes a vector similarity search using the cosine distance function, as indicated by the <code>&lt;=&gt;</code> vector operator, to compare the embedding representation of the user query (<code>query_embeddings</code>) from the previous step to values in the <code>embedding</code> column of the <code>invoice_validation_results</code> table. The similarity score of these matches is assigned as the <code>rank</code>. Scores closer to zero indicate a more semantically similar result.</p> </li> </ol> </li> <li> <p>If values are provided, the query is further refined to filter on a specific <code>invoice_id</code> or <code>vendor_id</code> and <code>sow_id</code>, as shown in lines 73-81.</p> </li> <li> <p>Finally, the results are ordered by <code>rank</code> to ensure the most relevant search results appear first in the rows returned.</p> </li> </ol> <p>In other cases, such as getting a list of all vendors, query vectorization is unnecessary. Direct data retrieval is handled via simple <code>SELECT</code> queries against the database to avoid the overhead of generating embeddings and querying vector fields.</p> Direct Data Retrieval Example Code <p>The <code>get_vendors</code> function below provides an example of a direct data retrieval technique used in the Woodgrove Bank API.</p> src/api/app/functions/chat_functions.py<pre><code>async def get_vendors(self):\n    \"\"\"Retrieves a list of vendors from the database.\"\"\"\n    rows = await self.__execute_query('SELECT * FROM vendors;')\n    return [dict(row) for row in rows]\n</code></pre> <p>The copilot creates a composite prompt with the retrieved data.</p> <p>The <code>/chat</code> API combines the user's original question with results returned from the database to create an enhanced or composite model prompt, augmenting the model with additional data to use when generating a response.</p> <p>The chat model uses the prompt to generate a grounded response.</p> <p>The composite prompt, grounded with (\"private\") data, is sent to a Large Language \"chat\" completion model, such as Azure OpenAI's <code>gpt-4o</code>. The completion model sees the enhanced prompt (hybrid search results and chat history) as the grounding context for generating the completion, improving the quality (e.g., relevance, groundedness) of the response returned from the Woodgrove Bank copilot.</p>"},{"location":"05-Build-Copilot/03/","title":"5.3 Leverage LangChain Orchestration","text":"<p>LangChain is a powerful tool that enhances the integration and coordination of multiple AI models and tools to create complex and dynamic AI applications. By leveraging orchestration capabilities, LangChain allows you to seamlessly combine various language models, APIs, and custom components into a unified workflow. This orchestration ensures that each element works together efficiently, enabling the creation of sophisticated applications capable of performing various tasks, from natural language understanding and generation to information retrieval and data analysis.</p> <p>LangChain's orchestration capabilities are particularly beneficial when building a copilot application using Python and Azure Database for PostgreSQL. Copilots must often combine natural language processing (NLP) models, knowledge retrieval systems, and custom logic to provide accurate and contextually relevant responses. LangChain facilitates this by orchestrating various NLP models and APIs, ensuring the copilot can effectively understand and generate responses to user queries.</p> <p>Moreover, integrating Azure Database for PostgreSQL with LangChain provides a scalable and flexible database solution that can handle large volumes of data with low latency. The PostgreSQL vector search features enabled by the <code>vector</code> extension allow for high-performance retrieval of relevant information based on the semantic similarity of data, which is especially useful for NLP applications. This means the copilot can perform sophisticated searches over large datasets, retrieving contextually relevant information for user queries.</p>"},{"location":"05-Build-Copilot/03/#rag-with-langchain","title":"RAG with LangChain","text":"<p>By leveraging LangChain's orchestration capabilities, RAG can seamlessly combine the retrieval of relevant information with the generative power of AI models. When a user poses a query, the RAG model can retrieve contextually appropriate data from PostgreSQL using hybrid search and then generate a comprehensive, coherent response based on the grounding data. This combination of retrieval and generation significantly enhances the copilot's ability to provide accurate, context-aware answers, leading to a more robust and user-friendly experience.</p>"},{"location":"05-Build-Copilot/03/#understand-function-calling-and-tools-in-langchain","title":"Understand function calling and tools in LangChain","text":"<p>Function calling in LangChain offers a more structured and flexible approach than using the Azure OpenAI client directly in Python. In LangChain, you can define and manage functions as modular components that are easily reusable and maintainable. This approach allows for more organized code, where each function encapsulates a specific task, reducing complexity and making the development process more efficient.</p> <p>When using the Azure OpenAI client in Python, function calls are typically limited to direct API interactions. While you can still build complex workflows, it often requires more manual orchestration and handling of asynchronous operations, which can become cumbersome and more challenging to maintain as the application grows.</p> <p>LangChain's tools play a crucial role in enhancing function calling. With a vast array of built-in tools and the ability to integrate external ones, LangChain allows you to create sophisticated pipelines where functions can call tools to perform specific operations, such as data retrieval, processing, or transformation. These tools can be configured to operate conditionally or in parallel, further optimizing the application's performance. Additionally, LangChain's tools simplify error handling and debugging by isolating functions and tools, making identifying and resolving issues easier.</p>"},{"location":"05-Build-Copilot/04/","title":"5.4 Enable the Chat Endpoint","text":"<p>In this step, you will review the backend API code for the <code>/chat</code> endpoint in the <code>completions</code> router. You will then add the <code>completions</code> router to the FastAPI application to make the <code>/chat</code> endpoint available.</p>"},{"location":"05-Build-Copilot/04/#review-chat-endpoint-implementation","title":"Review Chat Endpoint Implementation","text":"<p>The Woodgove Bank API exposes endpoints in various routers... The <code>chat</code> endpoint resides in the <code>completions</code> router, defined in the <code>src/api/app/routers/completions.py</code> file. Open it now in VS Code and explore the code in sections. You can also expand the section below to see the code inline and review explanations for each line of code.</p> Chat endpoint code src/api/app/routers/completions.py<pre><code>from app.functions.chat_functions import ChatFunctions\nfrom app.lifespan_manager import get_chat_client, get_db_connection_pool, get_embedding_client, get_prompt_service\nfrom app.models import CompletionRequest\nfrom fastapi import APIRouter, Depends\nfrom langchain.agents import AgentExecutor, create_openai_functions_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.tools import StructuredTool\n\n# Initialize the router\nrouter = APIRouter(\n    prefix = \"/completions\",\n    tags = [\"Completions\"],\n    dependencies = [Depends(get_chat_client)],\n    responses = {404: {\"description\": \"Not found\"}}\n)\n\n@router.post('/chat', response_model = str)\nasync def generate_chat_completion(\n    request: CompletionRequest,\n    llm = Depends(get_chat_client),\n    db_pool = Depends(get_db_connection_pool),\n    embedding_client = Depends(get_embedding_client),\n    prompt_service = Depends(get_prompt_service)):\n    \"\"\"Generate a chat completion using the Azure OpenAI API.\"\"\"\n\n    # Retrieve the copilot prompt\n    system_prompt = prompt_service.get_prompt(\"copilot\")\n\n    # Provide the copilot with a persona using the system prompt.\n    messages = [{ \"role\": \"system\", \"content\": system_prompt }]\n\n    # Add the chat history to the messages list\n    # Chat history provides context of previous questions and responses for the copilot.\n    for message in request.chat_history[-request.max_history:]:\n        messages.append({\"role\": message.role, \"content\": message.content})\n\n    # Create a chat prompt template\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", system_prompt),\n            MessagesPlaceholder(\"chat_history\", optional=True),\n            (\"user\", \"{input}\"),\n            MessagesPlaceholder(\"agent_scratchpad\")\n        ]\n    )\n\n    # Get the chat functions\n    cf = ChatFunctions(db_pool, embedding_client)\n\n    # Define tools for the agent to retrieve data from the database\n    tools = [\n        # Hybrid search functions\n        StructuredTool.from_function(coroutine=cf.find_invoice_line_items),\n        StructuredTool.from_function(coroutine=cf.find_invoice_validation_results),\n        StructuredTool.from_function(coroutine=cf.find_milestone_deliverables),\n        StructuredTool.from_function(coroutine=cf.find_sow_chunks),\n        StructuredTool.from_function(coroutine=cf.find_sow_validation_results),\n        # Get invoice data functions\n        StructuredTool.from_function(coroutine=cf.get_invoice_id),\n        StructuredTool.from_function(coroutine=cf.get_invoice_line_items),\n        StructuredTool.from_function(coroutine=cf.get_invoice_validation_results),\n        StructuredTool.from_function(coroutine=cf.get_invoices),\n        # Get SOW data functions\n        StructuredTool.from_function(coroutine=cf.get_sow_chunks),\n        StructuredTool.from_function(coroutine=cf.get_sow_id),\n        StructuredTool.from_function(coroutine=cf.get_sow_milestones),\n        StructuredTool.from_function(coroutine=cf.get_milestone_deliverables),\n        StructuredTool.from_function(coroutine=cf.get_sow_validation_results),\n        StructuredTool.from_function(coroutine=cf.get_sows),\n        # Get vendor data functions\n        StructuredTool.from_function(coroutine=cf.get_vendors)\n    ]\n\n    # Create an agent\n    agent = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)\n    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)\n    completion = await agent_executor.ainvoke({\"input\": request.message, \"chat_history\": messages})\n    return completion['output']\n</code></pre> <ol> <li> <p>Import libraries (lines 1-7): Required classes and functions are imported from various libraries.</p> </li> <li> <p>Intialize the router (lines 10-15): This is the <code>completions</code> router, assigning the route prefix, dependencies, and other metadata.</p> </li> <li> <p>Define the chat endpoint (lines 17-23): The <code>/chat</code> endpoint is the entry point into the Woodgove Bank copilot implementation. It expects a <code>CompletionRequest</code>, which contains the user query, the chat history, and the maximum number of history messages to include in the prompt, and returns a text response.</p> <ul> <li>It accepts POST requests from clients and extracts required parameters.</li> <li>It invokes the get_chat_completion function with those parameters.</li> <li>It returns the LLM's response to the client.</li> </ul> </li> <li> <p>Chat endpoint implementation (lines 24-78). The \"/completions/chat\" route maps to the endpoint where we can invoke the Contoso Chat implementation.</p> <ul> <li> <p>Get the system prompt (line 27): The system prompt defines the copilot's persona, providing instructions about how the copilot should behave, respond to questions, and interact with customers. It also provides guidance about the RAG design pattern and how function calls (tools) should be used when answering questions. You will look at this in detail in the Prompt Engineering step of this section.</p> </li> <li> <p>Build messages collection (lines 30-35): The messages collection provides the LLM with system and user prompts and chat history messages. Each message consists of a <code>role</code> and message <code>content</code>. The role will be <code>system</code>, <code>assistant</code>, or <code>user</code>. After the <code>system</code> message, all subsequent messages must be <code>user</code> / <code>assistant</code> pairs, with a user query followed by an assistant response.</p> </li> <li> <p>Build the model prompt (lines 38-45): The LangChain <code>ChatPromptTemplate</code> class allows you to build a model prompt from a collection of messages.</p> <ul> <li>The system prompt is added to provide instructions to the model.</li> <li>The chat history is inserted as context about previous questions and responses.</li> <li>The user input provides the model with the current question it is attempting to answer.</li> <li>An agent scratchpad placeholder is included to allow responses from tools assigned to the agent to augment the model with grounding data.</li> <li>The resulting prompt provides a structured input for the conversational AI agent, helping it to generate a response based on the given context.</li> </ul> </li> <li> <p>Implement function calling (lines 48-72):</p> <ul> <li>Line 48 instantiates the <code>ChatFunctions</code> class, which contains the methods for interacting with the PostgreSQL database. You can review the functions in the <code>src/api/app/functions/chat_functions.py</code> file.</li> <li>The <code>tools</code> array created in lines 51-72 is the collection of functions available to the LangChain agent for performing retrieval operations to augment the model prompt during response generation.</li> <li> <p>Tools are created using the <code>StructuredTool.from_function</code> method provided by LangChain.</p> About the LangChain <code>StructuredTool</code> class <p>The <code>StructuredTool</code> class is a wrapper that allows LangChain agents to interact with functions. The <code>from_function</code> method creates a tool from the given function, describing the function using its input parameters and docstring description. To use it with async methods, you pass the function's name to the <code>coroutine</code> input parameter.</p> <p>In Python, a docstring (short for documentation string) is a special type of string used to document a function, method, class, or module. It provides a convenient way of associating documentation with Python code and is typically enclosed within triple quotes (\"\"\" or '''). Docstrings are placed immediately after the definition of the function (or method, class, or module) they document.</p> <p>Using the <code>StructuredTool.from_function</code> method automates the creation of the JSON function definitions required by Azure OpenAI function calling methods, simplifying function calling when using LangChain.</p> </li> </ul> </li> <li> <p>Create a LangChain agent (lines 75-76): The LangChain agent is responsible for interacting with the LLM to generate a response.</p> <ul> <li> <p>Using the <code>create_openai_functions_agent</code> method, a LangChain agent is instantiated. This agent handles function calling via the <code>tools</code> provided to the agent.</p> About the <code>create_openai_functions_agent</code> function <p>The <code>create_openai_functions_agent</code> function in LangChain creates an agent that can call external functions to perform tasks using a specified language model and tools. This enables the integration of various services and functionalities into the agent's workflow, providing flexibility and enhanced capabilities.</p> </li> <li> <p>LangChain's <code>AgentExecutor</code> class manages the agent's execution flow. It handles the processing of inputs, the invocation of tools or models, and the handling of outputs.</p> About LangChain's <code>AgentExecutor</code> <p>The <code>AgentExecutor</code> ensures that all the steps required to generate a response are executed in the correct order. It abstracts the complexities of execution for agents, providing an additional layer of functionality and structure, and making it easier to build, manage, and scale sophisticated agents.</p> </li> </ul> </li> <li> <p>Invoke the agent (line 77): The agent executor's async <code>invoke</code> method sends the incoming user message and chat history to the LLM.</p> <ul> <li>The <code>input</code> and <code>chat_history</code> tokens were defined in the prompt object created using the <code>ChatPromptTemplate</code>. The <code>invoke</code> method injects these into the model prompt, allowing the LLM to use that information when generating a response.</li> <li>The LangChain agent uses the LLM to determine if tool calls are necessary by evaluating the user query.</li> <li>Any tools required to answer the question are called, and the model prompt is augmented with grounding data from their results to formulate the final response.</li> </ul> </li> <li> <p>Return the response (line 78): The agent's completion response is returned to the user.</p> </li> </ul> </li> </ol>"},{"location":"05-Build-Copilot/04/#enable-chat-endpoint-calls","title":"Enable Chat Endpoint Calls","text":"<p>To enable the <code>/chat</code> endpoint to be called from the Woodgrove Bank Contract Management Portal, you will add the completions router to the FastAPI app.</p> <ol> <li> <p>In the VS Code Explorer, navigate to the <code>src/api/app</code> folder and open the <code>main.py</code> file.</p> </li> <li> <p>Locate the block of code where the API endpoint routers are added (lines 44-56).</p> </li> <li> <p>Insert the following code at the start of that block (just below the <code># Add routers to API endpoints</code> comment on line 43) to add the <code>completions/chat</code> endpoint to the exposed API.</p> Python<pre><code>app.include_router(completions.router)\n</code></pre> </li> <li> <p>The updated list of routers should look like this:</p> Python<pre><code># Add routers to API endpoints\napp.include_router(completions.router)\napp.include_router(deliverables.router)\napp.include_router(documents.router)\napp.include_router(embeddings.router)\napp.include_router(invoices.router)\napp.include_router(invoice_line_items.router)\napp.include_router(milestones.router)\napp.include_router(sows.router)\napp.include_router(status.router)\napp.include_router(statuses.router)\napp.include_router(validation.router)\napp.include_router(validation_results.router)\napp.include_router(vendors.router)\napp.include_router(webhooks.router)\n</code></pre> </li> <li> <p>Save the <code>main.py</code> file.</p> </li> </ol> <p>Congratulations! Your API is now enabled for copilot interactions!</p>"},{"location":"05-Build-Copilot/05/","title":"5.5 Test the Chat API","text":"<p>Now, all you need to do is run the FastAPI server and have it listen for incoming client requests on the API's <code>completions/chat</code> route. In this next section, you will see how to do this locally for rapid prototyping and testing.</p>"},{"location":"05-Build-Copilot/05/#testing-options","title":"Testing Options","text":"<p>The Chat API is deployed against the <code>/completions/chat</code> endpoint. So, how can you test this?</p> <ul> <li>You can use a third-party client to <code>POST</code> a request to the endpoint</li> <li>You can use a <code>CURL</code> command to make the request from the command line</li> <li> <p>You can use the built-in <code>/swagger</code> Swagger UI to try it out interactively</p> <p>Swagger UI</p> <p>Recall that the <code>src/api/app/main.py</code> file contained the definition for the FastAPI application. In that definition, the default documentation endpoint for the API was modified to use the more common <code>/swagger</code> path.</p> </li> </ul>"},{"location":"05-Build-Copilot/05/#test-with-swagger","title":"Test with Swagger","text":"<p>The Swagger UI provides an easy and intuitive way to test your endpoints rapidly. A side benefit of this approach is it shows you the <code>curl</code> command you can use to make the same request from the terminal if you want to try that out later.</p> <ol> <li> <p>In Visual Studio Code, select the Run and Debug icon from the Activity Bar on the left-hand side.</p> <p></p> </li> <li> <p>At the top of the Run and Debug menu, select the API Debugger option for your OS from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Once the API debugging session has started, indicated by an <code>Application startup complete.</code> message in the terminal output, followed by the URL on which Uvicorn is running, open the local API URL in a web browser (http://127.0.0.1:8000/).</p> <p></p> </li> <li> <p>Append <code>/swagger</code> to the URL to get the Swagger UI interactive testing page.</p> <p></p> </li> <li> <p>Expand the <code>POST</code> section for the <code>completions/chat</code> endpoint under the Completions tag and select <code>Try it out</code>:</p> <p></p> </li> <li> <p>In the Request body section, paste the following JSON snippet to request a list of vendors, then select Execute.</p> <p><code>json {     \"message\": \"What vendors are we working with?\",     \"chat_history\": [],     \"max_history\": 6 }</code></p> <p></p> </li> <li> <p>Scroll down to the Responses section for the <code>/completions/chat</code> endpoint in the Swagger UI and verify you received a valid response.</p> <p></p> View server execution traces in the VS Code console. <p>By running a debug session in VS Code, you can see the server execution traces of a request to the Chat endpoint in the Visual Studio Code console. These traces allow you to observe the agent scratchpad output from your LangChain agent, which can be useful in troubleshooting and understanding how the agent works.</p> VS Code terminal with LangChain output<pre><code>&gt; Entering new AgentExecutor chain...\n\nInvoking: `get_vendors` with `{}`\n\n[{'id': 1, 'name': 'Adatum Corporation', 'address': '789 Goldsmith Road, MainTown City', 'contact_name': 'Elizabeth Moore', 'contact_email': 'elizabeth.moore@adatum.com', 'contact_phone': '555-789-7890', 'website': 'http://www.adatum.com', 'type': 'Data Engineering'}, {'id': 2, 'name': 'Contoso Ltd.', 'address': '456 Industrial Road, Scooton City', 'contact_name': 'Nicole Wagner', 'contact_email': 'nicole@contoso.com', 'contact_phone': '555-654-3210', 'website': 'http://www.contoso.com', 'type': 'Software Engineering'}, {'id': 3, 'name': 'Lucerne Publishing', 'address': '789 Live Street, Woodgrove', 'contact_name': 'Ana Bowman', 'contact_email': 'abowman@lucernepublishing.com', 'contact_phone': '555-654-9870', 'website': 'http://www.lucernepublishing.com', 'type': 'Graphic Design'}, {'id': 4, 'name': 'VanArsdel Ltd.', 'address': '123 Innovation Drive, TechVille', 'contact_name': 'Gabriel Diaz', 'contact_email': 'gdiaz@vanarsdelltd.com', 'contact_phone': '555-321-0987', 'website': 'http://www.vanarsdelltd.com', 'type': 'Software Engineering'}, {'id': 5, 'name': 'Trey Research', 'address': '456 Research Avenue, Redmond', 'contact_name': 'Serena Davis', 'contact_email': 'serena.davis@treyresearch.net', 'contact_phone': '555-867-5309', 'website': 'http://www.treyresearch.net', 'type': 'DevOps'}, {'id': 6, 'name': 'Fabrikam Inc.', 'address': '24601 South St., Philadelphia', 'contact_name': 'Remy Morris', 'contact_email': 'remy.morris@fabrikam.com', 'contact_phone': '610-321-0987', 'website': 'http://www.fabrikam.com', 'type': 'AI Services'}, {'id': 7, 'name': 'The Phone Company', 'address': '10642 Meridian St., Indianapolis', 'contact_name': 'Ashley Schroeder', 'contact_email': 'ashley.schroeder@thephone-company.com', 'contact_phone': '719-444-2345', 'website': 'http://www.thephone-company.com', 'type': 'Communications'}]Here are the vendors we are currently working with:\n\n1. **Adatum Corporation**\n   - **Type:** Data Engineering\n   - **Address:** 789 Goldsmith Road, MainTown City\n   - **Contact:** Elizabeth Moore\n   - **Email:** [elizabeth.moore@adatum.com](mailto:elizabeth.moore@adatum.com)\n   - **Phone:** 555-789-7890\n   - **Website:** [www.adatum.com](http://www.adatum.com)\n\n2. **Contoso Ltd.**\n   - **Type:** Software Engineering\n   - **Address:** 456 Industrial Road, Scooton City\n   - **Contact:** Nicole Wagner\n   - **Email:** [nicole@contoso.com](mailto:nicole@contoso.com)\n   - **Phone:** 555-654-3210\n   - **Website:** [www.contoso.com](http://www.contoso.com)\n\n3. **Lucerne Publishing**\n   - **Type:** Graphic Design\n   - **Address:** 789 Live Street, Woodgrove\n   - **Contact:** Ana Bowman\n   - **Email:** [abowman@lucernepublishing.com](mailto:abowman@lucernepublishing.com)\n   - **Phone:** 555-654-9870\n   - **Website:** [www.lucernepublishing.com](http://www.lucernepublishing.com)\n\n4. **VanArsdel Ltd.**\n   - **Type:** Software Engineering\n   - **Address:** 123 Innovation Drive, TechVille\n   - **Contact:** Gabriel Diaz\n   - **Email:** [gdiaz@vanarsdelltd.com](mailto:gdiaz@vanarsdelltd.com)\n   - **Phone:** 555-321-0987\n   - **Website:** [www.vanarsdelltd.com](http://www.vanarsdelltd.com)\n\n5. **Trey Research**\n   - **Type:** DevOps\n   - **Address:** 456 Research Avenue, Redmond\n   - **Contact:** Serena Davis\n   - **Email:** [serena.davis@treyresearch.net](mailto:serena.davis@treyresearch.net)\n   - **Phone:** 555-867-5309\n   - **Website:** [www.treyresearch.net](http://www.treyresearch.net)\n\n6. **Fabrikam Inc.**\n   - **Type:** AI Services\n   - **Address:** 24601 South St., Philadelphia\n   - **Contact:** Remy Morris\n   - **Email:** [remy.morris@fabrikam.com](mailto:remy.morris@fabrikam.com)\n   - **Phone:** 610-321-0987\n   - **Website:** [www.fabrikam.com](http://www.fabrikam.com)\n\n7. **The Phone Company**\n   - **Type:** Communications\n   - **Address:** 10642 Meridian St., Indianapolis\n   - **Contact:** Ashley Schroeder\n   - **Email:** [ashley.schroeder@thephone-company.com](mailto:ashley.schroeder@thephone-company.com)\n   - **Phone:** 719-444-2345\n   - **Website:** [www.thephone-company.com](http://www.thephone-company.com)\n\n&gt; Finished chain.\n</code></pre> <p>In the trace above:</p> <ol> <li> <p>Line 1 shows the <code>AgentExecutor</code> chain starting.</p> </li> <li> <p>Lines 3-5 show the agent invoking the <code>get_vendors</code> method and outputting the results into an array of vendor data.</p> </li> <li> <p>Lines 7-61 show the response from the LLM in markdown format.</p> </li> <li> <p>Line 63 shows the <code>AgentExecutor</code> chain has finished.</p> </li> </ol> </li> <li> <p>Stop the API debugger in VS Code.</p> </li> </ol> <p>You have successfully tested your <code>/chat</code> API endpoint!</p>"},{"location":"05-Build-Copilot/06/","title":"5.6 Add Copilot Chat To UI","text":"<p>In this step, you will review the <code>CopilotChat</code> component of the front-end application and enable it so you can chat with your copilot via the Portal UI. You can create an interactive and responsive chat interface that enhances user engagement by leveraging React's dynamic component-based architecture and its seamless integration with real-time messaging APIs. The copilot chat feature empowers the application to provide real-time conversation capabilities, handle complex state management, and deliver intuitive interactions that make the user experience more engaging and efficient.</p>"},{"location":"05-Build-Copilot/06/#review-copilot-chat-ui-component","title":"Review Copilot Chat UI Component","text":"<p>The Woodgrove Bank Contract Management Portal is a single-page application (SPA) built using React.js, a popular JavaScript framework for interactive user interfaces.</p> <p>A React component, <code>CopilotChat</code>, has been provided to allow you to easily integrate the copilot capability into the UI of the portal application. This component is implemented in the <code>src/userportal/src/components/CopilotChat.jsx</code> file. Open it now in VS Code and explore the code in sections. You can also expand the section below to see the code inline and review the explanations for each line of code.</p> Copilot Chat REACT component code. src/userportal/src/components/CopilotChat.jsx<pre><code>import React, { useState, useEffect, useRef } from 'react';\nimport ReactMarkdown from 'react-markdown';\nimport { Row, Col, Button, OverlayTrigger, Tooltip } from 'react-bootstrap';\nimport ConfirmModal from './ConfirmModal'; \nimport api from '../api/Api'; // Adjust the path as necessary\nimport './CopilotChat.css';\n\nconst CopilotChat = () =&gt; {\n  const [sessionId, setSessionId] = useState(-1);\n  const [messages, setMessages] = useState([]);\n  const [input, setInput] = useState('');\n  const messagesEndRef = useRef(null);\n  const [error, setError] = useState('');\n  const [isThinking, setIsThinking] = useState(false);\n\n  const [sessions, setSessions] = useState([]);\n  const [sessionToDelete, setSessionToDelete] = useState(null);\n  const [showDeleteModal, setShowDeleteModal] = useState(false);\n\n  const handleSendMessage = async () =&gt; {\n    if (input.trim() === '') return;\n\n    const prompt = input;\n    setInput('');\n\n    setIsThinking(true);\n\n    // Add the user's message to the local mesage history\n    const userMessage = { role: 'user', content: prompt };\n    setMessages([...messages, userMessage]);\n\n    setError('');\n\n    try {\n      // Get the completion from the API\n      const output = await api.completions.chat(sessionId, prompt);\n\n      // make sure request for a different session doesn't update the messages\n      if (sessionId === output.session_id) {\n        // Add the assistant's response to the messages\n        const assistantMessage = { role: 'assistant', content: output.content };\n        setMessages([...messages, userMessage, assistantMessage]);\n      }\n\n      // only update the messages if the session ID is the same\n      // This keeps a processing completion from updating messages after a new session is created\n      if (sessionId === -1 || sessionId !== output.session_id) {\n        // Update the session ID\n        setSessionId(output.session_id);\n      }\n    } catch (error) {\n      console.error('Error sending message:', error);\n      setError('Error sending message. Please try again.');\n    } finally {\n        setIsThinking(false);\n    }\n\n  };\n\n  const createNewSession = async () =&gt; {\n    setSessionId(-1);\n    setMessages([]);\n    setIsThinking(false);\n    setError('');\n  };\n\n  const refreshSessionList = async () =&gt; {\n    try {\n      const data = await api.completions.getSessions();\n      setSessions(data);\n    } catch (error) {\n      console.error('Error loading session history:', error);\n      setError('Error loading session history. Please try again.');\n    }\n  }\n\n  const loadSessionHistory = async () =&gt; {\n    if (!sessionId || sessionId &lt;= 0) {\n      setMessages([]);\n      return;\n    }\n    try {\n      const data = await api.completions.getHistory(sessionId);\n      setMessages(data);\n    } catch (error) {\n      console.error('Error loading session history:', error);\n      setError('Error loading session history. Please try again.');\n    }\n  }\n\n  useEffect(() =&gt; {\n    refreshSessionList();\n    loadSessionHistory();\n  }, [sessionId]);\n\n  useEffect(() =&gt; {\n    if (messagesEndRef.current) {\n      messagesEndRef.current.scrollIntoView({ behavior: 'smooth' });\n    }\n  }, [messages]);\n\n  useEffect(() =&gt; {\n    refreshSessionList();\n  }, []);\n\n  const handleDelete = async () =&gt; {\n    if (!sessionToDelete) return;\n\n    setError(null);\n    try {\n      await api.completions.deleteSession(sessionToDelete);\n\n      console.log('Session deleted:', sessionToDelete);\n      console.log('Current session:', sessionId);\n      if (sessionId === sessionToDelete) {\n        setSessionId(-1);\n      }\n    } catch (err) {\n      console.error('Error deleting session:', err);\n      setError('Error deleting session. Please try again.');\n    }\n    setShowDeleteModal(false);\n    refreshSessionList();\n  };\n\n  return (\n    &lt;div className=\"ai-chat container mt-4\"&gt;\n      &lt;Row&gt;\n        &lt;Col style={{ width: '10%', maxWidth: '10em' }}&gt;\n          &lt;Row&gt;\n            &lt;Button area-label=\"New Session\" alt=\"New Session\" onClick={createNewSession}&gt;\n              &lt;i className=\"fas fa-plus\"&gt;&lt;/i&gt; Chat\n            &lt;/Button&gt;\n          &lt;/Row&gt;\n          &lt;Row className=\"mt-3\"&gt;\n            &lt;strong&gt;Chat History&lt;/strong&gt;\n            {!sessions || sessions.length === 0 &amp;&amp; &lt;p&gt;No sessions&lt;/p&gt;}\n            {sessions &amp;&amp; sessions.length &gt; 0 &amp;&amp; &lt;ul className=\"session-list\"&gt;\n              {sessions.map((session, index) =&gt; (\n                  &lt;li key={index}\n                    className={`session ${sessionId === session.id ? 'selected' : ''}`}\n                    style={{ display: 'flex', justifyContent: 'space-between', alignItems: 'center', padding: '10px', borderBottom: '1px solid #ccc', cursor: 'pointer' }}\n                    onClick={() =&gt; setSessionId(session.id)}\n                  &gt;\n                    &lt;OverlayTrigger\n                      placement=\"top\"\n                      delay={{ show: 250, hide: 400 }}\n                      overlay={&lt;Tooltip id={`tooltip-${index}`}&gt;{session.name.substring(0, 300)}&lt;/Tooltip&gt;}\n                    &gt;\n                      &lt;a alt={session.name}&gt;{session.name}&lt;/a&gt;\n                    &lt;/OverlayTrigger&gt;\n                    &lt;div&gt;\n                      &lt;OverlayTrigger\n                        placement=\"top\"\n                        delay={{ show: 250, hide: 400 }}\n                        overlay={&lt;Tooltip id={`delete-tooltip-${index}`}&gt;Delete Session&lt;/Tooltip&gt;}\n                      &gt;\n                        &lt;Button className=\"btn-danger\" style={{ marginRight: '10px' }}\n                          title=\"Delete Session\"\n                          onClick={(e) =&gt; { setSessionToDelete(session.id); setShowDeleteModal(true); e.stopPropagation(); }}&gt;\n                          &lt;i className=\"fas fa-trash\"&gt;&lt;/i&gt;\n                        &lt;/Button&gt;\n                      &lt;/OverlayTrigger&gt;\n                    &lt;/div&gt;\n                  &lt;/li&gt;\n                ))}\n              &lt;/ul&gt;}\n          &lt;/Row&gt;\n        &lt;/Col&gt;\n        &lt;Col&gt;\n          &lt;div className=\"messages mb-3 border p-3\" style={{ minHeight: '20em', maxHeight: '50em', overflowY: 'scroll' }}&gt;\n            {messages.map((msg, index) =&gt; (\n              &lt;div key={index} className={`message ${msg.role} mb-2 d-flex ${msg.role === 'user' ? 'justify-content-end' : 'justify-content-start'}`}&gt;\n                {!error &amp;&amp; index === messages.length - 1 &amp;&amp; &lt;div ref={messagesEndRef} /&gt;}\n                &lt;div className={`alert ${msg.role === 'user' ? 'alert-primary' : 'alert-secondary'}`} style={{ maxWidth: '90%' }} role=\"alert\"&gt;\n                  &lt;ReactMarkdown&gt;{msg.content}&lt;/ReactMarkdown&gt;\n                &lt;/div&gt;\n              &lt;/div&gt;\n            ))}\n            {error &amp;&amp; &lt;div className=\"alert alert-danger\" role=\"alert\"&gt;{error}&lt;div ref={messagesEndRef} /&gt;&lt;/div&gt;}\n            {isThinking &amp;&amp; &lt;div className=\"d-flex justify-content-center\"&gt;\n                &lt;div className=\"spinner-border text-info\" role=\"status\"&gt;\n                  &lt;span className=\"visually-hidden\"&gt;Thinking...&lt;/span&gt;\n                &lt;/div&gt;\n                &lt;div ref={messagesEndRef} /&gt;\n              &lt;/div&gt;}\n          &lt;/div&gt;\n          &lt;div className=\"input-container d-flex\"&gt;\n            &lt;textarea className=\"form-control me-2\"\n              value={input}\n              onChange={(e) =&gt; setInput(e.target.value)}\n              onKeyDown={(e) =&gt; { if (e.key === 'Enter') { handleSendMessage(e); e.preventDefault(); return false; } }}\n              placeholder=\"Type a message...\"\n            &gt;&lt;/textarea&gt;\n            &lt;Button onClick={handleSendMessage}&gt;Send&lt;/Button&gt;\n          &lt;/div&gt;\n        &lt;/Col&gt;\n      &lt;/Row&gt;\n\n      &lt;ConfirmModal\n        show={showDeleteModal}\n        handleClose={() =&gt; setShowDeleteModal(false)}\n        handleConfirm={handleDelete}\n        message=\"Are you sure you want to delete this session?\"\n      /&gt;\n    &lt;/div&gt;\n  );\n};\n\nexport default CopilotChat;\n</code></pre> <ol> <li> <p>Import components and libraries (lines 1-6): Required components and libraries are imported.</p> </li> <li> <p>Define the <code>CopilotChat</code> functional component (line 8). Components in React.js are created using a functional component or class component.</p> </li> <li> <p>Declare state variables (lines 9-18): State variables are used to maintain the chat session state, including message history.</p> </li> <li> <p>Provide function for sending messages to the API (lines 20-58): The <code>handleSendMessage</code> function sends messages asynchronously to the Woodgrove Bank API. This function handles the UI's interaction with the backend <code>/completions/chat</code> endpoint, sending the user query and session id to the API.</p> </li> <li> <p>Provide functions for handling loading chat sessions (lines 60-89): The <code>createNewSession</code> function sets up the state variables to start a new chat session, the <code>refreshSessionList</code> function handles the UI's interaction with the backend <code>/completions</code> endpoint for loading previous chat sessions, and the <code>loadSessionHistory</code> function handles the UI's interaction with the backend <code>/completions</code> endpoint for loading the chat history for the selected chat session.</p> </li> <li> <p>Handle changes in the Session ID (lines 91-94): The <code>useEffect</code> hook is used to run code in response to changes in the sessionId to refresh the chat session list and load the chat history for the newly selected chat session.</p> </li> <li> <p>Handle changes in the messages collection (lines 96-100): The <code>useEffect</code> hook is used to run code in response to changes in the messages array.</p> </li> <li> <p>Handle loading chat session list on page load (lines 102-104): The <code>useEffect</code> hook is used to run code that loads the list of previous chat sessions into the UI on page load.</p> </li> <li> <p>Provide function for deleting chat session (lines 106-124): The <code>handleDelete</code> function handles the UI's interaction with the backend <code>/completions</code> endpoint for deleting the chosen chat session.</p> </li> <li> <p>Return the component (lines 126-208): The return statement renders the component's JSX, defining how it is presented in the web browser.</p> </li> <li> <p>Export the <code>CopilotChat</code> component (line 210): Exports the <code>CopilotChat</code> component as the default export.</p> </li> </ol>"},{"location":"05-Build-Copilot/06/#enable-the-copilot-chat-ui","title":"Enable the Copilot Chat UI","text":"<p>To enable the copilot chat feature in the Woodgrove Bank Contract Management Portal UI, you will add a reference to the component on the <code>Dashboard</code> page of the UI. The dashboard page is defined in the <code>src/userportal/src/pages/dashboard/dashboard.jsx</code> file. Expand the section below to review the code for the page below.</p> Dashboard page code src/userportal/src/pages/dashboard/dashboard.jsx<pre><code>import React from 'react';\n\nconst Dashboard = () =&gt; {\n  return (\n    &lt;div className=\"table-responsive\"&gt;\n      &lt;div className=\"d-flex justify-content-between flex-wrap flex-md-nowrap align-items-center pt-3 pb-2 mb-3 border-bottom\"&gt;\n        &lt;h1 className=\"h2\"&gt;Dashboard&lt;/h1&gt;\n      &lt;/div&gt;\n\n    &lt;/div&gt;\n  );\n};\n\nexport default Dashboard;\n</code></pre> <ol> <li> <p>In the VS Code Explorer, navigate to the <code>src/userportal/src/pages/dashboard</code> folder and open the <code>dashboard.jsx</code> file.</p> </li> <li> <p>To import the AI chat component, insert the following <code>import</code> statement directly below the <code>import React from 'react';</code> line at the top of the file.</p> JavaScript<pre><code>import CopilotChat from '../../components/CopilotChat';\n</code></pre> </li> <li> <p>Insert the following code below the closing tag of <code>&lt;div&gt;</code> containing the Dashboard header (line 9). This will insert the component within the <code>const Dashboard =() =&gt; {}</code> functional component block of the dashboard page.</p> JavaScript<pre><code>&lt;CopilotChat /&gt;\n</code></pre> </li> <li> <p>The final <code>Dashboard</code> code should look like the following:</p> src/userportal/src/pages/dashboard/dashboard.jsx<pre><code>import React from 'react';\nimport CopilotChat from '../../components/CopilotChat';\n\nconst Dashboard = () =&gt; {\n  return (\n    &lt;div className=\"table-responsive\"&gt;\n      &lt;div className=\"d-flex justify-content-between flex-wrap flex-md-nowrap align-items-center pt-3 pb-2 mb-3 border-bottom\"&gt;\n        &lt;h1 className=\"h2\"&gt;Dashboard&lt;/h1&gt;\n      &lt;/div&gt;\n      &lt;CopilotChat /&gt;\n    &lt;/div&gt;\n  );\n};\n\nexport default Dashboard;\n</code></pre> </li> </ol>"},{"location":"05-Build-Copilot/07/","title":"5.7 Test the UI Copilot Chat","text":"<p>You are now ready to test the end-to-end copilot chat feature. You must run the FastAPI server and the React SPA locally from VS Code debug sessions. In this next section, you will see how to do this locally for rapid prototyping and testing.</p>"},{"location":"05-Build-Copilot/07/#test-with-vs-code","title":"Test with VS Code","text":"<p>Visual Studio Code provides the ability to run applications locally, allowing for debugging and rapid testing.</p>"},{"location":"05-Build-Copilot/07/#start-the-api","title":"Start the API","text":"<p>The UI relies on the Woodgrove Bank API to be running. As you did to test the API via its Swagger UI, follow the steps below to start a debug session for the API in VS Code.</p> <ol> <li> <p>In Visual Studio Code Run and Debug panel, select the API Debugger option for your OS from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Wait for the API application to start completely, indicated by an <code>Application startup complete.</code> message in the terminal output.</p> <p></p> </li> </ol>"},{"location":"05-Build-Copilot/07/#start-the-portal","title":"Start the Portal","text":"<p>With the API running, you can start a second debug session in VS Code for the Portal project.</p> <ol> <li> <p>Return to the Run and Debug panel in Visual Studio Code and select the Portal Debugger option from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>This should launch the Woodgrove Bank Contract Management Portal in a new browser window (http://localhost:3000/).</p> </li> <li> <p>On the Dashboard page, enter the following message into the chat and send it:</p> <pre><code>What IT vendors are we working with?\n</code></pre> <p></p> </li> <li> <p>Next, ask the following question about vendor invoicing accuracy:</p> <pre><code>What vendor has had the most invoicing issues?\n</code></pre> <p></p> </li> </ol> <p>The response to the previous question may be accurate, but the copilot did not provide any insights about why this vendor was specified as having the most invoicing issues. You may also have received an error because the copilot lacks guidance on correctly selecting the proper function for augmenting the prompt. In either case, prompt engineering can help improve your copilot's response!</p> <p>In the next task, you will iterate on your copilot's prompt to refine it and improve the quality and groundedness of the responses it provides.</p> <p>Leave the API and Portal debug sessions running for the next task.</p>"},{"location":"05-Build-Copilot/08/","title":"5.8 Refine the Copilot Prompt","text":"<p>Prompt engineering is the art of crafting precise and effective prompts to guide AI models in generating desired outputs, making them respond in a way that aligns closely with the user's intentions and context. It involves understanding the model's capabilities and tailoring the input to produce precise, relevant, and valuable results.</p> <p>The initial prompt used by your copilot looks like this:</p> Starter prompt<pre><code>You are an intelligent copilot for Woodgrove Bank, designed to provide insights about the bank's vendors, active Statements of Work (SOWs), and the accuracy of invoices submitted in relation to those SOWs. You are helpful, friendly, and knowledgeable, but can only answer questions about Woodgrove's vendors and associated documents (invoices and SOWs).\n</code></pre> <p>This is a very basic prompt that does little to provide guidance to your copilot in how it interacts with users, leverages the tools available to it, or responds to questions. Now, you are going to iterate on that prompt to improve its interaction with users and the types of responses it is able to provide.</p> <p>Prompt engineering typically involves an iterative process of making minor changes to the prompt and then testing their impact. This process generally requires multiple cycles of changes and evaluations to create an effective prompt for your particular use case. Typicall, the iterative follows steps similar to the following:</p> <ol> <li>Ask questions</li> <li>Review responses</li> <li>Modify prompt</li> <li>Restart the API</li> <li>Repeat the above steps until you are satisfied with the quality and relevance of the responses you are receiving.</li> </ol>"},{"location":"05-Build-Copilot/08/#iterate-to-improve-your-prompt","title":"Iterate to improve your prompt","text":"<p>You will use the active VS Code debug sessions for both the API and the Portal you started in the last task to test a few changes to the copilot's prompt and evaluate the responses you are receiving.</p> <ol> <li> <p>Open the copilot prompt by navigating to the <code>src/api/app/prompts</code> folder in the VS Code Explorer and opening the <code>copilot.txt</code> file.</p> <p>Prompts are loaded by the API's lifespan manager</p> <p>In the Woodgrove Bank API project, prompts are stored in text files and loaded into memory by the <code>lifespan_manager</code> used by the FastAPI application.</p> </li> <li> <p>To get started, add the following language to the bottom of the prompt to provide some guidelines it should follow when responding to questions. This guidance contains details about the information that should be included in the response based on the type of information the user query is about:</p> Provide response guidelines<pre><code>Use the following guidelines when responding:\n- If asked about a specific vendor, provide information about the vendor, their SOWs, and invoices. Always include a description of the vendor, the type of services they provide, and their contact information.\n- If asked about a specific SOW, always provide information about the SOW, its milestones, and deliverables. Include the name of the vendor performing the work.\n- If asked about a specific invoice, always provide information about the invoice and its line items.\n</code></pre> </li> <li> <p>Your prompt should now resemble this:</p> Intermediate prompt<pre><code>You are an intelligent copilot for Woodgrove Bank, designed to provide insights about the bank's vendors, active Statements of Work (SOWs), and the accuracy of invoices submitted in relation to those SOWs. You are helpful, friendly, and knowledgeable, but can only answer questions about Woodgrove's vendors and associated documents (invoices and SOWs).\n\nUse the following guidelines when responding:\n- If asked about a specific vendor, provide information about the vendor, their SOWs, and invoices. Always include a description of the vendor, the type of services they provide, and their contact information.\n- If asked about a specific SOW, always provide information about the SOW, its milestones, and deliverables. Include the name of the vendor performing the work.\n- If asked about a specific invoice, always provide information about the invoice and its line items.\n</code></pre> </li> <li> <p>In VS Code, restart the API Dugger session using the floating debugger toolbar. Ensure the API Debugger is selected in the configurations dropdown list before restarting the debugger. Alternatively, you can stop and restart the API Debugger via the Run and Debug panel.</p> <p></p> </li> <li> <p>In the Woodgrove Bank Contract Management Portal, ask a few questions about specific vendors, SOWs, and invoices and evaluate the responses.</p> </li> <li> <p>Now, make another change to the copilot prompt by adding the following guidance on using validation results to assess a vendor's performance and billing accuracy.</p> Provide instructions about assessing performance and accuracy<pre><code>When asked about a vendor's performance or billing accuracy:\n1. Use validation results for SOWs and invoices to perform your analysis.\n2. Assess timeliness and quality of deliverables based on the validation results.\n3. Provide a summary of the vendor's performance and accuracy based on the validation results.\n4. Include only your assessment in your response, without any invoice and SOW data, unless specifically asked otherwise.\n</code></pre> </li> <li> <p>Your updated prompt should look like:</p> Final prompt<pre><code>You are an intelligent copilot for Woodgrove Bank, designed to provide insights about the bank's vendors, active Statements of Work (SOWs), and the accuracy of invoices submitted in relation to those SOWs. You are helpful, friendly, and knowledgeable, but can only answer questions about Woodgrove's vendors and associated documents (invoices and SOWs).\n\nUse the following guidelines when responding:\n- If asked about a specific vendor, provide information about the vendor, their SOWs, and invoices. Always include a description of the vendor, the type of services they provide, and their contact information.\n- If asked about a specific SOW, always provide information about the SOW, its milestones, and deliverables. Include the name of the vendor performing the work.\n- If asked about a specific invoice, always provide information about the invoice and its line items.\n\nWhen asked about a vendor's performance or billing accuracy:\n1. Use validation results for SOWs and invoices to perform your analysis.\n2. Assess timeliness and quality of deliverables based on the validation results.\n3. Provide a summary of the vendor's performance and accuracy based on the validation results.\n4. Include only your assessment in your response, without any invoice and SOW data, unless specifically asked otherwise.\n</code></pre> </li> <li> <p>Restart the API Dugger session again.</p> </li> <li> <p>Return to the Woodgrove Bank Contract Management Portal, and ask some questions about vendor performance, accuracy, and billing issues. Select + Chat to ensure you're in a new chat session, then ask your questions. For example, ask, \"What vendor has had the most invoicing issues?\"</p> <p></p> </li> <li> <p>Review the response, comparing it to the more direct response from your initial prompt. Ask a few more questions about the vendors, SOWs, and invoices to see how your copilot performs.</p> </li> <li> <p>Continue to iterate on the prompt until you are happy with the responses you are receiving.</p> </li> </ol> <p>Congratulations! You have successfully implemented an intelligent copilot in the Woodgrove Bank Contract Management application!</p>"},{"location":"06-Improve-RAG-Accuracy/","title":"Improve RAG Accuracy","text":"<p>As Generative AI (GenAI) becomes increasingly integral to modern enterprises, users' trust in these applications is paramount and heavily reliant on the accuracy of their responses. The productivity loss from incorrect answers and the consequent erosion of user trust are issues that cannot be overlooked. For many organizations, the decision to deploy GenAI apps hinges on their ability to elevate the accuracy of the app's responses to an acceptable level.</p> <p>In this section, you will use Semantic Ranking and GraphRAG to enhance the accuracy of responses from the Woodgrove Bank Contract Management copilot. Here's what you will accomplish:</p> <ul> <li> Review Semantic Ranking</li> <li> Use Semantic Ranking to rerank hybrid search results</li> <li> Review GraphRAG and the Apache AGE extension</li> <li> Implement the AGE extension to enable graph queries against your PostgreSQL database</li> </ul>"},{"location":"06-Improve-RAG-Accuracy/#the-accuracy-problem","title":"The Accuracy Problem","text":"<p>Despite significant advancements, accuracy remains a challenge for GenAI. Retrieval Augmented Generation (RAG) helps by grounding responses in factual data. Still, as datasets expand or when documents become too similar, vector search techniques can falter, leading to a loss of user trust and diminished productivity. Enhancing accuracy requires optimizing the information retrieval pipeline through various techniques. General methods include chunking, larger embeddings, and hybrid search, while advanced, dataset-specific approaches like semantic ranking and GraphRAG are essential.</p>"},{"location":"06-Improve-RAG-Accuracy/#enhance-genai-accuracy-with-advanced-techniques","title":"Enhance GenAI Accuracy with Advanced Techniques","text":"<p>Innovations like Semantic Ranking and GraphRAG are crucial to address the accuracy problem. These techniques enhance the RAG approach by improving how grounding data is gathered and integrated into AI model responses, thereby increasing precision and reliability. Optimizing information retrieval pipelines through advanced techniques ensures that GenAI applications deliver accurate, trustworthy, and insightful responses, thereby maintaining user trust and productivity.</p> <p>Select the tabs below to understand how Semantic Ranking and GraphRAG can improve RAG accuracy.</p> Semantic RankingGraphRAG <p>Semantic ranking is an advanced technique used in information retrieval systems to enhance the relevance of search results by understanding the context and meaning of queries and documents rather than relying solely on keyword matching. By leveraging natural language processing and machine learning algorithms, semantic ranking can analyze the relationships between words and concepts to provide more accurate and meaningful results. This approach allows search engines to better comprehend the intent behind user queries and deliver results that are contextually aligned with what users are looking for. The benefits of semantic ranking include improved accuracy in search results, enhanced user satisfaction, and more efficient retrieval of information, making it a powerful tool for modern search engines and recommendation systems.</p> <p>GraphRAG, developed by Microsoft Research, is an innovative approach to information retrieval and generation, combining the power of knowledge graphs with large language models (LLMs) to enhance the accuracy and relevance of generated content. GraphRAG allows LLMs to provide more contextually aware and insightful responses by integrating structured data from knowledge graphs into the retrieval process. This method enhances the model's ability to understand complex queries and synthesize information from various sources, leading to more accurate and informative outputs. GraphRAG's benefits include improved information retrieval, enhanced reasoning capabilities, and the ability to deliver precise answers even when dealing with intricate or multi-faceted questions.</p>"},{"location":"06-Improve-RAG-Accuracy/01/","title":"6.1 Understand Semantic Ranking","text":"<p>Semantic ranking improves accuracy of the vector search by re-ranking results using a semantic ranker model, which brings more relevant items to the top of the ranked list. In the context of Azure Database for PostgreSQL, semantic ranking can enhance the accuracy and relevance of search results within the database, significantly improving the information retrieval pipelines in Generative AI (GenAI) applications. Unlike vector search, which primarily measures the similarity between two vector embeddings, semantic ranking delves deeper by analyzing the semantic relevance between text strings at the actual text level. This approach ensures that search results are more contextually aligned with user queries, improving information retrieval and higher user satisfaction.</p>"},{"location":"06-Improve-RAG-Accuracy/01/#reranking-search-results-with-semantic-ranker-models","title":"Reranking Search Results with Semantic Ranker Models","text":"<p>Semantic ranker models compare two text strings: the search query and the text of one of the items being searched. The ranker produces a relevance score that indicates how well the text string matches the query, essentially determining if the text holds an answer to the query. Typically, the semantic ranker is a machine learning model, often a variant of the BERT language model fine-tuned for ranking tasks, but it can also be a large language model (LLM). These ranker models, also called cross-encoder models, take two text strings as input and output a relevance score, usually ranging from 0 to 1.</p>"},{"location":"06-Improve-RAG-Accuracy/01/#semantic-ranker-model-inference-from-postgresql","title":"Semantic Ranker Model Inference from PostgreSQL","text":"<p>The <code>azure_ai</code> extension enables the invocation of machine learning models deployed on Azure Machine Learning online endpoints directly from within PostgreSQL.</p> <p></p> <p>Using the <code>azure_ml.invoke()</code> method, inference with a semantic ranker model can be performed through SQL queries. This allows for the reranking of vector search results based on the semantic relevance of the text, rather than relying solely on keyword matching.</p>"},{"location":"06-Improve-RAG-Accuracy/02/","title":"6.2 Use Semantic Ranking in PostgreSQL","text":"<p>By leveraging the power of semantic ranking, you can improve the accuracy of data retrieval and help ensure the success of your Generative AI applications. In this task, you will create a user-defined function (UDF) in your database that utilizes model inferencing capabilities of the Azure AI extension. Specifically, the <code>azure_ml.invoke()</code> method of the extension will be called from within the UDF to seamlessly invoke a semantic ranking model directly within SQL queries.</p> Your Semantic Ranker model was deployed to Azure ML. <p>The deployment of this solution accelerator included executing a post-deployment script to download the BGE-reranker-v2-m3 semantic ranker model and deploy it as an Azure Machine Learning (AML) inference endpoint in your subscription. This model was selected because it is lightweight, easy to deploy, and provides fast inference.</p>"},{"location":"06-Improve-RAG-Accuracy/02/#create-a-reranking-function","title":"Create a reranking function","text":"<p>To simplify the integration of semantic ranking and invocation of the semantic ranker model from Azure ML, you will create a user-defined function (UDF). The function uses the <code>azure_ai</code> extension to directly make remote calls to the AML inference endpoint from a SQL query. Using the <code>azure_ml.invoke()</code> function within a UDF, you can make calls to the semantic ranker model.</p> You already configured the extension's connection to Azure ML <p>Recall that you already configured the Azure AI extension's connection to Azure ML in task 3.2 Configure the Azure AI extension using the following commands:</p> <pre><code>SELECT azure_ai.set_setting('azure_ml.scoring_endpoint','&lt;endpoint&gt;');\nSELECT azure_ai.set_setting('azure_ml.endpoint_key', '&lt;api-key&gt;');\n</code></pre> <p>The UDF will be defined within your SQL database, allowing you to seamlessly incorporate semantic ranking into your queries. You will use pgAdmin to create it.</p> <ol> <li> <p>Return to the open instance of pgAdmin on your local machine and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Run the following query to create the <code>semantic_reranking</code> function:</p> Create Semantic Reranking UDF<pre><code>CREATE OR REPLACE FUNCTION semantic_reranking(query TEXT, vector_search_results TEXT[])\nRETURNS TABLE (content TEXT, relevance jsonb) AS $$\nBEGIN\n    RETURN QUERY\n        WITH\n        json_pairs AS(\n        SELECT jsonb_build_object(\n                    'pairs', \n                    jsonb_agg(\n                        jsonb_build_array(query, content_)\n                    )\n                ) AS json_pairs_data\n                FROM (\n                    SELECT a.content as content_\n                    FROM unnest(vector_search_results) as a(content)\n                )\n        ), \n        relevance_scores AS(\n            SELECT jsonb_array_elements(invoke.invoke) as relevance_results\n            FROM azure_ml.invoke(\n                        (SELECT json_pairs_data FROM json_pairs),\n                        deployment_name=&gt;'bgev2m3-v1', timeout_ms =&gt; 120000)\n        ),\n        relevance_scores_rn AS (\n            SELECT *, ROW_NUMBER() OVER () AS idx\n            FROM relevance_scores\n        )\n        SELECT a.content,\n               r.relevance_results\n            FROM\n                unnest(vector_search_results) WITH ORDINALITY AS a(content, idx2)\n            JOIN\n                relevance_scores_rn AS r(relevance_results, idx)\n            ON\n                a.idx2 = r.idx;\n\nEND $$ LANGUAGE plpgsql;\n</code></pre> How does the `semantic_reranking function work? <p>The <code>semantic_reranking</code> function enhances search results by re-ranking them based on their semantic relevance to a given query. Here's a breakdown of how it works:</p> <p>Input Parameters</p> <ul> <li><code>query</code>: A text string representing the search query.</li> <li><code>vector_search_results</code>: An array of text strings representing the initial search results obtained from a vector search.</li> </ul> <p>Return Value</p> <ul> <li>The function returns a table with two columns: <code>content</code> (the original search result content) and <code>relevance</code> (a JSONB object representing the relevance score).</li> </ul> <p>Steps</p> <ul> <li>Json Pairs Construction: The function starts by constructing a JSON object that pairs the query with each initial search result.</li> <li>Relevance Scoring: It then calls the <code>azure_ml.invoke</code> function to send the JSON pairs to an Azure Machine Learning endpoint, which computes the relevance scores for each pair. The results are returned as a JSON array.</li> <li>Row Number Assignment: The relevance scores are assigned row numbers to maintain their order.</li> <li>Combining Results: Finally, the function combines the original search results with corresponding relevance scores using the row numbers, ensuring that each result is paired with the correct relevance score.</li> </ul> <p>This function's overall purpose is to improve the relevance of search results by leveraging a semantic model hosted on Azure ML. This ensures that the results returned are more contextually relevant to the user's query.</p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/02/#test-the-udf","title":"Test the UDF","text":"<p>To see how semantic ranking works in your queries, you can execute a test query from pgAdmin. The example below shows how to perform semantic reranking of vector search results for the query \"cost management and optimization\" over the <code>sow_chunks</code> table using the <code>semantic_reranking</code> UDF you created.</p> <ol> <li> <p>In the pgAdmin query window, run the following query and observe the results.</p> Semantic ranking query<pre><code>WITH vector_results AS (\n    SELECT content FROM sow_chunks c\n    ORDER BY embedding &lt;=&gt; azure_openai.create_embeddings('embeddings', 'cost management and optimization')::vector\n    LIMIT 10\n)\nSELECT content, relevance\nFROM semantic_reranking('cost management and optimization',  ARRAY(SELECT content from vector_results))\nORDER BY relevance DESC\nLIMIT 3;\n</code></pre> </li> </ol> <p>Next, you will update the <code>get_sow_chunks</code> function used by your copilot to use semantic ranking to improve the accuracy and quality of the copilot's responses.</p>"},{"location":"06-Improve-RAG-Accuracy/03/","title":"6.3 Update Copilot With Semantic Ranking","text":"<p>The next step is to update your API to use the semantic ranking capability. For this, you will update how it finds and retrieves SOW chunks, which are the blocks of content extracted from each SOW uploaded into the application.</p>"},{"location":"06-Improve-RAG-Accuracy/03/#review-the-function","title":"Review the function","text":"<p>Following the function calling pattern used by your LangChain agent to retrieve data from the database, you will use a Python function to execute semantic ranking queries from your copilot. Within the <code>src/api/app/functions/chat_functions.py</code> file, the <code>find_sow_chunks_with_semantic_ranking</code> function has been provided for executing queries using the <code>semantic_reranking</code> UDF you added to your database in the previous step. Open it now in Visual Studio Code and explore the code with the function. You can also expand the section below to see the code inline.</p> Find SOW Chunks with Semantic Ranking code src/api/app/functions/chat_functions.py<pre><code>async def find_sow_chunks_with_semantic_ranking(self, user_query: str, vendor_id: int = None, sow_id: int = None, max_results: int = 3):\n        \"\"\"\n        Retrieves content chunks similar to the user query for the specified SOW.\n        \"\"\"\n\n        # Get the embeddings for the user query\n        query_embeddings = await self.__create_query_embeddings(user_query)\n\n        # Create a vector search query\n        cte_query = f\"SELECT content FROM sow_chunks\"\n        cte_query += f\" WHERE sow_id = {sow_id}\" if sow_id is not None else f\" WHERE vendor_id = {vendor_id}\" if vendor_id is not None else \"\"\n        cte_query += f\" ORDER BY embedding &lt;=&gt; '{query_embeddings}'\"\n        cte_query += f\" LIMIT 10\"\n\n        # Create the semantic ranker query\n        query = f\"\"\"\n        WITH vector_results AS (\n            {cte_query}\n        )\n        SELECT content, relevance\n        FROM semantic_reranking('{user_query}',  ARRAY(SELECT content from vector_results))\n        ORDER BY relevance DESC\n        LIMIT {max_results};\n        \"\"\"\n\n        rows = await self.__execute_query(f'{query};')\n        return [dict(row) for row in rows]\n</code></pre> <ol> <li> <p>Generate embeddings (line 18): Azure OpenAI generates embeddings representing the user query.</p> </li> <li> <p>Create vector search query (lines 21-24): The UDF expects vector search results as input, so a vector search query is created. The query selects the <code>content</code> field from the <code>sow_chunks</code> table and orders them by semantic similarity.</p> </li> <li> <p>Create a semantic ranker query (lines 27-35): The results of the vector search query are required to call the <code>semantic_reranking</code> UDF.</p> <ul> <li>Create CTE (lines 28-30): A common table expression (CTE) executes the vector search query and extracts the content values and relevancy scores from that query.</li> <li>Execute the <code>semantic_reranking</code> UDF (lines 31-33): Using the results of the CTE, the results are reranked using the <code>semantic_reranking</code> UDF.</li> <li>Limit results (line 34): The number of results is limited to ensure the more relevant records are sent to the LLM.</li> </ul> </li> <li> <p>Return the results (lines 37-38): The query results are extracted and returned to the LLM.</p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/03/#implement-semantic-ranker","title":"Implement semantic ranker","text":"<p>To use the semantic ranking functionality instead of the vector search to retrieve SOW chunks, you must replace the function assigned to your LangChain agent's <code>tools</code> collection. You will replace the <code>find_sow_chunks</code> tool with the <code>find_sow_chunks_with_semantic_ranking</code> function.</p> <ol> <li> <p>In the VS Code Explorer, navigate to the <code>src/api/app/routers</code> folder and open the <code>completions.py</code> file.</p> </li> <li> <p>Within the <code>tools</code> array, locate the following line:</p> Python<pre><code>StructuredTool.from_function(coroutine=cf.find_sow_chunks),\n</code></pre> </li> <li> <p>Replace that line with the following:</p> Python<pre><code>StructuredTool.from_function(coroutine=cf.find_sow_chunks_with_semantic_ranking),\n</code></pre> </li> <li> <p>Your new <code>tools</code> array should look like this:</p> Python<pre><code># Define tools for the agent to retrieve data from the database\ntools = [\n    # Hybrid search functions\n    StructuredTool.from_function(coroutine=cf.find_invoice_line_items),\n    StructuredTool.from_function(coroutine=cf.find_invoice_validation_results),\n    StructuredTool.from_function(coroutine=cf.find_milestone_deliverables),\n    StructuredTool.from_function(coroutine=cf.find_sow_chunks_with_semantic_ranking),\n    StructuredTool.from_function(coroutine=cf.find_sow_validation_results),\n    # Get invoice data functions\n    StructuredTool.from_function(coroutine=cf.get_invoice_id),\n    StructuredTool.from_function(coroutine=cf.get_invoice_line_items),\n    StructuredTool.from_function(coroutine=cf.get_invoice_validation_results),\n    StructuredTool.from_function(coroutine=cf.get_invoices),\n    # Get SOW data functions\n    StructuredTool.from_function(coroutine=cf.get_sow_chunks),\n    StructuredTool.from_function(coroutine=cf.get_sow_id),\n    StructuredTool.from_function(coroutine=cf.get_sow_milestones),\n    StructuredTool.from_function(coroutine=cf.get_milestone_deliverables),\n    StructuredTool.from_function(coroutine=cf.get_sow_validation_results),\n    StructuredTool.from_function(coroutine=cf.get_sows),\n    # Get vendor data functions\n    StructuredTool.from_function(coroutine=cf.get_vendors)\n]\n</code></pre> </li> <li> <p>Save the <code>completions.py</code> file.</p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/03/#test-with-vs-code","title":"Test with VS Code","text":"<p>As you have done previously, you will test your updates using Visual Studio Code.</p>"},{"location":"06-Improve-RAG-Accuracy/03/#start-the-api","title":"Start the API","text":"<p>Follow the steps below to start a debug session for the API in VS Code.</p> <ol> <li> <p>In Visual Studio Code Run and Debug panel, select the API Debugger option for your OS from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Wait for the API application to start completely, indicated by an <code>Application startup complete.</code> message in the terminal output.</p> <p></p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/03/#start-the-portal","title":"Start the Portal","text":"<p>With the API running, you can start a second debug session in VS Code for the Portal project.</p> <ol> <li> <p>Return to the Run and Debug panel in Visual Studio Code and select the Portal Debugger option from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>This should launch the Woodgrove Bank Contract Management Portal in a new browser window (http://localhost:3000/).</p> </li> <li> <p>On the Dashboard page, enter the following message into the chat and send it:</p> <pre><code>Show me SOWs pertaining to cost management and optimization.\n</code></pre> </li> <li> <p>Observe the results provided using your UDF and semantic ranking.</p> </li> </ol> <p>Congratulations! You just learned how to leverage the semantic ranking capabilities in Azure Database for PostgreSQL!</p>"},{"location":"06-Improve-RAG-Accuracy/04/","title":"6.4 Review GraphRAG","text":"<p>GraphRAG (Graph Retrieval-Augmented Generation) is an innovative technique developed by Microsoft Research. It significantly enhances the accuracy and relevance of responses generated by Retrieval-Augmented Generation (RAG) systems. Graph refers to a data structure representing entities and their relationships. This structure is often visualized as nodes (vertices) and edges, where nodes represent entities and edges represent the relationships or connections between these entities.</p> <p>In the context of Azure Database for PostgreSQL, GraphRAG leverages the structure of knowledge graphs extracted from source data to provide better context and improve the quality of responses from language models. By integrating GraphRAG with PostgreSQL, you can enhance the information retrieval pipeline, making it more accurate and context-aware. This is particularly useful for applications where the accuracy and relevance of information are critical.</p> Want a more extensive example of GraphRAG with Azure Database for PostgreSQL? <p>This solution accelerator presents a simplified GraphRAG implementation to show you how you can improve the accuracy of RAG by leveraging a graph database. For a more comprehensive example that combines the results of semantic ranking and GraphRAG, please refer to the Introducting the GraphRAG Solution for Azure Database for PostgreSQL blob post and associated GraphRAG Solution Accelerator for Azure Database for PostgreSQL.</p>"},{"location":"06-Improve-RAG-Accuracy/04/#graphrag-with-age","title":"GraphRAG with AGE","text":"<p>Apache Graph Extension (AGE) is a PostgreSQL extension developed under the Apache Incubator project. It offers a significant advancement that provides graph processing capabilities within the PostgreSQL ecosystem, enabling users to store efficiently and query graph data. This new extension brings a powerful toolset for developers looking to leverage a graph database with the robust enterprise features of Azure Database for PostgreSQL.</p> <p></p> <p>With AGE, you can manage and analyze complex relationships within your data, uncovering insights that traditional relational databases and even semantic search might miss.</p>"},{"location":"06-Improve-RAG-Accuracy/04/#key-features","title":"Key Features","text":"<ul> <li>Graph and relational data integration: AGE allows seamless integration of graph data with existing relational data in PostgreSQL. This hybrid approach lets you benefit from both graph and relational models simultaneously.</li> <li>openCypher query language: AGE incorporates openCypher, a powerful and user-friendly query language designed explicitly for graph databases. This feature simplifies the process of writing and executing graph queries.</li> <li>High performance: AGE is optimized for performance, ensuring efficient storage and retrieval of graph data thanks to support for indexing graph properties using GIN indices.</li> <li>Scalability: Built on PostgreSQL's proven architecture, AGE inherits its scalability and reliability, allowing it to handle growing datasets and increasing workloads.</li> </ul>"},{"location":"06-Improve-RAG-Accuracy/04/#benefits","title":"Benefits","text":"<p>The integration of AGE in Azure Database for PostgreSQL brings numerous practical benefits to developers and businesses looking to leverage graph processing capabilities:</p> <ul> <li>Simplified data management: AGE's ability to integrate graph and relational data simplifies data management tasks, reducing the need for separate graph database solutions.</li> <li>Enhanced data analysis: With AGE, you can perform complex graph analyses directly within your PostgreSQL database, gaining deeper insights into relationships and patterns in your data.</li> <li>Cost efficiency: By utilizing AGE within Azure Database for PostgreSQL, you can consolidate your database infrastructure, lowering overall costs and reducing the complexity of your data architecture.</li> <li>Security and compliance: AGE leverages Azure's industry-leading security and compliance features to ensure your graph data is protected and meets regulatory requirements, providing peace of mind.</li> </ul>"},{"location":"06-Improve-RAG-Accuracy/05/","title":"6.5 Enable AGE","text":"<p>The Apache AGE (<code>age</code>) extension enhances PostgreSQL by allowing it to be used as a graph database, providing a comprehensive solution for analyzing interconnected data. With <code>age</code>, you can define and query complex data relationships using graph structures.</p> <p>To simplify the extraction of data from your PostgreSQL database into CSV files hosted in Azure Blob Storage, you will also use the Azure Storage extension (<code>azure_storage</code>). This extension allows you to connect directly to an Azure Storage account from your PostgreSQL database, copy data out of the database, and write it into files in blob storage. From your storage account, the CSV files will be used as the data source for your graph database.</p>"},{"location":"06-Improve-RAG-Accuracy/05/#allowlist-the-extensions","title":"Allowlist the extensions","text":"<p>Before using <code>age</code> and <code>azure_storage</code> extensions, add them to the PostgreSQL server's allowlist, configure them as shared preloaded libraries, and install them in your database.</p> <p>Select the tab for the method you want to use for allowlisting the extensions and follow the instructions provided.</p> Azure CLIAzure portal <ol> <li> <p>Open a new integrated terminal window in VS Code and execute the following Azure CLI command at the prompt.</p> <p>Ensure you replace the tokens in the command below with the appropriate values from your Azure environment.</p> <ul> <li>[YOUR_RESOURCE_GROUP]: The name of the resource group hosting your Azure Database for PostgreSQL flexible server.</li> <li>[YOUR_POSTGRESQL_SERVER]: The name of your Azure Database for PostgreSQL server.</li> <li>[YOUR_SUBSCRIPTION_ID]: Your Azure subscription ID.</li> </ul> Bash<pre><code>az postgres flexible-server parameter set --resource-group [YOUR_RESOURCE_GROUP] --server-name [YOUR_POSTGRESQL_SERVER] --subscription [YOUR_SUBSCRIPTION_ID] --name azure.extensions --value age,azure_storage\n</code></pre> </li> </ol> <ol> <li> <p>Navigate to your Azure Database for PostgreSQL flexible server instance in the Azure portal.</p> </li> <li> <p>From the left-hand resource menu:</p> <ol> <li>Expand the Settings section and select Server parameters.</li> <li>Enter \"azure.extensions\" into the search filter.</li> <li>Expand the VALUE dropdown list.</li> <li>Select the AGE and AZURE_STORAGE extensions by checking the box for each in the VALUE dropdown list.</li> </ol> <p></p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/05/#load-extensions-on-server-start","title":"Load extensions on server start","text":"<p>Some Postgres libraries need to perform certain operations that can only take place at postmaster start, such as allocating shared memory, reserving lightweight locks, or starting background workers. <code>AGE</code> relies on shared memory for its operations, so it must be loaded at server start. The <code>shared_preload_libraries</code> parameter in PostgreSQL is used to specify libraries that should be loaded at server startup, enabling additional functionalities or extensions before any connections are made.</p> <p>Select the tab for the method you want to use to update the <code>shared_preload_libraries</code> parameter and follow the instructions provided.</p> Azure CLIAzure portal <ol> <li> <p>In the VS Code integrated terminal window, execute the following Azure CLI command at the prompt.</p> <p>Ensure you replace the tokens in the command below with the appropriate values from your Azure environment.</p> <ul> <li>[YOUR_RESOURCE_GROUP]: The name of the resource group hosting your Azure Database for PostgreSQL flexible server.</li> <li>[YOUR_POSTGRESQL_SERVER]: The name of your Azure Database for PostgreSQL server.</li> <li>[YOUR_SUBSCRIPTION_ID]: Your Azure subscription ID.</li> </ul> Bash<pre><code>az postgres flexible-server parameter set --resource-group [YOUR_RESOURCE_GROUP] --server-name [YOUR_POSTGRESQL_SERVER] --subscription [YOUR_SUBSCRIPTION_ID] --name shared_preload_libraries --value age,azure_storage,pg_cron,pg_stat_statements\n</code></pre> <p><code>pg_cron</code> and <code>pg_stat_statements</code> are set by default, so they are included in the above command to avoid removing them from the <code>shared_preload_libraries</code> parameter.</p> </li> <li> <p>The above command sets the parameter, but your PostgreSQL flexible server requires a restart for the setting to take effect. Run the following command to restart your server:</p> <p>Ensure you replace the tokens in the command below with the appropriate values from your Azure environment.</p> <ul> <li>[YOUR_RESOURCE_GROUP]: The name of the resource group hosting your Azure Database for PostgreSQL flexible server.</li> <li>[YOUR_POSTGRESQL_SERVER]: The name of your Azure Database for PostgreSQL server.</li> <li>[YOUR_SUBSCRIPTION_ID]: Your Azure subscription ID.</li> </ul> Bash<pre><code>az postgres flexible-server restart --resource-group [YOUR_RESOURCE_GROUP] --name [YOUR_POSTGRESQL_SERVER] --subscription [YOUR_SUBSCRIPTION_ID]\n</code></pre> </li> </ol> <ol> <li> <p>On the Server parameters page of your Azure Database for PostgreSQL flexible server instance in the Azure portal:</p> <ol> <li>Enter \"shared_preload\" into the search filter.</li> <li>Expand the VALUE dropdown list.</li> <li>Select the AGE and AZURE_STORAGE extensions by checking the box for each in the VALUE dropdown list.</li> <li>Select Save on the toolbar.</li> </ol> <p></p> </li> <li> <p>Select Save will trigger a restart of the PostgreSQL server and will take a few seconds to complete.</p> </li> <li> <p>In the Save server parameter dialog that appears, select Save and Restart.</p> <p></p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/05/#install-extensions","title":"Install extensions","text":"<p>With the <code>AGE</code> and <code>AZURE_STORAGE</code> extensions added to the allowlist and loaded on your PostgreSQL server, you can install them in your database using the CREATE EXTENSION command.</p> <p>At this time, the AGE extension is in preview and will only be available for newly created Azure Database for PostgreSQL Flexible Server instances running at least PG13 up to PG16.</p> <p>You will use pgAdmin to install the extension by executing a SQL command against your database.</p> <ol> <li> <p>On your local machine, return to the open instance of pgAdmin (or open it if you closed it after the setup tasks) and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Install the <code>age</code> and <code>azure_storage</code> extensions by running the following <code>CREATE EXTENSION</code> commands in the pgAdmin query window:</p> <pre><code>CREATE EXTENSION IF NOT EXISTS age;\nCREATE EXTENSION IF NOT EXISTS azure_storage;\n</code></pre> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/06/","title":"6.6 Export Graph Data","text":"<p>You must export data from your PostgreSQL database to populate your graph database. Using the Azure Storage (<code>azure_storage</code>) extension for Azure Database for PostgreSQL provides a streamlined method for copying data out of your PostgreSQL database into CSV files in Azure Blob Storage. In the context of <code>AGE</code> and loading data into a graph database, the <code>azure_storage</code> extension facilitates the extraction of relational data from your PostgreSQL database, enabling efficient transfer to Blob Storage. This process ensures that the data needed for constructing and querying your graph database is readily available and can be seamlessly integrated into your data workflows.</p>"},{"location":"06-Improve-RAG-Accuracy/06/#connect-your-database-to-azure-storage","title":"Connect your database to Azure Storage","text":"<p>You will use pgAdmin to configure the <code>azure_storage</code> extension's connection to your storage account by executing SQL commands against your database.</p> <p>Ensure you replace the token in the commands below with the appropriate values from your Azure environment.</p> <p>Each SQL statement you will execute below contains a <code>[YOUR_STORAGE_ACCOUNT_NAME]</code> token. Before running any of the queries, you must replace this token with the name of your Storage account resource, which you can copy from the Azure portal.</p> <ol> <li> <p>On your local machine, return to the open instance of pgAdmin (or open it if you have closed it) and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Run the following command to use the <code>azure_storage.account_add()</code> function to define a connection between your storage account and your PostgreSQL database:</p> SQL<pre><code>SELECT azure_storage.account_add(azure_storage.account_options_managed_identity('[YOUR_STORAGE_ACCOUNT_NAME]', 'blob'));\n</code></pre> About the `account_add() function <p>In the above <code>account_add()</code> function call, the <code>azure_storage.account_options_managed_identity()</code> function is passed as the <code>account_config</code> parameter. This function returns a JSONB object containing the configuration required to authenticate against the storage account using your PostgreSQL database's managed identity. It expects the name of the target storage account and the storage type, which currently only supports <code>blob</code>. Your PostgreSQL database was configured with a system-assigned identity during resource provisioning. That identity was granted the <code>Storage Blob Data Contributor</code> role on your storage account, which allows your database to read and write files from blob storage.</p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/06/#export-data-to-blob-storage","title":"Export data to blob storage","text":"<p>As part of the data export process, you will use queries to reshape the source data into the format required to efficiently define nodes and edges in your graph database. Traditional relational databases organize data in tables, while graph databases use nodes and edges to represent entities and their relationships. Converting tabular data into nodes and edges aligns with the graph structure, making relationship analysis more efficient. This transformation enables natural modeling of real-world entities, optimizes query performance, and allows for complex relationship analysis, such as evaluating the connections between vendors, SOWs, and associated invoices. By reshaping your data, you can fully leverage the strengths of <code>AGE</code> and Azure Database for PostgreSQL for deeper insights and sophisticated analyses.</p> <p>You will define two nodes and one edge in your graph database. The nodes will contain data vendor, and SOW data. The edge will define the relationship between these.</p> <p>You will use pgAdmin to execute data export queries leveraging the <code>azure_storage</code> extension.</p> <ol> <li> <p>Return to the open Query Tool in pgAdmin.</p> </li> <li> <p>Run the following query using the <code>azure_storage.blob_put()</code> function to write all data from the <code>vendors</code> table into a CSV file named <code>vendors.csv</code> into your storage account's <code>graph</code> container. This data will define the <code>vendor</code> node in your graph database.</p> SQL<pre><code>-- Extract data for the vendors node\nSELECT azure_storage.blob_put(\n    '[YOUR_STORAGE_ACCOUNT_NAME]',\n    'graph',\n    'vendors.csv',\n    vendors,\n    'csv',\n    'none',\n    azure_storage.options_csv_put(header:=true)\n)\nFROM (\n    SELECT * FROM vendors\n) AS vendors;\n</code></pre> </li> <li> <p>Execute this query to extract <code>sow</code> node data from the <code>sows</code> table and write it into a CSV file named <code>sows.csv</code> into your storage account's <code>graph</code> container. The query excludes a few columns from the <code>sows</code> table, including the <code>embedding</code> column, as they are unnecessary in the graph database and can cause errors.</p> SQL<pre><code>-- Extract data for the SOWs node\nSELECT azure_storage.blob_put(\n    '[YOUR_STORAGE_ACCOUNT_NAME]',\n    'graph',\n    'sows.csv',\n    sows,\n    'csv',\n    'none',\n    azure_storage.options_csv_put(header:=true)\n)\nFROM (\n    SELECT id, number, vendor_id, start_date, end_date, budget FROM sows\n) AS sows;\n</code></pre> </li> <li> <p>Finally, run the following query to extract <code>has_invoices</code> edge data from the <code>invoices</code> table and write it into a CSV file named <code>has_invoices.csv</code> into the <code>graph</code> container in your storage account:</p> SQL<pre><code>-- Create the has_invoices edge\nSELECT azure_storage.blob_put(\n    '[YOUR_STORAGE_ACCOUNT_NAME]',\n    'graph',\n    'has_invoices.csv',\n    invoices,\n    'csv',\n    'none',\n    azure_storage.options_csv_put(header:=true)\n)\nFROM (\n    SELECT id, vendor_id as start_id, 'vendor' AS start_vertex_type, sow_id AS end_id, 'sow' AS end_vertex_type, number, amount, invoice_date, payment_status FROM invoices\n) AS invoices;\n</code></pre> <p>Edge definition details</p> <p>When using <code>AGE</code>, edges must contain details about the relationships between nodes. These are defined in the above query by specifying the <code>start_id</code>, <code>start_vertex_type</code>, <code>end_id</code>, and <code>end_vertex_type</code> columns. The '_id' columns are mapped to the <code>vendor_id</code> and <code>sow_id</code>, respectively, and the start and end vertex types are strings specifying the node type associated with the ID.</p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/06/#verify-csv-files","title":"Verify CSV files","text":"<p>You can use the <code>azure_storage</code> extension to verify the CSV files were successfully written into the <code>graph</code> container in your storage account.</p> <ol> <li> <p>Execute the following query in the Query Tool in pgAdmin. Ensure you replace the <code>[YOUR_STORAGE_ACCOUNT_NAME]</code> token with the name of your storage account.</p> SQL<pre><code>-- Verify the CSV files were written into blob storage\nSELECT azure_storage.blob_list('[YOUR_STORAGE_ACCOUNT_NAME]', 'graph');\n</code></pre> </li> <li> <p>You should see a list of blobs in the Data output panel in pgAdmin that includes the three CSV files you exported above.</p> </li> </ol> <p>Congratulations! You have successfully exported data to create your graph database!</p>"},{"location":"06-Improve-RAG-Accuracy/07/","title":"6.7 Create Graph Database","text":"<p>Hosting graph databases in Azure Database for PostgreSQL using the Apache AGE extension offers a powerful way to analyze relationships within your data. AGE combines graph and relational data seamlessly, leveraging the openCypher query language for efficient graph processing. This integration brings PostgreSQL's scalability, performance, and security to the table while enabling advanced data analysis and management. When incorporated into a copilot, this setup empowers you to evaluate vendor performance of SOW deliverables through invoice validation, ensuring your data-driven decisions are robust and insightful.</p>"},{"location":"06-Improve-RAG-Accuracy/07/#create-graph-database-with-agefreighter","title":"Create Graph Database with AGEFreighter","text":"<p>AGEFreighter is a Python library designed to simplify the process of creating and loading graph databases in Azure Database for PostgreSQL, allowing data to be ingested from various sources, including file formats (CSV, AVRO, and Parquet), Azure Storage, Azure Cosmos DB, and Azure Database for PostgreSQL.</p>"},{"location":"06-Improve-RAG-Accuracy/07/#review-code","title":"Review code","text":"<p>The solution accelerator includes the <code>graph_loader.py</code> file in the <code>src/api/app</code> folder, which allows you to quickly run a Python script to create a graph database and populate it with data from CSV files.</p> <p>The graph loader is implemented in the <code>src/api/app/graph_loader.py</code> file. Open it now in Visual Studio Code and explore the code in sections. You can also expand the section below to see the code inline and review explanations for the code.</p> Graph Loader code src/api/app/graph_loader.py<pre><code>import os\nfrom agefreighter import Factory\nfrom dotenv import load_dotenv\nfrom azure.identity import DefaultAzureCredential\nfrom azure.storage.blob.aio import BlobServiceClient\n\nasync def main():\n    \"\"\"Load data into Azure Database for PostgreSQL Graph Database.\"\"\"\n    # Load environment variables from the .env file\n    load_dotenv()\n    print(\"Loading environment variables...\")\n\n    # Get environment variables\n    server = os.getenv(\"POSTGRESQL_SERVER_NAME\")\n    database = 'contracts'\n    username = os.getenv(\"ENTRA_ID_USERNAME\")\n    account_name = os.getenv(\"STORAGE_ACCOUNT_NAME\")\n\n    # Create an AGEFreigher factory instance to load data from multiple CSV files.\n    print(\"Creating AGEFreighter factory instance...\")\n    factory = Factory.create_instance('MultiCSVFreighter')\n\n    # Connect to the PostgreSQL database.\n    print(\"Connecting to the PostgreSQL database...\")\n    await factory.connect(\n        dsn=get_connection_string(server, database, username),\n        max_connections=64\n    )\n\n    local_data_dir = 'graph_data/'\n\n    # Download CSV data files from Azure Blob Storage\n    print(\"Downloading CSV files from Azure Blob Storage...\")\n    await download_csvs(account_name, local_data_dir)\n\n    # Load data into the graph database\n    print(\"Loading data into the graph database...\")\n    await factory.load(\n        graph_name='vendor_graph',\n        vertex_csv_paths = [\n            f'{local_data_dir}vendors.csv',\n            f'{local_data_dir}sows.csv'\n        ],\n        vertex_labels = ['vendor', 'sow'],\n        edge_csv_paths = [f'{local_data_dir}has_invoices.csv'],\n        edge_types = [\"has_invoices\"],\n        use_copy=True,\n        drop_graph=True,\n        create_graph=True,\n        progress=True\n    )\n\n    print(\"Graph data loaded successfully!\")\n\ndef get_connection_string(server_name: str, database_name: str, username: str):\n    \"\"\"Get the connection string for the PostgreSQL database.\"\"\"\n\n    # Get a token for the Azure Database for PostgreSQL server\n    credential = DefaultAzureCredential()\n    token = credential.get_token(\"https://ossrdbms-aad.database.windows.net\")\n    port = 5432\n\n    conn_str = \"host={} port={} dbname={} user={} password={}\".format(\n        server_name, port, database_name, username, token.token\n    )\n    return conn_str\n\nasync def download_csvs(account_name:str, local_data_directory: str):\n    \"\"\"Download CSV files from Azure Blob Storage.\"\"\"\n\n    # Create connection to the blob storage account\n    account_blob_endpoint = f\"https://{account_name}.blob.core.windows.net/\"\n    # Connect to the blob service client using Entra ID authentication\n    client = BlobServiceClient(account_url=account_blob_endpoint, credential=DefaultAzureCredential())\n\n    # List the blobs in the graph container with a CSV extension\n    async with client:\n        async for blob in client.get_container_client('graph').list_blobs():\n            if blob.name.endswith('.csv'):\n                # Download the CSV file to a local directory\n                await download_csv(client, blob.name, local_data_directory)\n\nasync def download_csv(client: BlobServiceClient, blob_path: str, local_data_dir: str):\n    \"\"\"Download a CSV file from Azure Blob Storage.\"\"\"\n    # Get the blob\n    blob_client = client.get_blob_client(container='graph', blob=blob_path)\n\n    async with blob_client:\n        # Download the CSV file\n        if await blob_client.exists():\n            # create a local directory if it does not exist\n            if not os.path.exists(local_data_dir):\n                os.makedirs(local_data_dir)\n\n            with open(f'{local_data_dir}{blob_path.split('/')[-1]}', 'wb') as file:\n                stream = await blob_client.download_blob()\n                result = await stream.readall()\n                # Save the CSV file to a local directory\n                file.write(result)\n\nif __name__ == \"__main__\":\n    import asyncio\n    import sys\n\n    if sys.platform == \"win32\":\n        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n\n    asyncio.run(main())\n</code></pre> <ol> <li> <p>Import libraries (lines 1-5): Required classes and functions are imported from various libraries.</p> </li> <li> <p>Define <code>main</code> function (line 7): The <code>main</code> function is the entry point of the graph loader. This function serves as the orchestrator for executing the code within the file.</p> </li> <li> <p>Load environment variables (lines 10-17): The <code>load_dotenv()</code> method from the <code>dotenv</code> Python library allows variables from the <code>.env</code> file within the API project to be loaded as environment variables in the project. Note the names of the variables here, as you will be adding those to your <code>.env</code> file in the next step.</p> </li> <li> <p>Create an AGEFreighter factory (line 21): The entry point for the <code>agefreighter</code> package in the <code>factory</code> class. This method creates an instance of the library using the type specified. You are loading your graph using multiple CSV files, so the <code>MultiCSVFreighter</code> class type is indicated.</p> </li> <li> <p>Connect to PostgreSQL (lines 25-28): The <code>connect</code> method of the <code>factory</code> opens a connection to your Azure Database for PostgreSQL flexible server.</p> <ol> <li>The <code>get_connection_string()</code> function uses values from your environment variables to define the connection string the <code>factory</code> will use to connect to your database.</li> <li>The <code>get_connection_string()</code> function is defined on lines 55-66.</li> </ol> </li> <li> <p>Download CSV files from blob storage (line 34): The CSV files you created in the previous task are downloaded from blob storage and written into a local folder, where the graph loader can easily access them.</p> <ol> <li>The <code>download_csvs()</code> function is defined on lines 68-81. This function creates a <code>BlobServiceClient</code> instance, which is used to retrieve the blobs in your storage account's <code>graph</code> container.</li> <li>For each blob with the extension of <code>.csv</code>, the <code>download_csv</code> function defined on lines 83-99 is used to retrieve the blob's contents and write them into a local file.</li> </ol> </li> <li> <p>Create and load the graph database (lines 38-51): The <code>load</code> method of the <code>factory</code> does the following:</p> <ol> <li>Creates a graph named <code>vendor_graph</code>.</li> <li>Defines vertex (node) data and labels and inserts the nodes into the graph.</li> <li>Specifies edges using labels and inserts them to establish the relationships between the nodes.</li> </ol> </li> <li> <p>Define the main guard (lines 101-108): The main guard defines how the <code>graph_loader</code> is executed when called directly. This code block lets you run the script from a command line or VS Code debugging session.</p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/07/#update-env-file","title":"Update <code>.env</code> file","text":"<p>The <code>graph_loader.py</code> file references environment variables to retrieve information about your Azure Database for PostgreSQL flexible server instance, your Entra ID username, and the storage account from which to pull CSV files. Before executing the graph loader script, you must update your project's <code>.env</code> file with these values. The <code>.env</code> file can be found in the <code>src\\api\\app</code> folder of the repo.</p> <ol> <li> <p>In VS Code, navigate to the <code>src\\api\\app</code> folder in the Explorer panel.</p> </li> <li> <p>Open the <code>.env</code> file and add the following lines:</p> <pre><code>ENTRA_ID_USERNAME=\"{YOUR_ENTRA_ID_USERNAME}\"\nPOSTGRESQL_SERVER_NAME=\"{YOUR_POSTGRESQL_SERVER_NAME}\"\nSTORAGE_ACCOUNT_NAME=\"{YOUR_STORAGE_ACCOUNT_NAME}\"\n</code></pre> Follow these steps to retrieve the necessary values <ol> <li> <p>Replace the <code>{YOUR_ENTRA_ID_USERNAME}</code> token in the <code>ENTRA_ID_USERNAME</code> variable's value with your Microsoft Entra ID, which should be the email address of the account you are using for this solution accelerator.</p> </li> <li> <p>Replace the <code>{YOUR_POSTGRESQL_SERVER_NAME}</code> token with the name of your PostgreSQL server. To get your server name:</p> </li> <li> <p>Navigate to your Azure Database for PostgreSQL flexible server resource in the Azure portal.</p> </li> <li> <p>In the Essentials panel of the PostgreSQL flexible server's Overview page, copy the Server name value and paste it into your <code>.env</code> file as the <code>POSTGRESQL_SERVER_NAME</code> value.</p> <p></p> </li> <li> <p>Replace the <code>{YOUR_STORAGE_ACCOUNT_NAME}</code> token with the name of your storage account. To retrieve your storage account name:</p> </li> <li> <p>In the Azure portal, navigate to the Storage account resource in your resource group.</p> </li> <li> <p>On the Storage account page, copy the storage account name and paste it into your <code>.env</code> file as the <code>STORAGE_ACCOUNT_NAME</code> value.</p> <p></p> </li> </ol> </li> <li> <p>Save the <code>.env</code> file.</p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/07/#load-the-graph-database","title":"Load the graph database","text":"<p>You will use a VS Code debugging session to locally execute the <code>graph_loader.py</code> script. Follow the steps below to start a Graph Loader debug session in VS Code.</p> <ol> <li> <p>In Visual Studio Code Run and Debug panel, select the Graph Loader option from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Wait for the graph loader to finish running, indicated by the <code>Graph data loaded successfully!</code> message in the terminal output.</p> <p></p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/07/#verify-data-load","title":"Verify data load","text":"<p>You will execute openCypher queries using pgAdmin to verify the data load and explore relationships in your graph database.</p> <ol> <li> <p>Return to pgAdmin and ensure it is connected to your PostgreSQL database.</p> </li> <li> <p>In the pgAdmin Object Explorer, expand databases under your PostgreSQL server.</p> </li> <li> <p>Right-click the contracts database and select Query Tool from the context menu.</p> </li> <li> <p>Before you can run cypher queries, you must set the <code>ag_catalog</code> schema in your path:</p> SQL<pre><code>SET search_path = ag_catalog, \"$user\", public;\n</code></pre> </li> <li> <p>Now, run the following cypher query to view vendors with open invoices, the details of those invoices, and verify your graph database was loaded correctly:</p> SQL<pre><code>-- View vendors and SOWs, along with invoice details from edge properties\nSELECT * FROM ag_catalog.cypher('vendor_graph', $$\nMATCH (v:vendor)-[rel:has_invoices]-&gt;(s:sow)\nRETURN v.id AS vendor_id, v.name AS vendor_name, s.id AS sow_id, s.number AS sow_number, rel.payment_status AS payment_status, rel.amount AS invoice_amount\n$$) as graph_query(vendor_id BIGINT, vendor_name TEXT, sow_id BIGINT, sow_number TEXT, payment_status TEXT, invoice_amount FLOAT);\n</code></pre> </li> </ol> <p>Congratulations! You have successfully loaded your graph database with data from PostgreSQL.</p>"},{"location":"06-Improve-RAG-Accuracy/08/","title":"6.8 Update Copilot With GraphRAG","text":"<p>The next step is to update your API to use GraphRAG for data retrieval. You will update how your copilot finds and retrieves unpaid invoices for this.</p>"},{"location":"06-Improve-RAG-Accuracy/08/#review-the-function","title":"Review the function","text":"<p>Following the function calling pattern used by your LangChain agent to retrieve data from the database, you will use a Python function to execute openCypher queries against your graph database from your copilot. Within the <code>src/api/app/functions/chat_functions.py</code> file, the <code>get_unpaid_invoices_for_vendor</code> function has been provided for executing cypher queries. Open it now in Visual Studio Code and explore the code with the function. You can also expand the section below to see the code inline.</p> GraphRAG code src/api/app/functions/chat_functions.py<pre><code>async def get_unpaid_invoices_for_vendor(self, vendor_id: int):\n    \"\"\"\n    Retrieves a list of unpaid invoices for a specific vendor using a graph query.\n    \"\"\"\n    # Define the graph query\n    graph_query = f\"\"\"SELECT * FROM ag_catalog.cypher('vendor_graph', $$\n    MATCH (v:vendor {{id: '{vendor_id}'}})-[rel:has_invoices]-&gt;(s:sow)\n    WHERE rel.payment_status &lt;&gt; 'Paid'\n    RETURN v.id AS vendor_id, v.name AS vendor_name, s.id AS sow_id, s.number AS sow_number, rel.id AS invoice_id, rel.number AS invoice_number, rel.payment_status AS payment_status\n    $$) as (vendor_id BIGINT, vendor_name TEXT, sow_id BIGINT, sow_number TEXT, invoice_id BIGINT, invoice_number TEXT, payment_status TEXT);\n    \"\"\"\n    rows = await self.__execute_graph_query(graph_query)\n    return [dict(row) for row in rows]\n</code></pre> <ol> <li> <p>Define grapy query (line 6): Creates the cypher query that will be used to look up unpaid invoices for the specified <code>vendor_id</code></p> </li> <li> <p>Execute cypher query (lines 12): The cyper query is sent to the database for execution, using the <code>__execute_graph_query()</code> function.</p> </li> <li> <p>The <code>__execute_graph_query()</code> function, starting on line 25 on the <code>chat_functions.py</code> file, runs the query against the <code>ag_catalog</code> schema, which contains the graph database. To enable this, it also includes a <code>SET</code> query prior to running the graph query to add <code>ag_catalog</code> to the <code>search_path</code> in the connection.</p> </li> <li> <p>Return the results (lines 13): The query results are extracted and returned to the LLM.</p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/08/#implement-graphrag","title":"Implement GraphRAG","text":"<p>To implement GraphRAG functionality in your copilot, you must include the <code>get_unpaid_invoices_for_vendor</code> function in your LangChain agent's <code>tools</code> collection. You will add this function to the list of available tools to your agent.</p> <ol> <li> <p>In the VS Code Explorer, navigate to the <code>src/api/app/routers</code> folder and open the <code>completions.py</code> file.</p> </li> <li> <p>Within the <code>tools</code> array, locate the following line (line 75):</p> Python<pre><code>StructuredTool.from_function(coroutine=cf.get_invoices),\n</code></pre> </li> <li> <p>Insert the following code on the line just below that:</p> Python<pre><code>StructuredTool.from_function(coroutine=cf.get_unpaid_invoices_for_vendor),\n</code></pre> </li> <li> <p>Your new <code>tools</code> array should look like this:</p> Python<pre><code># Define tools for the agent to retrieve data from the database\ntools = [\n    # Hybrid search functions\n    StructuredTool.from_function(coroutine=cf.find_invoice_line_items),\n    StructuredTool.from_function(coroutine=cf.find_invoice_validation_results),\n    StructuredTool.from_function(coroutine=cf.find_milestone_deliverables),\n    StructuredTool.from_function(coroutine=cf.find_sow_chunks_with_semantic_ranking),\n    StructuredTool.from_function(coroutine=cf.find_sow_validation_results),\n    # Get invoice data functions\n    StructuredTool.from_function(coroutine=cf.get_invoice_id),\n    StructuredTool.from_function(coroutine=cf.get_invoice_line_items),\n    StructuredTool.from_function(coroutine=cf.get_invoice_validation_results),\n    StructuredTool.from_function(coroutine=cf.get_invoices),\n    StructuredTool.from_function(coroutine=cf.get_unpaid_invoices_for_vendor),\n    # Get SOW data functions\n    StructuredTool.from_function(coroutine=cf.get_sow_chunks),\n    StructuredTool.from_function(coroutine=cf.get_sow_id),\n    StructuredTool.from_function(coroutine=cf.get_sow_milestones),\n    StructuredTool.from_function(coroutine=cf.get_milestone_deliverables),\n    StructuredTool.from_function(coroutine=cf.get_sow_validation_results),\n    StructuredTool.from_function(coroutine=cf.get_sows),\n    # Get vendor data functions\n    StructuredTool.from_function(coroutine=cf.get_vendors)\n]\n</code></pre> </li> <li> <p>Save the <code>completions.py</code> file.</p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/08/#test-with-vs-code","title":"Test with VS Code","text":"<p>As you have done previously, you will test your updates using Visual Studio Code.</p>"},{"location":"06-Improve-RAG-Accuracy/08/#start-the-api","title":"Start the API","text":"<p>Follow the steps below to start a debug session for the API in VS Code.</p> <ol> <li> <p>In Visual Studio Code Run and Debug panel, select the API Debugger option for your OS from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>Wait for the API application to start completely, indicated by an <code>Application startup complete.</code> message in the terminal output.</p> <p></p> </li> </ol>"},{"location":"06-Improve-RAG-Accuracy/08/#start-the-portal","title":"Start the Portal","text":"<p>With the API running, you can start a second debug session in VS Code for the Portal project.</p> <ol> <li> <p>Return to the Run and Debug panel in Visual Studio Code and select the Portal Debugger option from the debug configurations dropdown list.</p> <p></p> </li> <li> <p>Select the Start Debugging button (or press F5 on your keyboard).</p> <p></p> </li> <li> <p>This should launch the Woodgrove Bank Contract Management Portal in a new browser window (http://localhost:3000/).</p> </li> <li> <p>In the copilot chat on the Dashboard page, enter the following message and send it:</p> <pre><code>Tell me about the accuracy of unpaid invoices from Adatum.\n</code></pre> </li> <li> <p>Observe the results provided using GraphRAG.</p> <p>GraphRAG improves accuracy</p> <p>Add a breakpoint in the <code>get_unpaid_invoices_for_vendor</code> function in the <code>chat_functions.py</code> file. The breakpoint will allow you to see the graph query executing and enable you to step through the remaining function calls to observe that the invoice validation results are only retrieved for the unpaid invoices. This precision reduces the data returned from the database and allows the RAG pattern to only receive the data it needs to generate a response.</p> </li> </ol> <p>Congratulations! You just learned how to leverage the GraphRAG capabilities of Azure Database for PostgreSQL and AGE!</p>"},{"location":"07-Redeploy/","title":"Redeploy App to Azure","text":"<p>The workshop began with a pre-provisioned version of the Woodgrove Bank Contract Management application on Azure Container Apps. Now that you have modified elements of the app and tested them out locally, you might want to redeploy the application.</p> <p>Because you used <code>azd</code> for provisioning and deployment, this is as simple as calling <code>azd up</code> (to push all changes in both infrastructure and application) or running <code>azd deploy</code> if you want only to rebuild and deploy the application changes you made in this project.</p> <p>Understand the difference between <code>azd up</code> and <code>azd deploy</code></p> <p>The <code>azd up</code> and <code>azd deploy</code> commands are both part of the Azure Developer CLI, but they serve different purposes:</p> <ul> <li> <p><code>azd up</code> is used to package, provision, and deploy your application to Azure. It sets up the entire environment, including infrastructure and application code, from scratch. It's typically used when you're starting a new project or making significant changes to your infrastructure.</p> </li> <li> <p><code>azd deploy</code> is used to update an existing deployment. It's helpful when making iterative changes to your application without needing to re-provision the entire environment. This command is ideal for continuous development and deployment scenarios where you frequently update your application.</p> </li> </ul> <p>In other words, use <code>azd up</code> when setting everything up from the beginning and <code>azd deploy</code> when updating an existing deployment.</p>"},{"location":"07-Redeploy/#deploy-the-updated-app","title":"Deploy the Updated App","text":"<p>To deploy the updated app, you need to follow the below steps for the Self-Guided or Instructor-Led workshop that matches what you chose during the Setup section of this guide.</p> Self-GuidedInstructor-Led <ol> <li> <p>Open a new integrated terminal in Visual Studio Code.</p> </li> <li> <p>Ensure you are at the root of your repository.</p> </li> <li> <p>Execute this command to deploy your application with changes.</p> <pre><code>azd deploy\n</code></pre> </li> </ol> <ol> <li> <p>Open a new integrated terminal in Visual Studio Code.</p> </li> <li> <p>Ensure you are in the <code>./workshop</code> folder of your repository.</p> <pre><code>cd workshop\n</code></pre> </li> <li> <p>Execute this command to deploy your application with changes.</p> <pre><code>azd deploy\n</code></pre> </li> </ol>"},{"location":"07-Redeploy/#test-the-deployed-app","title":"Test the Deployed App","text":"<ol> <li> <p>In the Azure portal, return to the resource group containing your resources and select the Container app resource whose name begins with ca-portal.</p> <p></p> </li> <li> <p>In the Essentials section of the Portal Container App's Overview page, select the Application Url to open the deployed Woodgrove Bank Portal in a new browser tab.</p> <p></p> </li> <li> <p>In the Woodgrove Bank Contract Management Portal, select the Dashboard page and use the copilot to ask a few questions and verify that your app changes are live!</p> </li> </ol> <p>You made it! That was a lot to cover - but don't worry! Now that you have a fork of the repo, you can check out the Self-Guided Workshop option to revisit ideas at your own pace! Before you go, there are some important cleanup tasks you need to do!!</p> <p>THANK YOU: Let's wrap up the session by cleaning up resources!</p>"},{"location":"Tear-Down/","title":"Cleanup Resources","text":""},{"location":"Tear-Down/#1-give-us-a-on-github","title":"1. Give us a \u2b50\ufe0f on GitHub","text":"<p>FOUND THIS WORKSHOP AND SAMPLE USEFUL? MAKE SURE YOU GET UPDATES.</p> <p>The PostgreSQL Solution Accelerator: Build Your Own AI Copilot sample is an actively updated project that will reflect the latest features and best practices for code-first development of RAG-based copilots on the Azure AI platform. Visit the repo or click the button below, to give us a \u2b50\ufe0f.</p> <p> Give the PostgreSQL Solution Accelerator a Star!</p>"},{"location":"Tear-Down/#2-provide-feedback","title":"2. Provide Feedback","text":"<p>Check that the right tab is selected for your session, and complete the steps!</p> Self-Guided <p>Reminder 1: Give us Feedback</p> <p>Have feedback that can help us make this lab better for others? Open an issue and let us know.</p>"},{"location":"Tear-Down/#3-clean-up","title":"3. Clean-up","text":"<p>Once you have completed this workshop, delete the Azure resources you created. You are charged for the configured capacity, not how much the resources are used. Follow these instructions to delete your resource group and all resources you created for this solution accelerator.</p> <p>To clean-up the deployment, you need to follow the below steps for the Self-Guided or Instructor-Led workshop that matches what you chose during the Setup section of this guide.</p> Self-GuidedInstructor-Led <ol> <li> <p>In VS Code, open a new integrated terminal prompt.</p> </li> <li> <p>At the terminal prompt, execute the following command to delete the resources created by the deployment script:</p> <pre><code>azd down --purge\n</code></pre> <p>The <code>--purge</code> flag purges the resources that provide soft-delete functionality in Azure, including Azure KeyVault and Azure OpenAI. This flag is required to remove all resources completely.</p> </li> <li> <p>In the terminal window, you will be shown a list of the resources that will be deleted and prompted about continuing. Enter \"y\" at the prompt to being the resource deletion.</p> </li> </ol> <ol> <li> <p>In VS Code, open a new integrated terminal prompt.</p> </li> <li> <p>At the terminal, ensure you are within the <code>./workshop</code> folder of the repository.</p> <pre><code>cd workshop\n</code></pre> </li> <li> <p>At the terminal prompt, execute the following command to delete the resources created by the deployment script:</p> <pre><code>azd down --purge\n</code></pre> <p>The <code>--purge</code> flag purges the resources that provide soft-delete functionality in Azure, including Azure KeyVault and Azure OpenAI. This flag is required to remove all resources completely.</p> </li> <li> <p>In the terminal window, you will be shown a list of the resources that will be deleted and prompted about continuing. Enter \"y\" at the prompt to being the resource deletion.</p> </li> </ol>"},{"location":"Tear-Down/#4-persist-changes-to-github","title":"4. Persist changes to GitHub","text":"<p>If you want to save any changes you have made to files, use the Source Control tool in VS Code to commit and push your changes to your fork of the GitHub repo.</p>"}]}